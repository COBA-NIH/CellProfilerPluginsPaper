{"76290": ["<p>Hello!</p>\n<p>I am using the identifyprimaryobjects function to identify nuclei count in a imunohistological sample, and I have been able to save the data in excel. However, I also need to save the output image as shown below (I know I can manually save them, but when I analyze a big number of samples it goes away very fast and I am not able to manually save them all)</p>\n<p>The image I want to save is this:</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/hAJoyQi0l4UV5O0cKxFAX8HFvZV.bmp\">capture_20230119140936942.bmp</a> (15.3 MB)</p>\n<p>I do not need the table with the results (because I have it in excel), but primarily the three images that show which nuclei was selected for the count.<br>\nIf i try the SaveImages pipeline, it only saves the 8-bit original image (left up), which I obviously already have.</p>\n<p>Is there another way to save the output image other than manually during the analysis?</p>", "<p>Hi,</p>\n<p>I don\u2019t know how to save the result from any Module figure/window either. However, you could save the image on the right and the bottom left separately - by using OverlayObjects and OverlayOutlines modules (and save them separately using SaveImages module). Perhaps someone might have better idea if you would like the whole window.</p>", "<p>Hi,</p>\n<p>For technical reasons, this is more difficult than you might think, so unfortunately it\u2019s not an option CellProfiler offers. <a class=\"mention\" href=\"/u/pearl_wichaidit\">@pearl_wichaidit</a> 's suggestions are good ones - if you want one that\u2019s purely like the top right image, ConvertObjectsToImage will give you a more direct version than OverlayObjects but it will be close either way.</p>", "<p>Amazing! I managed to do it with the overlayobjects and saveimages. Thank you so much! You saved me a lot of time</p>", "<p>Thank you for your suggestion! I am sorry this feature is not available, it would have been a great help, im sure if in the future it will be available we will greatly appreciate it. But the other way worked just fine anyway. Thanks!</p>"], "74242": ["<p>Hi,<br>\nI\u2019m doing a microscopy screen of 96 conditions. For that, I image my cells at very low confluence as I need them to not touch each other. I took all my images on a confocal Opera Phoenix, which take a define number of picture per well. However, I end up with a very large number of image (22 000) and a larger amount of them do not contain cell. I\u2019m wondering if their is a way to automatically sort my images to keep only those who have cells in it ?</p>\n<p>Thanks in advance for your help!</p>", "<p>Hi <a class=\"mention\" href=\"/u/chloe-ap\">@Chloe-Ap</a> ,</p>\n<p>Are you using Harmony software to acquire your images in the opera phenix? If yes, harmony has a function called Pre-scan, where you take a low resolution image of the entire well (usually 10x magnification) and can make an analysis pipeline to automate the microscope to only image areas of interest (in your case, areas where cells are present). Let me know if you are using harmony and I can explain in more detail how to use it.</p>\n<p>BW,<br>\nCamelia</p>", "<p>Hi <a class=\"mention\" href=\"/u/cameliam\">@CameliaM</a>,<br>\nThanks for your answer !</p>\n<p>Indeed, I\u2019m using Harmony with the opera phenix. If you can explain to me how to do this pre-scan it would be perfect for my new experiment!<br>\nHowever, I do not have an easy access to the microscope as it is not on site. So I will not be able to take new images of this experiment and I still need to find out how to sort the images that I already have.</p>\n<p>Thanks for your help!</p>\n<p>Chlo\u00e9</p>", "<p>Hi <a class=\"mention\" href=\"/u/chloe-ap\">@Chloe-Ap</a>,</p>\n<p>I have attached the application guide from Harmony.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/2hvS2BJ0ocmYJV5t33K1izUnivs.pdf\">Opera Phenix Application Guide (1).pdf</a> (12.9 MB)</p>\n<p>For you, pages 106-109 would be of interest. In their example, they are using H3pS10 staining to detect mitotic cells. However, what you want is actually simpler than this. When you are making the analysis pipeline (in the Image Analysis tab), simply use your nuclei staining (DAPI, Hoescht or whichever you are using to identify the nuclei) to detect the cell nucleus.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/4490e5f43447b1a6fa320aa932064517feb2f82d.png\" data-download-href=\"/uploads/short-url/9MyUuaNLGYNNqdb6oJ49AhyOGHH.png?dl=1\" title=\"harmony 1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/4490e5f43447b1a6fa320aa932064517feb2f82d_2_690x440.png\" alt=\"harmony 1\" data-base62-sha1=\"9MyUuaNLGYNNqdb6oJ49AhyOGHH\" width=\"690\" height=\"440\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/4/4490e5f43447b1a6fa320aa932064517feb2f82d_2_690x440.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/4490e5f43447b1a6fa320aa932064517feb2f82d.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/4/4490e5f43447b1a6fa320aa932064517feb2f82d.png 2x\" data-dominant-color=\"7C7B7B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">harmony 1</span><span class=\"informations\">934\u00d7596 308 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Then go to Determine well layout, and make your choices. In this case I put for 40x magnification and to take only 4 fields. What this means is that in your pre-scan, if you have 9 fields (which is usually the case if you are using a 10x magnification for the pre-scan of the well), in each one of these 9 fields it will take 4 images (so in total it would be 36 images per well). These numbers are for you to decide depending how the cells are seeded.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/98c393f8590e010c7ad263a7ff8086a7df17ebc5.png\" data-download-href=\"/uploads/short-url/lNpBajIypiHe83xHFY2xRNfRmHr.png?dl=1\" title=\"harmony 2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98c393f8590e010c7ad263a7ff8086a7df17ebc5_2_690x497.png\" alt=\"harmony 2\" data-base62-sha1=\"lNpBajIypiHe83xHFY2xRNfRmHr\" width=\"690\" height=\"497\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98c393f8590e010c7ad263a7ff8086a7df17ebc5_2_690x497.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/98c393f8590e010c7ad263a7ff8086a7df17ebc5.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/98c393f8590e010c7ad263a7ff8086a7df17ebc5.png 2x\" data-dominant-color=\"808280\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">harmony 2</span><span class=\"informations\">950\u00d7685 469 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>In regards to data you already have, can you please explain what you mean by sorting them? Is it for storage purposes or analysis?<br>\nIf it is for storage, there is no way that I am aware of in which you can pick and chose which images to keep or delete.</p>\n<p>Also, I should also mention that doing this will in a way bias your analysis. It depends why some wells do not have cells (is it because of drug treatments, technical issues -e.g. the cells do not like being seeded on the glass, or the seeding is not done properly-, or how the cells are handled?)</p>\n<p>Good luck!</p>", "<p>Thank you so much for all this explanation! I will definitely use it in the future.</p>\n<p>Concerning the sorting - I want to sort them for the analysis. As a large number of them (around 40%) do not contain cells because of the low confluency, I just want to keep only the images that have cell  and remove the empty one. This will help my to save of lot of time during the analysis. I will then use CellProfiler to analyse the data. I was thinking may be to AI, but I just don\u2019t know this field and what is possible to do with it.</p>\n<p>Again - Thanks for your help.</p>\n<p>Chlo\u00e9</p>", "<p>Hi <a class=\"mention\" href=\"/u/chloe-ap\">@Chloe-Ap</a></p>\n<p>If it is regarding analysis, my suggestion is to use the Harmony software if you have access to one. It is tailored for high throughput image analysis for tens of thousands of images. Even if you have images without cells, it does not matter as they wont influence the analysis.<br>\nIf you are using a free software for analysis on the exported tiffs from Harmony, my feeling is that it would still be quicker to run the analysis on these empty images then trying to figure out a way to skip them. But perhaps someone else can advise here.<br>\nWhich sort of analysis do you need to run?</p>\n<p>If it is for storage purposes, as I mentioned I am not aware of a way to pick and chose which images to keep when working with Harmony.</p>"], "78344": ["<p>Hi,<br>\nI generated a classifier model using CPA 3.0.4 based on features from the actin cytoskeleton of cells. When I plug this model into the ClassifyObjects module in CP 4.2.4 and run the pipeline on new images, I get an error message indicating that some measurements don\u2019t exist, but the appropriate modules are present and running. I would appreciate any help or insight I can get for this issue. I am attaching here all the required information for others to look into (error message, CP pipeline, CPA model, sample images).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png\" data-download-href=\"/uploads/short-url/sAwr8M64Nvc1FyOISchV1Hie36Y.png?dl=1\" title=\"errorCellProfiler\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png\" alt=\"errorCellProfiler\" data-base62-sha1=\"sAwr8M64Nvc1FyOISchV1Hie36Y\" width=\"421\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 2x\" data-dominant-color=\"CBD2DA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">errorCellProfiler</span><span class=\"informations\">430\u00d7510 73.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a class=\"attachment\" href=\"/uploads/short-url/5v48dYmZdYzze8TvbWMxMGMPWX.zip\">CPAmodel_CPpipeline.zip</a> (9.8 MB)</p>\n<p>Thank you!</p>\n<p>Victoria</p>"], "78862": ["<p>Hello Everyone.</p>\n<p>Thank you for your good advice for my last article.<br>\nFinally, I fixed that problem and succeeded in separating individual images from the \u201c*_seg.npy\u201d file.<br>\nNow I am implementing the cell segmentation by using the following command.</p>\n<blockquote>\n<p>python -m cellpose --image_path \u201c/app/mainApi/app/static/6414f306d4e67a92627f2b78/image3/3-Channel Cells_Z010_458.75 nm.ome.tiff\u201d --pretrained_model <strong>tn3</strong> --chan 0 --chan2 0 --diameter 50 --stitch_threshold 0.0 --flow_threshold 0.4 --cellprob_threshold 0.7 --fast_mode  --save_png --save_outlines</p>\n</blockquote>\n<p>Whenever I use this command, the following error occurs.</p>\n<blockquote>\n<p><code>pretrained model has incorrect path</code></p>\n</blockquote>\n<p>Because of this error, the cellpose uses the \u201ccyto\u201d model automatically.</p>\n<p>I think that the cellpose 2.2 provides some standard pretrained models(like cyto or tissuenet) which can be used without specifying the path.<br>\nAccording to the cellpose 2.2 document, we can use some pretrained models and I think that \u201cTN3\u201d is one of them.<br>\nWhy do you think this error occurred?<br>\nHow can I fix this error?<br>\nWhich models can I use without specifying the path?</p>\n<p>Any help would be appreciated.</p>", "<blockquote>\n<p>\u2013pretrained_model <strong>tn3</strong></p>\n</blockquote>\n<p>Can you try using capital T and capital N?<br>\nHere\u2019s the relevant cellpose code I think:</p><aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20\" target=\"_blank\" rel=\"noopener nofollow ugc\">MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/cellpose/models.py#L19-L20</a></h4>\n\n\n\n    <pre class=\"onebox\"><code class=\"lang-py\">\n      <ol class=\"start lines\" start=\"19\" style=\"counter-reset: li-counter 18 ;\">\n          <li>MODEL_NAMES = ['cyto','nuclei','tissuenet','livecell', 'cyto2', 'general',</li>\n          <li>                'CP', 'CPx', 'TN1', 'TN2', 'TN3', 'LC1', 'LC2', 'LC3', 'LC4']</li>\n      </ol>\n    </code></pre>\n\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "80401": ["<p>Hello,</p>\n<p>I am studying cell motility in fibroblasts and am hoping to measure cell protrusions (location, size, tracking). I have segmentation and tracking working, but am having trouble with getting a seed so that I can use Morph and then MeasureObjectSkeleton. My cells do not have a clear nucleus, so I only have PrimaryObjects. I think I could use the center of cells as a seed, but do not know how to set this up/have the \u201ccenters\u201d be identified as objects. I\u2019ve attached an example image, the teal cell (<span class=\"hashtag\">#14</span>) is a good example of cell with a protrusion that I\u2019d like to measure.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/c/3c4cd1703d517b91aff007c0c47d4b9185968acb.jpeg\" data-download-href=\"/uploads/short-url/8BrehdZeAV9q5jDHFHGoDVBtx11.jpeg?dl=1\" title=\"July7_1_03_2_0009_011_020\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/c/3c4cd1703d517b91aff007c0c47d4b9185968acb_2_690x172.jpeg\" alt=\"July7_1_03_2_0009_011_020\" data-base62-sha1=\"8BrehdZeAV9q5jDHFHGoDVBtx11\" width=\"690\" height=\"172\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/c/3c4cd1703d517b91aff007c0c47d4b9185968acb_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/c/3c4cd1703d517b91aff007c0c47d4b9185968acb_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/c/3c4cd1703d517b91aff007c0c47d4b9185968acb_2_1380x344.jpeg 2x\" data-dominant-color=\"0A1408\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">July7_1_03_2_0009_011_020</span><span class=\"informations\">1920\u00d7480 27.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Many thanks!</p>", "<p>Since you mention PrimaryObjects, I\u2019m guessing this should be in CellProfiler?</p>", "<p>Hi <a class=\"mention\" href=\"/u/lilianna_houston\">@Lilianna_Houston</a></p>\n<p>Have a look at the module - <strong>ShrinkToObjectCenters</strong>, there\u2019s also<br>\n<strong>ExpandOrShrinkObjects</strong>, which offers more options.</p>", "<p>Ah, yes, it should. First time posting, sorry about that!</p>", "<p>Ok thanks, I\u2019ll look into those modules.</p>"], "78866": ["<p>Hi everyone,</p>\n<p>I am new to image analysis, so apologies if this is a basic question! I\u2019ve been trying to figure it out for a few days now.</p>\n<p>I have .vsi images that I can open and visualize very nicely in QuPath with my four fluorescent channels (DAPI plus 3 specific stains). My end goal is to analyze individual channels in CellProfiler. Currently, I need to separate the fluorescent channels (without downsampling) so that I can export individual channels using the tile_exporter.</p>\n<p>I know you can split channels in ImageJ, but going from QuPath to ImageJ requires significant downsampling. I would like to avoid downsampling and instead use tile_exporter so that I can analyze in CellProfiler. Is it possible to split fluorescent channels in QuPath and export them individually?</p>\n<p>Thanks!</p>\n<p>Best,<br>\nSarah</p>", "<aside class=\"quote no-group\" data-username=\"svkremer\" data-post=\"1\" data-topic=\"78866\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/5fc32e/40.png\" class=\"avatar\"> Sarah:</div>\n<blockquote>\n<p>My end goal is to analyze individual channels in CellProfiler.</p>\n</blockquote>\n</aside>\n<p>Not sure about the channel handling, that might have changed recently, <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> or <a class=\"mention\" href=\"/u/finglis\">@finglis</a> might know more, but you shouldn\u2019t need to split the channels for CellProfiler, it should handle multichannel files. <a href=\"https://carpenter-singh-lab.broadinstitute.org/blog/input-modules-tutorial\" class=\"inline-onebox\">Input Modules Tutorial | Carpenter-Singh Lab</a><br>\nThe existence of the other channels doesn\u2019t prevent you from ignoring them.</p>"], "16917": ["<p>Hi,</p>\n<p>My images filenames contain a well ID (e.g. A1, A2, A3, etc\u2026). I would like to group the images per well triplicates: group <span class=\"hashtag\">#1</span> would images A1 + images A2 + images A3. Is there a way to do that?<br>\nMany thanks.</p>", "<p>Not natively, but you can take advantage of CP\u2019s metadata extraction tools to do it.  If you\u2019re already extracting well metadata from the file name, you can feed in a CSV that links \u201cWell\u201d to \u201cTriplicate\u201d - see <a href=\"http://forum.image.sc/t/failure-to-load-ome-tiff-sample-data/17763/12?u=bcimini\">this recent post</a> for an example of what such a CSV might look like.  Then just import the CSV in the Metadata module and group however you\u2019d like.</p>", "<p>Thanks.</p>\n<p>Could I use a regular expressions rules that enable me to group wells A1,2,3?</p>", "<p>Not that I could think of, though certainly if someone has an idea please jump in; the creation of the grouping CSV should take only a very few minutes though, so personally that\u2019s how I\u2019d do it.  Up to you, of course.</p>", "<p>happy to try the grouping in csv, still need to understand a bit more what to do\u2026</p>", "<p>Sure!</p>\n<ol>\n<li>Turn on Metadata extraction in CP, if you haven\u2019t already.</li>\n<li>Turn on the \u2018Extract metadata from file name\u2019 settings and set up a regular expression for extracting the Well information from your file names.  If you need help with this, I\u2019d read the CP help on it or search the forum for other examples; I also like <a href=\"https://regex101.com/#python\">this online tool</a> as a way to test out different regular expressions to make sure they work.</li>\n<li>Make a CSV (in Excel or some other program like that) matching the well information to the triplicate information- it\u2019ll look a bit like this:</li>\n</ol>\n<pre><code class=\"lang-auto\">Well     Triplicate\nA01      1\nA02      1\nA03      1\nA04      2\n</code></pre>\n<ol start=\"4\">\n<li>Click \u201cAdd another extraction method\u201d in the Metadata module and hit \u201cImport from file\u201d, then load your CSV.</li>\n<li>When it asks you how to associate the file with your images, tell it the \u201cWell\u201d metatdata you extracted from the file name should be the same as the \u201cWell\u201d metadata in the CSV.</li>\n</ol>", "<p>Great, many thanks.<br>\nHowever, I\u2019m getting an error, step 5 does not seem to work. The regular expression seems ok, I can extract the Well info. but I can\u2019t make \u201ctriplicate\u201d appears anywhere. I\u2019ve attached my pipeline, if you have time, could I ask you to have a look?</p>\n<p>Many thanks.</p>\n<p><a class=\"attachment\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/5/57339b4026e4a4340a98d003892321284f3e5ba9.cppipe\" rel=\"nofollow noopener\">test2.cppipe</a> (13.4 KB)<a class=\"attachment\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/9/90666e2603886d8e073597920857927250521e32.csv\" rel=\"nofollow noopener\">Book1.csv</a> (689 Bytes)</p>", "<p>Can you upload your CSV as well?</p>", "<p>Hi, yes, just realised I forgot the csv file.</p>\n<p>Many thanks</p>", "<p>Can you maybe provide a link to a sample image or two as well?  It seems like you\u2019re trying to edit your previous post to include them but they seem to be stuck at the \u2018Uploading\u2019 stage.  A google drive/dropbox/ etc link might work better.</p>", "<p>Yes, I was trying indeed.</p>\n<p>now, on google drive: <a href=\"https://drive.google.com/drive/folders/0B0kBXUdK3mphS1NzZTJHR01zaHc?usp=sharing\" rel=\"nofollow noopener\">https://drive.google.com/drive/folders/0B0kBXUdK3mphS1NzZTJHR01zaHc?usp=sharing</a></p>\n<p>thanks</p>", "<p>Two things-</p>\n<ol>\n<li>Your regular expression does not seem to work on the images you shared; make sure that you\u2019re getting metadata extraction from that expression alone.</li>\n<li>I\u2019m not sure exactly how you created the CSV, but even once I got 1) working there was something weird with the CSV; I literally just re-saved it in Excel and now CP works fine with it- <a class=\"attachment\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/3/3884bd6bab578819ae23b940701c634c56f0ed88.csv\" rel=\"nofollow noopener\">test1.csv</a> (686 Bytes)</li>\n</ol>", "<p>many thanks, it works now. Even though I did not upload the exact images for this exact pipeline (sorry), with your csv file, it works, that\u2019s great. I don\u2019t understand why my csv file does not work, in excel, both yours and mine look exactly the same. Anyway, it works now, many thanks.</p>", "<p>Hi,</p>\n<p>Now that I can group my images per well triplicates, can I, when exporting the data, have in the data export csv file the results of the measure modules and the group number for each IDed object?<br>\nthanks</p>", "<p>If you\u2019re asking what I think you\u2019re asking, yes you can have the Group ID in the object CSVs- there\u2019s an option in ExportToSpreadsheet to \u2018Add image metadata columns to your object data file\u2019, set that to yes and then all of the metadata (including your Triplicate values) will be on the object spreadsheets.  If that\u2019s NOT what you were asking, can you please clarify?</p>", "<p>yes, thanks, that\u2019s what I was looking for. Many thanks.</p>", "<p>Hello <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a>, I just found this thread but this seems to be a little old. Is there a way to do this in CellProfiler 4.2.1.?</p>\n<p>Many thanks,<br>\nSaana</p>"], "80918": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p>                    <a href=\"https://dl.dropboxusercontent.com/s/ssk33y6w5hudb2x/LIM_B3.jpg?dl=0\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/42856039bff19d6661a2a6bac4ce6a2b937e05c0.jpeg\" data-dominant-color=\"4C443C\" width=\"375\" height=\"500\">\n          </a>\n\n</p>\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<p>I have fungal spores grown in different media with different supplements. The goal is to analyze the germination rate of the fungal spores.</p>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<p>I want to count the total number of spores present in the image, as well as the number of cells developing a germ tube.</p>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<p>I am a complete newbie to image analysis, and I am looking for recommendations on what software and workflows to use, as well as any tutorials teaching me the skills that I would need.<br>\nIdeally, I would like to be able to automate this, since I have two 96-well plates for analysis.</p>", "<p>Good morning,</p>\n<p>As someone also working with filamentous fungi I am pleased to see such a topic.</p>\n<p>Multiples approaches could be tested. You could try to use a skeleton-based approach. The skeleton allows you to automate the measurement of the germ tube length and hence to decipher conidia with germination tubes and without germination tubes yet. It is probably <strong>not</strong> the best approach, but it could be a start!</p>\n<p>Try to:</p>\n<ul>\n<li>First, crop your image to only keep the center region</li>\n<li>Change the type of your image from RGB to 8 or 16 bit</li>\n<li>Blur your image using the smooth option (Process \u2192 Smooth) or with a Gaussian Blur (it will help the threshold)</li>\n<li>Use a threshold (Image-&gt;Adjust-&gt;Threshold) to isolate spores from the background</li>\n<li>Transform your image into a Binary image</li>\n<li>Use Process-&gt;Binary-&gt;Fill holes to close gaps</li>\n<li>Use Process-&gt;Binary-&gt;Skeletonize</li>\n<li>Use Analyze-&gt;Skeleton-&gt;Analyze skeleton</li>\n<li>Use the branch length to decipher conidia without germ tubes from conidia with germ tubes</li>\n</ul>\n<p>By testing different parameters you should be able to find a way to optimize the correct detection of germination tubes. But I am sure someone will find a smarter way to solve the problem.</p>\n<p>You could write a macro to automate all the steps.</p>\n<p>Try to optimize your wet lab part too, by decreasing the amount of conidia/ml you will prevent overlapping conidia which are a major issue for the analysis using the skeleton, find the phase contrast for a more \u2018homogenous\u2019 one as the image quality will be key for the quality of the analysis.</p>\n<p>I am looking forward to read other propositions and approaches.</p>\n<p>Have a good day,</p>", "<p>Thanks for your answer <a class=\"mention\" href=\"/u/adrienh\">@AdrienH</a> . The workflow you presented was done in imagej I assume?</p>", "<p>I am using FIJI, but all or most of the functions proposed are probably shared with ImageJ</p>"], "76822": ["<p>Hello Everyone,<br>\nI am looking for a little bit of help and direction.<br>\nI don\u2019t feel like ImageJ is doing what I fully need it to do. I am looking for more in-depth automated confluence readings and also potentially label free detection of cell death.</p>\n<p>ImageJ is free though and I would have to ask for a budget for new software.<br>\nIs there any helpful resources that people know of that include other image analysis software available and its pricing? I don\u2019t want to reach out to every company for a quote and find its over my budget.</p>\n<p>These are some of the analysis packages I have found thus far:</p>\n<ol>\n<li>Zen Black (Zeiss)</li>\n<li>NIS Elements (Nikon)</li>\n<li>MetaXpress (Molecular Devices)</li>\n<li>Huygens Professional (Scientific Volume Imaging)</li>\n<li>Imaris (Bitplane)</li>\n<li>Acapella (PerkinElmer)</li>\n<li>Gen5 (BioTek)</li>\n<li>Cell Profiler (Broad Institute)</li>\n<li>ImgLib2 (ImgLib)</li>\n</ol>\n<p>Also, can I feed images into these software\u2019s, or do they only work with the instrument?</p>\n<p>Any direction or previous experience would be greatly appreciated!<br>\nThank you!<br>\nNoah</p>", "<p>Would probably help to have a bit more information about the project. Sample image, modality, etc. Label free could be phase, polarized light, SHG, etc.<br>\nYou can do a lot of things with classification in CellProfiler, QuPath etc. Are those included because you used them and they failed?</p>\n<p>I don\u2019t think you\u2019d want to use Zen Black. Zen Blue is where everything has shifted to as far as I am aware, <a class=\"mention\" href=\"/u/sebi06\">@sebi06</a> ?</p>\n<p>Imaris is fun for 3D, but are your images 3D? 3D and 4D analysis capabilities are likely to vary among types of software so it really helps to be specific\u2026 unless you are a core facility.</p>", "<p>Seems like the software you list is a mix of commercial products, open source applications and open source libraries (imglib2 is a library).</p>\n<p>I think this <a href=\"https://febs.onlinelibrary.wiley.com/doi/full/10.1002/1873-3468.14451\">article</a> by <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a> will be useful for you to learn more about some of these.</p>", "<p>You hit the nail on the head, I am so very much appreciative of this resource.</p>", "<p>The Hitchhikers Guide article the <a class=\"mention\" href=\"/u/bnorthan\">@bnorthan</a> in indeed a great start! There\u2019s also biii.eu, though that might just have the effect of making your list of things to try longer, not shorter.</p>\n<p>Since you mention CellProfiler, its pricing is \u201cfree, forever\u201d.</p>\n<p>For confluence measurement, as with everything, the devil is going to be in the details of \u201cwhat your pictures look like\u201d, but I suspect you could hit the ground running pretty quickly by using the <a href=\"https://cellprofiler.org/examples\">Wound Healing example pipeline</a> on our website; it\u2019s designed to measure % area covered in brightfield, so you possibly might not need to modify it at all or only slightly.</p>\n<p>\u201cLabel free detection of cell death\u201d is a harder problem, depending on what you exactly mean by it - are you trying to detect dead floating cells? Predict which cells will eventually die? Some of these things will be easy, and some will be harder, but I encourage you to post pictures and your question and you should get some good advice here! Our vision for this forum has always been making it a place where people could say \u201cHere\u2019s my picture, here\u2019s what I want to know from it, I don\u2019t know which tool to use, help!\u201d</p>", "<p>Yes indeed.</p>\n<p>For advanced image analysis incl. various AI workflows Zeiss offers Zen Blue, Apeer and Vision4d + VisonHub.</p>"], "74261": ["<p>All Gurus,</p>\n<p>Like the title says, how can one tell if an image is well focused? We are here studying image processing/analysis. But we need a good image to begin with. While a good image needs to be taken from microscope well focused. Human eyes may be able to tell if an image is well focused. But is there a computational criteria/algorithm to tell?</p>\n<p>Sorry for raising a primitive question from a rookie.</p>\n<p>Thanks ever so much indeed in advance for any help.</p>\n<p>C.-J. Mei</p>", "<p>I\u2019d say it generally isn\u2019t possible to tell if a single image is in focus without anything to compare it to. A widefield image will have different background and blur than a confocal image. Or a moving sample vs a fixed one. One common method for determining focus is comparing within a stack of images to see which is <em>most</em> in focus. There are a variety of metrics for that in papers on autofocusing that can be found online.</p>\n<p>If your samples are all the same, you could do something fairly simple like create a pixel classifier based on features that detect blur. If you expect no blur due to the type of images you are taking, then it\u2019s easy to use the % of the image detected as blurry by the pixel classifier to toss out a blurry image.</p>\n<p>That doesn\u2019t help you figure out whether the image is the most in focus it could be, or why it\u2019s blurry, or in which direction it might be out of focus.</p>", "<p>Hi <a class=\"mention\" href=\"/u/cj.mei_ikonisys.com\">@cj.mei_ikonisys.com</a></p>\n<p>Adding to <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> you can use the Normalized variance to find the in focus plane of multiple images for example.<br>\nHere is a imagej macro with the implementation to determine image focal quality image-wide (not ROI-wide)</p>\n<p><a href=\"https://imagejdocu.list.lu/macro/normalized_variance\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://imagejdocu.list.lu/macro/normalized_variance</a></p>\n<p>Best,<br>\nMario</p>", "<p>There are a variety of metrics that can be used to compare images to see which one is better in focus. For the most part, these metrics quantify the presence of sharp features relative to blurry features (or equivalently, assess the magnitude of high vs low spatial frequencies in the image). This <a href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.3708/MediaObjects/41587_2016_BFnbt3708_MOESM19_ESM.pdf\" rel=\"noopener nofollow ugc\">supplementary information</a> has a lengthy description of various metrics used.</p>\n<p>In order to estimate focus from a single image, you need to include some prior information about the class of objects being imaged. For example, it would be hard to tell whether a blurry-looking image comes from a smooth object with few high spatial frequency details vs an object with sharp details that are lost due to the low-pass filtering of a defocused imaging system. But if you have some idea of what an in focus object <em>should</em> look like, you can disambiguate these cases. Sometimes you can use an algorithm that learns this information from data, and use it to build a single-image focus detector. For example, <a href=\"https://opg.optica.org/optica/fulltext.cfm?uri=optica-6-6-794&amp;id=413486\" rel=\"noopener nofollow ugc\">this</a>.</p>", "<p><a class=\"mention\" href=\"/u/henrypinkard\">@henrypinkard</a><br>\nThank you, Henry, very much for your informatic response. I\u2019ll study the materials.</p>\n<p>C.-J. Mei</p>", "<p><a class=\"mention\" href=\"/u/mcruz\">@Mcruz</a></p>\n<p>Thank you, Mario, very much for your informatic response. I\u2019ll study the materials.</p>\n<p>C.-J. Mei</p>", "<p>And if you\u2019re feeling fancy, you can use a deep neural network, like this plugin.</p>\n<p>Though in our experience, if you want to do this, you are going to need to train it with your own data most likely.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://imagej.net/plugins/microscope-focus-quality\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/a/dac2223c0c306ed411dd167f5f897e347916f4e8.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://imagej.net/plugins/microscope-focus-quality\" target=\"_blank\" rel=\"noopener\">ImageJ Wiki</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5d2c5e23c9685fc566d532e71e8fd567a481edc1.png\" class=\"thumbnail onebox-avatar\" width=\"256\" height=\"256\">\n\n<h3><a href=\"https://imagej.net/plugins/microscope-focus-quality\" target=\"_blank\" rel=\"noopener\">Microscope Focus Quality</a></h3>\n\n  <p>The ImageJ wiki is a community-edited knowledge base on topics relating to ImageJ, a public domain program for processing and analyzing scientific images, and its ecosystem of derivatives and variants, including ImageJ2, Fiji, and others.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Hello <a class=\"mention\" href=\"/u/cj.mei_ikonisys.com\">@cj.mei_ikonisys.com</a>,</p>\n<p>There have been some great answers already but wanted to add this in here too. Whilst this blog post is about general photography, the principles would be the same for digital pathology and a blurred image should have a distinctly different value from a sharper image. A laplacian is convolved with the image and the sum taken, the higher the result the sharper the image. <a href=\"https://pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\" rel=\"noopener nofollow ugc\">Laplacian blur detector</a></p>", "<p><a class=\"mention\" href=\"/u/matthew_lee\">@Matthew_Lee</a><br>\nThank you very much, Matthew, for your helpful comments and wonderful reference literatures.</p>\n<p>CJ M</p>", "<p>Hello <a class=\"mention\" href=\"/u/cj.mei_ikonisys.com\">@cj.mei_ikonisys.com</a> ,</p>\n<p>The responses here have been great so I don\u2019t have much to add. You can also reference <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6112602/\" rel=\"noopener nofollow ugc\">this paper</a> for step-by-step instructions on how to classify and implement a focus-detection algorithm using Cellprofiler.</p>", "<p><a class=\"mention\" href=\"/u/mposkus\">@mposkus</a><br>\nAh. Great. I have always wished to learn CellProfiler, especially the AI version. I\u2019ll definitely consult you more about how to use it.</p>\n<p>Thank you very much.</p>\n<p>C.-J. Mei</p>", "<p><a class=\"mention\" href=\"/u/mposkus\">@mposkus</a> <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> <a class=\"mention\" href=\"/u/mcruz\">@Mcruz</a> <a class=\"mention\" href=\"/u/oburri\">@oburri</a> <a class=\"mention\" href=\"/u/henrypinkard\">@henrypinkard</a> <a class=\"mention\" href=\"/u/matthew_lee\">@Matthew_Lee</a><br>\nThank you all for your help. The Python statement for blur-ness, or sharpness is: \u201clpls = cv2.Laplacian(image, cv2.CV_64F).var()\u201d. What\u2019s the corresponding C# statement using Emgu, for example. I\u2019m sure there must be expert here knowing the answer.</p>\n<p>Thank you.</p>\n<p>C.-J. Mei</p>", "<p>Hi C.-J,</p>\n<p>I\u2019m not familiar with emgu or C# for that matter but I did take a look at the docs. I found a link to this <a href=\"https://www.emgu.com/wiki/files/4.6.0/document/html/M_Emgu_CV_CvInvoke_Laplacian.htm\" rel=\"noopener nofollow ugc\">method</a> which seems to be the equivalent to the python implementation in the pyimagesearch link I sent. In any case it is a convolution with a 3x3 kernel and the same could be achieved using a convolution method in emgu or elsewhere. Sorry I can\u2019t help more- I\u2019m not familiar with C#.</p>\n<p>All the best<br>\nMatt</p>", "<p><a class=\"mention\" href=\"/u/matthew_lee\">@Matthew_Lee</a><br>\nThank you, Matthew, it\u2019s already a big help. I\u2019ll look into the ref you pointed.</p>\n<p>C.-J. Mei</p>", "<p><a class=\"mention\" href=\"/u/matthew_lee\">@Matthew_Lee</a><br>\nMatthew, Thank you for your continuing help. The Emgu calculation of \u201cLaplacian Variance\u201d issue is resolved. The Emgu documentation is not quite helpful. But with repeated trying, the intelliSense system helped. So it is:<br>\npython: <strong>cv2.Laplacian(image, cv2.CV_64F).var()</strong><br>\nC# Emgu: <strong>image.Laplace(3).AvgSdv(out _bgr, out _sdv);</strong> where _bgr is Emgu.CV.Strcture.Bgr object and _sdv is Emgu.CV.Structure.MCvScalar object.</p>\n<p>For any one who is involved in productivity application development using C#.</p>\n<p>Thank you.</p>\n<p>C.-J. Mei</p>", "<aside class=\"quote no-group\" data-username=\"cj.mei_ikonisys.com\" data-post=\"15\" data-topic=\"74261\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/cj.mei_ikonisys.com/40/54563_2.png\" class=\"avatar\"> CJ MEI:</div>\n<blockquote>\n<p>cv2.Laplacian(image, cv2.CV_64F).var()</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/oburri\">@oburri</a> <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> <a class=\"mention\" href=\"/u/mcruz\">@Mcruz</a> <a class=\"mention\" href=\"/u/henrypinkard\">@henrypinkard</a> <a class=\"mention\" href=\"/u/mposkus\">@mposkus</a> <a class=\"mention\" href=\"/u/matthew_lee\">@Matthew_Lee</a><br>\nGurus,</p>\n<p>Here I found an exception where the blurness/sharpness parameter is not a good indicator of best focusing, which can be validated with human eyes. Here is again a case which may need further insight.</p>\n<p>Bellow, blurness/sharpness parameter value and the corresponding image are given. The best focused images are No. 5 or 6. But the Max of blurness/sharpness is from No. 0.</p>\n<ol start=\"0\">\n<li>\n<p>144.20963;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/1/31143e314f228fb4477af6dc9588810ccacf6e25.jpeg\" data-download-href=\"/uploads/short-url/70aLvQPCkElpqNKP0Ix3w4HJyyp.jpeg?dl=1\" title=\"Channel0_SL0\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31143e314f228fb4477af6dc9588810ccacf6e25_2_666x500.jpeg\" alt=\"Channel0_SL0\" data-base62-sha1=\"70aLvQPCkElpqNKP0Ix3w4HJyyp\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31143e314f228fb4477af6dc9588810ccacf6e25_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31143e314f228fb4477af6dc9588810ccacf6e25_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/1/31143e314f228fb4477af6dc9588810ccacf6e25_2_1332x1000.jpeg 2x\" data-dominant-color=\"686868\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL0</span><span class=\"informations\">1600\u00d71200 54.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>143.06407;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/b/ab42f3309efb7148c955f7f2f7308057735f1620.jpeg\" data-download-href=\"/uploads/short-url/or35oIDlkJfO77bkgCZD3OMxrMs.jpeg?dl=1\" title=\"Channel0_SL1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/b/ab42f3309efb7148c955f7f2f7308057735f1620_2_666x500.jpeg\" alt=\"Channel0_SL1\" data-base62-sha1=\"or35oIDlkJfO77bkgCZD3OMxrMs\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/b/ab42f3309efb7148c955f7f2f7308057735f1620_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/b/ab42f3309efb7148c955f7f2f7308057735f1620_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/b/ab42f3309efb7148c955f7f2f7308057735f1620_2_1332x1000.jpeg 2x\" data-dominant-color=\"676767\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL1</span><span class=\"informations\">1600\u00d71200 58.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>142.70059;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df4c3d447c0aa8340e7d173f721fa0081d6ee38a.jpeg\" data-download-href=\"/uploads/short-url/vRnOI3DwfcETfCiDkfyK3BiSl6O.jpeg?dl=1\" title=\"Channel0_SL2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df4c3d447c0aa8340e7d173f721fa0081d6ee38a_2_666x500.jpeg\" alt=\"Channel0_SL2\" data-base62-sha1=\"vRnOI3DwfcETfCiDkfyK3BiSl6O\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df4c3d447c0aa8340e7d173f721fa0081d6ee38a_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df4c3d447c0aa8340e7d173f721fa0081d6ee38a_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df4c3d447c0aa8340e7d173f721fa0081d6ee38a_2_1332x1000.jpeg 2x\" data-dominant-color=\"676767\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL2</span><span class=\"informations\">1600\u00d71200 58.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>141.93317;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/9/b9b194289a3e6a6478c4185d75426af957974b56.jpeg\" data-download-href=\"/uploads/short-url/quIN9hNRRcArew523e5DdggUZTg.jpeg?dl=1\" title=\"Channel0_SL3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b194289a3e6a6478c4185d75426af957974b56_2_666x500.jpeg\" alt=\"Channel0_SL3\" data-base62-sha1=\"quIN9hNRRcArew523e5DdggUZTg\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b194289a3e6a6478c4185d75426af957974b56_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b194289a3e6a6478c4185d75426af957974b56_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/9/b9b194289a3e6a6478c4185d75426af957974b56_2_1332x1000.jpeg 2x\" data-dominant-color=\"676767\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL3</span><span class=\"informations\">1600\u00d71200 61.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>141.30496;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/d/bda1bf47f63f3b3832983c4a1c3cd210499e87d3.jpeg\" data-download-href=\"/uploads/short-url/r3yMArIbi8EytRdmibOZX8uJcAP.jpeg?dl=1\" title=\"Channel0_SL4\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bda1bf47f63f3b3832983c4a1c3cd210499e87d3_2_666x500.jpeg\" alt=\"Channel0_SL4\" data-base62-sha1=\"r3yMArIbi8EytRdmibOZX8uJcAP\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bda1bf47f63f3b3832983c4a1c3cd210499e87d3_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bda1bf47f63f3b3832983c4a1c3cd210499e87d3_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bda1bf47f63f3b3832983c4a1c3cd210499e87d3_2_1332x1000.jpeg 2x\" data-dominant-color=\"666666\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL4</span><span class=\"informations\">1600\u00d71200 65.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>140.65546;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7c765c146c81946bd08c62a4bccab1f62544eeac.jpeg\" data-download-href=\"/uploads/short-url/hL2N1OxCFZwaXtgO2FScp0wDLJ2.jpeg?dl=1\" title=\"Channel0_SL5\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c765c146c81946bd08c62a4bccab1f62544eeac_2_666x500.jpeg\" alt=\"Channel0_SL5\" data-base62-sha1=\"hL2N1OxCFZwaXtgO2FScp0wDLJ2\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c765c146c81946bd08c62a4bccab1f62544eeac_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c765c146c81946bd08c62a4bccab1f62544eeac_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c765c146c81946bd08c62a4bccab1f62544eeac_2_1332x1000.jpeg 2x\" data-dominant-color=\"656565\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL5</span><span class=\"informations\">1600\u00d71200 67.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>140.74555;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/869c44e076fe86ae7a0564e7502eb68aa8fac106.jpeg\" data-download-href=\"/uploads/short-url/jcOMJ8vPZcyApSwcutChR48Q5bE.jpeg?dl=1\" title=\"Channel0_SL6\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/869c44e076fe86ae7a0564e7502eb68aa8fac106_2_666x500.jpeg\" alt=\"Channel0_SL6\" data-base62-sha1=\"jcOMJ8vPZcyApSwcutChR48Q5bE\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/869c44e076fe86ae7a0564e7502eb68aa8fac106_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/869c44e076fe86ae7a0564e7502eb68aa8fac106_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/869c44e076fe86ae7a0564e7502eb68aa8fac106_2_1332x1000.jpeg 2x\" data-dominant-color=\"656565\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL6</span><span class=\"informations\">1600\u00d71200 67.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>140.14579;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/b/2b858899acfd2359b7d5961150a26e47696d0362.jpeg\" data-download-href=\"/uploads/short-url/6d0CMWNEW7DbwHyM9qMnjIVkTZg.jpeg?dl=1\" title=\"Channel0_SL7\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b858899acfd2359b7d5961150a26e47696d0362_2_666x500.jpeg\" alt=\"Channel0_SL7\" data-base62-sha1=\"6d0CMWNEW7DbwHyM9qMnjIVkTZg\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b858899acfd2359b7d5961150a26e47696d0362_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b858899acfd2359b7d5961150a26e47696d0362_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/b/2b858899acfd2359b7d5961150a26e47696d0362_2_1332x1000.jpeg 2x\" data-dominant-color=\"656565\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL7</span><span class=\"informations\">1600\u00d71200 66 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n<li>\n<p>139.80784;<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0de33778187747dcf480a823f18169aa5344a7db.jpeg\" data-download-href=\"/uploads/short-url/1YR11O97gxLIsg1S2OjLAQO6y9R.jpeg?dl=1\" title=\"Channel0_SL8\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de33778187747dcf480a823f18169aa5344a7db_2_666x500.jpeg\" alt=\"Channel0_SL8\" data-base62-sha1=\"1YR11O97gxLIsg1S2OjLAQO6y9R\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de33778187747dcf480a823f18169aa5344a7db_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de33778187747dcf480a823f18169aa5344a7db_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/d/0de33778187747dcf480a823f18169aa5344a7db_2_1332x1000.jpeg 2x\" data-dominant-color=\"656565\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Channel0_SL8</span><span class=\"informations\">1600\u00d71200 63.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n</li>\n</ol>", "<p>I think it\u2019s a bit tricky with three dimensional cultures like this. The objects that stand out the most, like the ones in the open areas are in poor focus in the last image, but the densely populated areas are in better focus than the previous images, and may take up a greater percentage of the total image. Also, areas that are of less interest to you may be of no less interest to the algorithm, like the background.</p>\n<p>Finding an algorithm to keep \u201cwhat you want\u201d in focus is going to be much harder. Maybe segmenting first and then only analyzing the area of interest\u2026 though that would probably not be great for run-time analysis.</p>\n<p>*rereading the Laplacian, this result seems weird though. I would expect all of the above to be true, but your output values do seem strange, and almost unrelated to the image. Regardless of the direction of the change, I wouldn\u2019t expect to see <em><strong>so little change</strong></em> across those images. Possibly some of it has to do with the saturation. The more in focus the image is, the more pixels will be saturated if the imaging settings are too bright. The more pixels are saturated, the less edges you have, and the less in focus the image will appear.</p>", "<p>How do you compute the laplacian? With a kernel of which size?</p>", "<aside class=\"quote no-group\" data-username=\"cj.mei_ikonisys.com\" data-post=\"15\" data-topic=\"74261\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/cj.mei_ikonisys.com/40/54563_2.png\" class=\"avatar\"> CJ MEI:</div>\n<blockquote>\n<p>cv2.Laplacian(image, cv2.CV_64F).var()</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/gabriel\">@gabriel</a> Hi, there. Thank you for your help. I calculated the Laplacian using the OpenCV function: <strong>cv2.Laplacian(image, cv2.CV_64F).var()</strong>. I\u2019m not quite sure how the number is actually computed.</p>\n<p>C.-J. Mei</p>", "<aside class=\"quote no-group\" data-username=\"cj.mei_ikonisys.com\" data-post=\"20\" data-topic=\"74261\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/cj.mei_ikonisys.com/40/54563_2.png\" class=\"avatar\"> CJ MEI:</div>\n<blockquote>\n<p>cv2.Laplacian</p>\n</blockquote>\n</aside>\n<p><a class=\"mention\" href=\"/u/gabriel\">@gabriel</a> Sorry. This is actually the variance of the Laplacian. A sharp image is supposed to have a \u201csharp\u201d change in signal strength. This result in sharp change in differentiation of the signal strength, which leads to large variance of the Laplacian.</p>\n<p>C.-J. Mei</p>"], "28704": ["<p>Hello everyone,</p>\n<p>I am new to microscopic image analysis. I want to perform cell lineage tracing of yeast cells. For cell segmentation, I am using cellprofiler 2.2 (specifically because I want to use cellstar plugin which is compatible with version 2.2). For cell tracking I am using trackObjects module using the \u201cdistance algorithm\u201d. I am providing my pipeline for the results.<br>\nMy concern is that when I look at the exported csv file, the parent objects of are non-sensical to me. To elaborate, I feel that parent object number of a cell are somewhat random and not representative of the truth.<br>\nI am also providing the raw image stack, the segmented image stack along with cell identities labelled and the associated spreadsheet. I would be much helped if some one could explain it to me how to interpret the data out of trackObject module or if I should make any improvements to my analysis methodology.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/2Zjz1UdLFNeU8zzhT3ROaZSuyf2.csv\">MyExpt_YeastCells.csv</a> (46.1 KB) <a class=\"attachment\" href=\"/uploads/short-url/4kKwP8xAb0MbX222XrfL7Kv87VN.tif\">raw_yeast_stack.tif</a> (6.2 MB) <a class=\"attachment\" href=\"/uploads/short-url/80p8bZD9XB0izQ3qm9Tk5Ui9Aez.tif\">Tracked_objects_stack.tif</a> (9.3 MB) <a class=\"attachment\" href=\"/uploads/short-url/uWbj7zCIzmWhWzDlfF7GDPL1QQ0.cppipe\">Yeast_lineage_tracing_pipeline.cppipe</a> (8.8 KB)<br>\nThank you!<br>\nVarshit</p>", "<p>Just wanted to leave a note that CellStar plugin has been updated to work with all the main versions of CellProfiler: <a href=\"https://www.cellstar-algorithm.org/\" rel=\"noopener nofollow ugc\">https://www.cellstar-algorithm.org/</a><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a0f795d5a379d69e1f809957fb8dd9f269a8ef4b.png\" alt=\"image\" data-base62-sha1=\"mXYQlx6LWFOjimwc7k2RHnvPTjt\" width=\"530\" height=\"127\"></p>", "<p>Just took a look ath your question and for the children I just want to post a screen:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc0cadb8185d882d7d4305c8b2f1d6a56be482a9.png\" data-download-href=\"/uploads/short-url/t76zcl1r8Ac01wPSJE5UR8ITqRz.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc0cadb8185d882d7d4305c8b2f1d6a56be482a9.png\" alt=\"image\" data-base62-sha1=\"t76zcl1r8Ac01wPSJE5UR8ITqRz\" width=\"690\" height=\"280\" data-dominant-color=\"F1F1F1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1437\u00d7585 39.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I think it is has proper values in the spreadsheet:</p>\n<ul>\n<li>TrackObjects_Label_5: is the unique id of each cell throughout the time-series</li>\n<li>TrackObjects_ParentObjectNumber_5: is the <em>Number_Object_Number</em> <strong>in the previous frame</strong>\n</li>\n</ul>"], "74788": ["<p>Hi,</p>\n<p>I am hoping to better understand the features provided by cell profiler modules.  Might there be any documentation that contains details regarding what each feature measures/the definition of features?</p>\n<p>Thank you.</p>", "<p>Hi <a class=\"mention\" href=\"/u/sandra1\">@Sandra1</a>,</p>\n<p>Welcome to the Forum!</p>\n<p>Good question and yes, we have all the documentation with the information about the CellProfiler modules here (<a href=\"https://cellprofiler-manual.s3.amazonaws.com/CellProfiler-4.2.1/index.html#\" class=\"inline-onebox\">Welcome to CellProfiler\u2019s documentation! \u2014 CellProfiler 4.2.1 documentation</a>).</p>\n<p>We also have examples and tutorials at our website (<a href=\"https://tutorials.cellprofiler.org/\">https://tutorials.cellprofiler.org/</a>).</p>\n<p>And inside CellProfiler the Help menu can drive you.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/5/850ed7c0624a1ff62530926831e77353a2d3a9ba.png\" data-download-href=\"/uploads/short-url/iZ5iQSejxjhUH1GoFUPyOc3k3xw.png?dl=1\" title=\"Screen Shot 2022-12-08 at 8.58.31 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/5/850ed7c0624a1ff62530926831e77353a2d3a9ba_2_335x500.png\" alt=\"Screen Shot 2022-12-08 at 8.58.31 AM\" data-base62-sha1=\"iZ5iQSejxjhUH1GoFUPyOc3k3xw\" width=\"335\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/5/850ed7c0624a1ff62530926831e77353a2d3a9ba_2_335x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/5/850ed7c0624a1ff62530926831e77353a2d3a9ba.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/5/850ed7c0624a1ff62530926831e77353a2d3a9ba.png 2x\" data-dominant-color=\"D7DBDB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-08 at 8.58.31 AM</span><span class=\"informations\">355\u00d7529 48.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best,<br>\nMario</p>", "<p>Hi Mario,</p>\n<p>Thank you for your response.</p>\n<p>I think I might have mixed up the terminology a bit.  I wrote \u2018feature,\u2019 but should have written \u2018measurement.\u2019 I\u2019m not very clear on what some measurements such as \u2018Intensity_MassDisplacement,\u2019 might refer to.  Might there definitions for these measurements?</p>\n<p>Thank you.</p>\n<p>Kind Regards,<br>\nSandra</p>", "<p>Hi <a class=\"mention\" href=\"/u/sandra1\">@Sandra1</a>,</p>\n<p>No worries! I think you can find this information both in the manual or inside CellProfiler, for example if you add a module to your pipeline you can click in the question button (left bottom corner) or right click and ask for help. That will pop up a new window showing the explanation about the module itself and details about the measurements made by this module.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/4835f1ce58861af6790331cd4a7e755d62e8f096.png\" data-download-href=\"/uploads/short-url/aiNX7rcf0Bffr5GNL3HqhP6KVEO.png?dl=1\" title=\"Screen Shot 2022-12-09 at 1.55.24 PM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/4835f1ce58861af6790331cd4a7e755d62e8f096_2_690x451.png\" alt=\"Screen Shot 2022-12-09 at 1.55.24 PM\" data-base62-sha1=\"aiNX7rcf0Bffr5GNL3HqhP6KVEO\" width=\"690\" height=\"451\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/4835f1ce58861af6790331cd4a7e755d62e8f096_2_690x451.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/4835f1ce58861af6790331cd4a7e755d62e8f096.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/4835f1ce58861af6790331cd4a7e755d62e8f096.png 2x\" data-dominant-color=\"E7E7E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-09 at 1.55.24 PM</span><span class=\"informations\">961\u00d7629 311 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>The Intensity_MassDisplacement is the distance between the centers of gravity in the gray-level representation of the object and the binary representation of the object.</p>\n<p><a href=\"https://cellprofiler-manual.s3.amazonaws.com/CellProfiler-4.2.1/modules/measurement.html?highlight=massdisplacement\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://cellprofiler-manual.s3.amazonaws.com/CellProfiler-4.2.1/modules/measurement.html?highlight=massdisplacement</a></p>\n<p>Best,<br>\nMario</p>", "<p>Hi Mario,</p>\n<p>Thank you!</p>\n<p>Kind Regards,<br>\nSandra</p>"], "75815": ["<p>Hi all,</p>\n<p>I\u2019m happy to report that COBA-CellProfiler office hours will be continuing in 2023! If you\u2019ve been trying to solve issues on the forum and are almost there but think 30 minutes with an expert could help, you can sign up for slots every-other-Friday at <a href=\"http://broad.io/imagingofficehours\">broad.io/imagingofficehours</a> . We hope to be able to help you out!</p>"], "78888": ["<p>I previously ran into this issue while I was trying to run multiple objects to be exported to database. However, I only have one \u2018object\u2019 selected currently, and I am still receiving an error when I begin to analyze images. Is there a way to avoid this? Thank you</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f3e3eb25ac1e60484e878ef344cce703fe4fda4.png\" alt=\"error1\" data-base62-sha1=\"6JVNC1i5C4eqxFcbPKdTlbZmOyw\" width=\"649\" height=\"301\"><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3.png\" data-download-href=\"/uploads/short-url/czWvj2dMfaF18mQ4lDLnUEBAQOD.png?dl=1\" title=\"error2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3_2_690x449.png\" alt=\"error2\" data-base62-sha1=\"czWvj2dMfaF18mQ4lDLnUEBAQOD\" width=\"690\" height=\"449\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3_2_690x449.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3_2_1035x673.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/582a3c192099e64055cc4672836af8ebbb6fe0c3_2_1380x898.png 2x\" data-dominant-color=\"EAEBED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">error2</span><span class=\"informations\">1768\u00d71153 239 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/c/1cfcfd631da48ca8b93087b4d21b2ec36f34ccbc.png\" alt=\"pipeline\" data-base62-sha1=\"48roDf3xq264fau72sJV5iufcHW\" width=\"432\" height=\"409\"></p>"], "80937": ["<p>The image is of cells which have DAPI, GFP and mCherry. Channel 1 is DAPI, Channel 2 is GFP and Channel 3 is mCherry</p>\n<p>Some cells contain GFP, some mCherry and some express both. All the cells express DAPI</p>\n<p>I am new to this. I want to count only cells which are positive for both GFP and mCherry, that is the yellow cells. In addition to this, there is another small problem that when I open the .tiff file, all the channels appear in blue rather than their default color. How to overcome this issue? I want all the channels to open in their default color.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/dHuBcnDlsHIuoV2pJlhV1HyzGYr.tiff\">00_MP Samples.lif - 00_MP_T+P+_S2R3.tiff</a> (4.1 MB)</p>", "<p>Hi <a class=\"mention\" href=\"/u/shubhanshu25\">@Shubhanshu25</a>,</p>\n<p>If you save your original images in the original file format from your microscopic software instead of Tiff, the color issue should normally solve.<br>\nIn the current setup there is now way that the computer recognizes the original pseudo-color. Even opening your images with Bio-Formats is only assigning the default color sequence red-green-blue.</p>\n<p>However, as long as you know which channel is which protein of interest that is not really dramatic.</p>\n<p>One way to answer your question is to segment the cells from the two channels of interest and then check for an overlap.</p>\n<p>If you do image analysis for the first time, getting to know a little about segmentation is definitely recommendable. There are plenty of excellent resources about it out there. one very general compendium of easy to understand information is <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a>\u2019s <a href=\"https://bioimagebook.github.io/\">Introduction to Bioimage Analysis</a>. There is a special section for processing and analysis.</p>\n<p>Understanding those basics helps to achieve individual image segmentation task easier by yourself.</p>\n<p>For the moment here a small ImageJ Macro, which extracts your cells from your example image and then does the comparison and counting you describe. The methods used are based on all the basics described above but combined in single user friendly tools from the <a href=\"https://biovoxxel.github.io/bv3dbox/\">BioVoxxel 3D Box (bv3dbox)</a>.</p>\n<p>To be able to run the macro you will first need to install a up-to-date Fiji and then run the updater and follow the <a href=\"https://biovoxxel.github.io/bv3dbox/#installation\">installation instructions</a> here.</p>\n<p>Then you can press [Shift] + [n] which opens the Fiji script editor.</p>\n<p>there you paste the following macro in:</p>\n<pre><code class=\"lang-auto\">original_image = getTitle();\nrun(\"Duplicate...\", \"duplicate channels=2\");\nsecond_channel = getTitle();\nrun(\"Voronoi Threshold Labler (2D/3D)\", \"filtermethod=[DoG (diff to r*3)] filterradius=3.0 backgroundsubtractionmethod=None backgroundradius=10.0 histogramusage=full thresholdmethod=Li fillholes=Off separationmethod=[Maxima Spheres] spotsigma=1.0 maximaradius=1.0 volumerange=27-Infinity excludeonedges=false outputtype=Labels stackslice=1 applyoncompleteimage=false processonthefly=false\");\nextracted_second = getTitle();\nselectWindow(original_image);\nrun(\"Duplicate...\", \"duplicate channels=3\");\nthird_channel = getTitle();\nrun(\"Voronoi Threshold Labler (2D/3D)\", \"filtermethod=[DoG (diff to r*3)] filterradius=3.0 backgroundsubtractionmethod=None backgroundradius=10.0 histogramusage=full thresholdmethod=Li fillholes=Off separationmethod=[Maxima Spheres] spotsigma=1.0 maximaradius=1.0 volumerange=27-Infinity excludeonedges=false outputtype=Labels stackslice=1 applyoncompleteimage=false processonthefly=false\");\nextracted_third = getTitle();\nrun(\"Overlap Extractor (2D/3D)\", \"image_plus_1=[\"+extracted_second+\"] image_plus_2=[\"+extracted_third+\"] volume_range=0-100 exclude_edge_objects=false show_original_primary_statistics=false show_extracted_objects=false show_count_statistics=true show_volume_statistics=false show_percent_volume_map=false\");\n</code></pre>\n<p>Then open your image and press the [Run] button in the script editor.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/7/e72e4ad463d8f7a5f0e379ed2bfd9963ad6540e3.png\" data-download-href=\"/uploads/short-url/wZ7txzBVg10zCKOeiH2irj6iQbV.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/7/e72e4ad463d8f7a5f0e379ed2bfd9963ad6540e3_2_528x500.png\" alt=\"image\" data-base62-sha1=\"wZ7txzBVg10zCKOeiH2irj6iQbV\" width=\"528\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/7/e72e4ad463d8f7a5f0e379ed2bfd9963ad6540e3_2_528x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/7/e72e4ad463d8f7a5f0e379ed2bfd9963ad6540e3_2_792x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/7/e72e4ad463d8f7a5f0e379ed2bfd9963ad6540e3_2_1056x1000.png 2x\" data-dominant-color=\"2F312C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1056\u00d71000 78.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Even though this might be a little complex to fully understand the individual steps in the macro you can use this to get started and learn more on image.</p>"], "75308": ["<p>I have a question about the measurement on \"Central Moment features\u2019\u2019<br>\nmade by CellProfiler:<br>\n<a href=\"https://cellprofiler-manual.s3.amazonaws.com/CellProfiler-4.0.5/modules/measurement.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://cellprofiler-manual.s3.amazonaws.com/CellProfiler-4.0.5/modules/measurement.html</a><br>\nUsing Cellprofiler, we obtain some fields related to this measurement, such as \u201cAreaShape_CentralMoment_0_3\u201d and \u201cAreaShape_CentralMoment_2_1\u201d. But, I cannot find their exact definitions from the manual.</p>\n<p>Could you please let me know if you kindly give some information on them? Especially, I should like to interpret the meaning of indexes appearing in their suffix, like \"0_3\u2019\u2019 and \"2_1\u2019'.</p>\n<p>Best regards,<br>\nNakayama</p>", "<p>Hi <a class=\"mention\" href=\"/u/p26m7iv5\">@p26m7iv5</a></p>\n<p>Welcome to the forum!</p>\n<p>Cell profiler <a href=\"https://github.com/CellProfiler/CellProfiler/blob/6dee40ca804806731e2c3d0a16dd1675a4b7687a/cellprofiler/modules/measureobjectsizeshape.py#L773-L775\">uses scikit-image</a> to compute some features, including the central moment features.<br>\nDetailed descriptions of the central moment features are here:<br>\n<a href=\"https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops</a></p>\n<p>From that page:</p>\n<blockquote>\n<p>Central moments (translation invariant) up to 3rd order:</p>\n<p><code>mu_ij = sum{ array(row, col) * (row - row_c)^i * (col - col_c)^j }</code></p>\n<p>where the sum is over the row, col coordinates of the region, and row_c and col_c are the coordinates of the region\u2019s centroid.</p>\n</blockquote>\n<p>So it looks like the first number is the power for the rows, and second numbers is the power over columns.</p>\n<p>At a high level, <a href=\"https://en.wikipedia.org/wiki/Variance\">variance</a> is the second central moment, and <a href=\"https://en.wikipedia.org/wiki/Skewness\">skewness</a> is the third central moment. So intuitively the second and third moments measure the spread and asymmetry in a given direction.</p>\n<p>Hope some of that is helpful,<br>\nJohn</p>", "<p>Good evening John</p>\n<p>Thank you for letting me know!</p>"], "81453": ["<p>Hi all,</p>\n<p>I\u2019ve recently started using CellProfiler and am impressed by its functionality. Although I have a lot of experience in ImageJ (including Macro writing), I am struggling a bit with getting CellProfiler to work on a large experiment that I performed.</p>\n<h3>\n<a name=\"background-1\" class=\"anchor\" href=\"#background-1\"></a>Background</h3>\n<ul>\n<li>I imaged two channels (GFP and mCherry, both nuclear staining) over time in microtiter plate format (24-well).</li>\n<li>The images vary a bit some time and I want to account for the plate-to-plate variation using illumination correction. However, every time I set up the pipeline it returns the following error on the SaveImages module: Error while processing SaveImages: Could not determine source path for image %s\u2019 % (self.image_name.value)</li>\n<li>The error is showing on multiple OS (Windows and the CellProfiler docker image ran on a linux server)</li>\n</ul>\n<h3>\n<a name=\"analysis-goals-2\" class=\"anchor\" href=\"#analysis-goals-2\"></a>Analysis goals</h3>\n<ul>\n<li>Calculate the illumination correction to apply it in another pipeline?</li>\n</ul>\n<h3>\n<a name=\"challenges-3\" class=\"anchor\" href=\"#challenges-3\"></a>Challenges</h3>\n<ul>\n<li>\n<p>What stops you from proceeding?<br>\nSaveImages module: Error while processing SaveImages: Could not determine source path for image %s\u2019 % (self.image_name.value)</p>\n</li>\n<li>\n<p>What have you tried already?<br>\nRenaming the folders, files etc (removed spaces and replaced with underscores).</p>\n</li>\n<li>\n<p>What software packages and/or plugins have you tried?<br>\nCellProfiler 4.2.5</p>\n</li>\n</ul>\n<p>Has anyone faced similar problems and perhaps knows how to solve this issue?</p>"], "76333": ["<p>Hi,</p>\n<p>I am a regular CellProfiler user - via GUI, and now we would like to run it (for our large screening data) on a cluster at work. I have a minimum knowledge of Docker/Sigularity/Linux/Python and I really don\u2019t know where/how to start. I understand that I can get docker from here <a href=\"https://hub.docker.com/r/cellprofiler/distributed-cellprofiler\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Docker</a>, perhaps? but I am wondering if there is a version that includes Plug-ins as well. I have CellPose and/or Stardist in my pipelines. I saw a response on a similar topic <a href=\"https://forum.image.sc/t/cellprofiler-3-1-9-with-classify-pixels-docker/46564\">here</a>  Any detailed suggestion/help would be greatly appreciated. Thank you very much</p>", "<p>Hi Pearl,</p>\n<p>That Dockerhub is for our tool on running on AWS, the \u201cplain\u201d CellProfiler dockers are <a href=\"https://hub.docker.com/repository/docker/cellprofiler/cellprofiler/tags?page=1&amp;ordering=last_updated\">here</a>.</p>\n<p>The Dockers do not contain the plugins, but it would be very easy to make your own that does! It could be as simple as this. The <a href=\"https://github.com/CellProfiler/CellProfiler-plugins#beginner-level-instructions\">plugin installation instructions</a> here should help with package version issues, though note this Docker uses regular python, not conda.</p>\n<pre><code class=\"lang-auto\">FROM cellprofiler/cellprofiler:some_version\n\nWORKDIR /some/directory\nRUN git clone https://github.com/CellProfiler/CellProfiler-plugins.git\nRUN pip install someprogram someotherprogram\nENTRYPOINT [\"cellprofiler\"]\n</code></pre>\n<p>From there</p>\n<pre><code class=\"lang-auto\">docker run \\\n\t--volume=some/local/input/directory:/input \\\n\t--volume=some/local/output/directory:/output \\\n\tyourdocker:yourversion \\\n\t--image-directory=/input \\\n\t--output-directory=/output \\\n\t--pipeline=/input/ExampleHuman.cppipe \\\n\t--file-list=/input/filelist.txt \\\n    --plugins-directory=/some/directory\n</code></pre>\n<p>General guidelines on setting the command line flags is available <a href=\"https://github.com/CellProfiler/CellProfiler/wiki/Getting-started-using-CellProfiler-from-the-command-line\">here</a> and in links within.</p>\n<p>Very best of luck!</p>"], "74799": ["<p>Hi everyone,</p>\n<p>I recently started to use Cellprofiler 4.2.4 with RunCellpose (cellpose 2.1.1) to analyze some 3D datasets. This works fairly well, when using RunCellpose within the pipeline and analyzing the detected objects straight away. However, I struggle with the loading of existing cellpose 3D masks into Cellprofiler as these do not produce the same analysis results and seem somehow broken.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/edfa56342ed0cd3d64525ff16bd2e7b70fd1dd42.png\" data-download-href=\"/uploads/short-url/xXfvhcurqHkFisrZP5a4T7R73lE.png?dl=1\" title=\"cellprofiler_masksLoaded_and_newlyGenerated2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edfa56342ed0cd3d64525ff16bd2e7b70fd1dd42_2_517x225.png\" alt=\"cellprofiler_masksLoaded_and_newlyGenerated2\" data-base62-sha1=\"xXfvhcurqHkFisrZP5a4T7R73lE\" width=\"517\" height=\"225\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edfa56342ed0cd3d64525ff16bd2e7b70fd1dd42_2_517x225.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edfa56342ed0cd3d64525ff16bd2e7b70fd1dd42_2_775x337.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/edfa56342ed0cd3d64525ff16bd2e7b70fd1dd42_2_1034x450.png 2x\" data-dominant-color=\"C3C3C3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">cellprofiler_masksLoaded_and_newlyGenerated2</span><span class=\"informations\">1822\u00d7794 252 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nLeft: OverlayObjects of loaded masks.tif     /    right: OverlayObjects of newly generated cellpose objects)</p>\n<p>Modules like MeasureObjectSizeShape and MeasureImageAreaOccupied show different outcomes for the loaded cellpose objects and the newly generated cellpose objects, although both should be equal. I\u2019ve attached one example stack, masks.tif and pipeline.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/9tUacCHHUzW14kkuCnHQwtMtT9N.cppipe\">2022-11-30_3D-analysis_load_CellposeMasks.cppipe</a> (10.5 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/qsZ1SoVfQROxvGocH2LnfH99OJr.cpproj\">2022-11-30_3D-analysis_load_CellposeMasks.cpproj</a> (637.5 KB)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/62hvnUqKLR6waRchuqxdiaVY0uS.tiff\">30112022_FixationTest_HEp2_M_WELL_G11_zstack_Ch3.tiff</a> (15.9 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/AcvbRMhTe3rRJBVslUv5Dos6lB5.tiff\">30112022_FixationTest_HEp2_M_WELL_G11_zstack_Ch3_nuc_masks.tiff</a> (71.7 KB)</p>\n<p>The cellpose masks were originally created within this pipeline by usage of the modules RunCellpose, ConvertObjectsToImage and SaveImages. The saved masks.tif together with the raw data can be loaded correctly into the Cellpose GUI and doesn\u2019t seem to be corrupt.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f8e562c2266094626e94cfe98ca8ca65295503dd.jpeg\" data-download-href=\"/uploads/short-url/zvPRU4Y0nbO0KHbGNN4whqf4pzL.jpeg?dl=1\" title=\"Cellpose_masksLoaded2.PNG\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8e562c2266094626e94cfe98ca8ca65295503dd_2_296x250.jpeg\" alt=\"Cellpose_masksLoaded2.PNG\" data-base62-sha1=\"zvPRU4Y0nbO0KHbGNN4whqf4pzL\" width=\"296\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8e562c2266094626e94cfe98ca8ca65295503dd_2_296x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8e562c2266094626e94cfe98ca8ca65295503dd_2_444x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8e562c2266094626e94cfe98ca8ca65295503dd_2_592x500.jpeg 2x\" data-dominant-color=\"1B1D19\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Cellpose_masksLoaded2.PNG</span><span class=\"informations\">1369\u00d71155 89.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Still loading this masks.tif as objects into Cellprofiler doesn\u2019t seem to work as it did in 2D. Did maybe the way change how these masks need to be loaded in NamesAndTypes in 3D?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/9/59d864028249d9582e3144e187ea00b902dc3eec.png\" data-download-href=\"/uploads/short-url/cOO6NNe2SJiF1pgjVouQAuGepWc.png?dl=1\" title=\"Cellprofiler_namesandtypes\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/9/59d864028249d9582e3144e187ea00b902dc3eec_2_345x246.png\" alt=\"Cellprofiler_namesandtypes\" data-base62-sha1=\"cOO6NNe2SJiF1pgjVouQAuGepWc\" width=\"345\" height=\"246\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/9/59d864028249d9582e3144e187ea00b902dc3eec_2_345x246.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/9/59d864028249d9582e3144e187ea00b902dc3eec_2_517x369.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/9/59d864028249d9582e3144e187ea00b902dc3eec_2_690x492.png 2x\" data-dominant-color=\"EDEDED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Cellprofiler_namesandtypes</span><span class=\"informations\">1300\u00d7928 260 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Or did I oversee something? Any hints are much appreciated. Maybe <a class=\"mention\" href=\"/u/nabdo\">@Nabdo</a> can share his <a href=\"https://forum.image.sc/t/integration-of-cellpose-3d-output-from-google-colab-with-cellprofiler-or-icy\">experience</a> on loading 3D-masks into Cellprofiler?  Or <a class=\"mention\" href=\"/u/rebecca_senft\">@Rebecca_Senft</a> could comment on that?</p>\n<p>Thanks a lot,<br>\nAnna</p>", "<p>So I\u2019m not 100% sure why loading as objects is not working for you, but there is a workaround. If you load your image as grayscale and then use ConvertImageToObjects then you can use the objects and create the overlay just fine:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed1ba33126863c5cb1eb4c7a9379c7997d0d188a.jpeg\" data-download-href=\"/uploads/short-url/xPyn9JwL2DOMKD1SvfirvFTv3J0.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed1ba33126863c5cb1eb4c7a9379c7997d0d188a_2_583x500.jpeg\" alt=\"image\" data-base62-sha1=\"xPyn9JwL2DOMKD1SvfirvFTv3J0\" width=\"583\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/d/ed1ba33126863c5cb1eb4c7a9379c7997d0d188a_2_583x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed1ba33126863c5cb1eb4c7a9379c7997d0d188a.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/ed1ba33126863c5cb1eb4c7a9379c7997d0d188a.jpeg 2x\" data-dominant-color=\"A1A2A3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">752\u00d7644 58.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Here\u2019s a working pipeline to show you what I mean with just a few modules:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/kMyj1Yc0JIqP2h6FFXad6oxEOM5.cppipe\">FIX_2022-11-30_3D-analysis_load_CellposeMasks.cppipe</a> (6.7 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/2uqKs9Xy49PrNV3GEzZUJlbPx45.cpproj\">FIX_2022-11-30_3D-analysis_load_CellposeMasks.cpproj</a> (1.4 MB)</p>", "<p>Hi Rebecca,</p>\n<p>thanks a lot for that workaround! In addition, I tried the <a href=\"https://forum.image.sc/t/cellprofiler-4-2-5-released/75096\">just released 4.2.5 Cellprofiler version</a> on my sample pipeline and in 4.2.5 the mask loading works fine again!</p>\n<p>BR, Anna</p>"], "79409": ["<p>Hi <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a>,</p>\n<p>In ExportToSpreadSheet: When using \u201cCombine these object measurements with those of the previous\u201d one gets a file with two line for the column headers. While this is fine in principle, in practice it requires writing special code to deal with the two header lines for most downstream analysis software.</p>\n<p>Would it be possible to add an option to \u201cCreate one-line headers\u201d?</p>\n<p>The column names would then be, e.g. \u201cNuclei_Area_Shape\u201d and \u201cCytoplasm_Area_Shape\u201d.</p>", "<p>Definitely possible - can you make a <a href=\"https://github.com/CellProfiler/CellProfiler/issues/new?assignees=&amp;labels=Enhancement%2C+Feature+request&amp;template=feature_request.md&amp;title=\">feature request</a> so we don\u2019t forget? There\u2019s a lot of rewrites planned for the Export modules for CP5!</p>", "<p>Done: <a href=\"https://github.com/CellProfiler/CellProfiler/issues/4772\" class=\"inline-onebox\">Option to join header rows of object tables \u00b7 Issue #4772 \u00b7 CellProfiler/CellProfiler \u00b7 GitHub</a></p>\n<p>Thank you for considering this!</p>"], "12852": ["<p>Hi all,<br>\nI\u2019m a very new CellProfiler user and have spent some time reading through the manual and trying to decode existing pipelines that our lab has used, and have a basic grasp on some settings and logic behind them but am a little lost on others.</p>\n<p>I am trying to create a pipeline that achieves the same measurement information as described in this paper: <a href=\"http://jms.org.br/PDF/v22n2a06.pdf\" rel=\"nofollow noopener\">http://jms.org.br/PDF/v22n2a06.pdf</a><br>\nwhere they used SigmaScan to analyze collagen content and fiber color, as well as spatial distribution of fibers in polarized light images of picrosirius red stained histological sections. This and other papers describe hue definition ranges for red, orange, yellow, and green collagen fibers in their methods and I\u2019m a little confused as to how I can define these for my images using CellProfiler.</p>\n<p>I\u2019ve tried building a pipeline where I used custom definitions for my stain under \u201cUnmixColors\u201d, and used little snippets of a picture (where by eye I isolated a portion of collagen fibers that were strictly red, strictly orange, etc.) to use as my \u201creference photos\u201d for the estimation. However, this (probably incorrect) method clearly grabs most of the photo using these parameters instead of just selecting the portions that each color is supposed to refer to, as evidenced when I step through test mode.</p>\n<p>I think my biggest hurdle is trying to figure out how to ID just the color portions that I want. Any help would be greatly appreciated!</p>\n<p>First attempt at pipeline: <a class=\"attachment\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/c/cabb9770de765bd0c7e8cbfb7d4768256f7cf1d6.cppipe\" rel=\"nofollow noopener\">PicroSiriusRed_polarized_pipeline.cppipe</a> (20.6 KB)</p>\n<p>Thanks,<br>\nChrissy!</p>", "<p>Hi,</p>\n<p>The values you input in UnmixColors are quite incorrect.</p>\n<p>For example, if you want to pick Red fibers, it\u2019s better just use only 1 image in UnmixColors module (don\u2019t add Orange, Yellow, Green subsequently in the same module)<br>\nThe values for custom absorbance would be then:<br>\nR:0.048589<br>\nG:0.448951<br>\nB:0.892234</p>\n<p>You can see the values for Red would be lowest, i.e. \u201cplease don\u2019t absorb red color away\u201d</p>\n<p>Then you make another UnmixColors module just for Orange / Yellow / Green one at a time.</p>\n<p>At the moment you put all values near 0.57 for all three channels, that would read \u201cplease eat each of the color half way\u201d\u2026</p>\n<p>Bests.</p>", "<p>Thank you for your response! I guess another follow-up question is how did you determine these absorbances? I know in ImageJ I can hover over a given pixel and get the RGB coordinates for a pixel but am not sure how this translates to absorbance values.</p>\n<p>Secondly, I think I\u2019m going to need to ID pixels that are within a range of values (e.g. all red pixels that fall between the hue definitions of 2-9 and 230-256 for an 8-bit image). Is this possible?</p>", "<p>Hi,</p>\n<p>If you use ImageJ, you can crop a portion of the pure red region into a new image. Then click Image &gt; Color &gt; Color deconvolution<br>\nand tick the option \u201cShow matrix\u201d, then you can see the absorbance range.</p>\n<p>I\u2019m not quite certain if there\u2019s a specific method for the second question. My workaround would be first use <strong>RescaleIntensity</strong> to scale the image into 8-bit. Then in the <strong>IdentifyPrimaryObjects</strong> , in the box \u201cLower and Upper bounds on threshold\u201d, input a range that is form 1/256<em>2 to 1/256</em>9 (0.0078125 to 0.03515625) for 2-9 and so on.</p>", "<blockquote>\n<p>I guess another follow-up question is how did you determine these absorbances?</p>\n</blockquote>\n<p>As <a class=\"mention\" href=\"/u/minh\">@Minh</a> stated, the first step is to crop down a region that contains ONLY one stain of interest- ImageJ/FIJI is usually best for this.  You can then either use the trick he showed you or  in CP\u2019s UnmixColors module use the \u201cCustom-&gt;Estimate\u201d tool.</p>\n<blockquote>\n<p>Secondly, I think I\u2019m going to need to ID pixels that are within a range of values (e.g. all red pixels that fall between the hue definitions of 2-9 and 230-256 for an 8-bit image). Is this possible?</p>\n</blockquote>\n<p>This is definitely possible, though it\u2019ll take a couple of steps- if you DON\u2019T want pixels higher than a given number (such as 9), you\u2019ll have to mask them out in a first step, then take everything betwen 2-9.  See <a href=\"http://forum.image.sc/t/otsus-thresholding-upper-bounds-are-not-respected/12603/4?u=bcimini\">here</a> for a bit more detail.</p>", "<p>Hi bcimini,</p>\n<p>Thanks for your response and link to the other discussion. I\u2019ve read through the other thread and I <em>think</em> I understand what  the steps are that you\u2019re outlining for determining a threshold range:</p>\n<ol>\n<li>IdentifyPrimaryObjects: Set my upper and lower bounds for Otsu thresholding at 2-9 (or more specifically, 2/256 to 9/256)</li>\n<li>MaskImage: Use initial grayscale image that I thresholded in first IPO as input image for this step as well</li>\n</ol>\n<ul>\n<li>\u201cSelect object for mask\u201d is the output from first IPO which would ostensibly mask anything above 9 for this image (correct?)</li>\n<li>Mask is inverted</li>\n</ul>\n<ol start=\"3\">\n<li>IdentifyPrimaryObjects: Set my upper and lower bounds for Otsu thresholding at 0-2 and input image to my MaskImage output from Step2 to ID any pixels between 2-9 range</li>\n</ol>\n<p>Does this make sense or am I missing something?</p>\n<p>Thanks so much for your input!</p>", "<p>Hi,</p>\n<p>This is how I\u2019d do it:</p>\n<ol>\n<li>(Apply)Threshold- Use a manual threshold and set this to 10/256.  This will give you a mask of anything with an intensity greater than 9.</li>\n<li>MaskImage- mask your original image, and set \u2018Invert the mask\u2019 to \u2018Yes\u2019.  You\u2019ll now have an image that shows you ONLY pixels with a value less than or equal to 9/256.</li>\n<li>IdentifyPrimaryObjects- again use a manual threshold, and set it to 2/256.  Now you\u2019ll be identifying objects only in the 2-9 range.</li>\n</ol>\n<p>Does that make sense?</p>", "<p>Hi Beth,<br>\nThanks for responding! This is very helpful- I am such a newbie at all of this image analysis that having a step-by-step really helps. I have a pipeline written using slight modifications on my initial color range values, instead calling Red 1-13, Orange 14-25, Yellow 26-52, and Green 53-110.</p>\n<p>I\u2019ve done as you suggested with the steps. For instance, for my Red pixels, I\u2019ve thresholded my grayscale image using 14/256, masked the grayscale image with my thresholded mask, then used IdentifyingPrimaryObjects with a manual threshold of 1/256. I then \u201cConvert Objects to Image\u201d and had a binary image for each color range to get my area measurement in pixels.</p>\n<p>I\u2019m just having some issues where I can clearly see green sections by eye in my original images, yet for every single image my return for green area is \u201c0\u201d. Therefore I\u2019m questioning whether my other areas (Red, Yellow, Orange) are also giving me incorrect values.</p>\n<p>Attached is the most current version of the pipeline. Any ideas on what may be happening?</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/AtgojnxBnXiICHvSmV5GapCuTVz.cppipe\">30Nov17_PSR_pipeline.cppipe</a> (20.3 KB)</p>", "<p>Your threshold to make the green image is 0.<strong>0</strong>4296, I think you wanted 0.4296.</p>\n<p>FWIW if you\u2019d like to simplify this a bit you don\u2019t need to do the \u201cConvertObjectsToImage\u201d step, MeasureImageAreaOccupied can measure the area inside objects.</p>", "<p>Hi Beth,<br>\nThanks for catching that! One question I have with my results- with the MeasureImageAreaOccupied function on my objects, what is the difference between the outputs \u201cAreaOccupied_AreaOccupied_Red_Objects\u201d versus \u201cAreaOccupied_TotalArea_Red_Objects,\u201d for example? I have labeled \u201cRed_Objects\u201d to be the outcome of \u201cIdentifyPrimaryObjects\u201d within the specified range after masking all pixels above this range.</p>\n<p>The numbers for each differ from one another in my output file and I\u2019m not sure why or what exactly they\u2019re measuring. I\u2019d ultimately like to find the % area for each color range by: % area covered by color = (total pixel area of color / total pixel area of tissue)*100</p>\n<p>Best,<br>\nChrissy</p>", "<p>From the module help:</p>\n<blockquote>\n<p><strong>Measurements made by this module</strong><br>\nAreaOccupied: The total area occupied by the input objects or binary image.<br>\nTotalImageArea: The total pixel area of the image that was subjected to measurement, excluding masked regions.</p>\n</blockquote>\n<p>So TotalArea_Objects is the area of the image that you masked and fed to IdentifyPrimaryObjects (so, for example, the TotalArea_Orange_Objects would be the total area of all pixels 0-25/256)- the AreaOccupied_Objects is the total area of all the objects found by IdentifyPrimaryObjects (so again for Orange_objects it\u2019s just the objects within your size range and your intensity range of 14-25/256).</p>\n<p>You can make those % calculations after the fact in Excel etc or by using the CalculateMath module to create them within the pipeline.</p>", "<p>Wonderful, thanks for clarifying.</p>\n<p>I\u2019m having a little issue upon closer examination of my pipeline and the outputs. My current pipeline is thus:<br>\n<a class=\"attachment\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/e/ec05288a7d3576ed4d976b2aa9dc3944218dc32e.cppipe\" rel=\"nofollow noopener\">PSR_ColorCalculation_Pipeline.cppipe</a> (18.8 KB)</p>\n<p>I\u2019m finding particularly with the Red range (which I\u2019ve set to be 1/256- 13/256 after masking everything 14/256 and higher), CP is calling everything in the background as within my range. My images are on a black background, and my results from MaskImage and IdentifyPrimaryObjects are below for a sample image:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/d/d457ffe62d59fa53c5a672dedee8c596145a834b.png\" data-download-href=\"/uploads/short-url/uitLfd3hW5F142H21PkmYIewFKz.png?dl=1\" title=\"MaskImage_Red\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/d/d457ffe62d59fa53c5a672dedee8c596145a834b_2_690x387.png\" alt=\"MaskImage_Red\" data-base62-sha1=\"uitLfd3hW5F142H21PkmYIewFKz\" width=\"690\" height=\"387\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/d/d457ffe62d59fa53c5a672dedee8c596145a834b_2_690x387.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/d/d457ffe62d59fa53c5a672dedee8c596145a834b.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/d/d457ffe62d59fa53c5a672dedee8c596145a834b.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/d/d457ffe62d59fa53c5a672dedee8c596145a834b_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">MaskImage_Red</span><span class=\"informations\">1000\u00d7562 120 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/9/9ecda50bd261b9bb4ef2d159e783a51c5c84d362.png\" data-download-href=\"/uploads/short-url/mEQ1ZmOsVmRnQdcuQTYpYLUKw0O.png?dl=1\" title=\"IdentifyPrimaryObjects_Red\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ecda50bd261b9bb4ef2d159e783a51c5c84d362_2_690x388.png\" alt=\"IdentifyPrimaryObjects_Red\" data-base62-sha1=\"mEQ1ZmOsVmRnQdcuQTYpYLUKw0O\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ecda50bd261b9bb4ef2d159e783a51c5c84d362_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/9/9ecda50bd261b9bb4ef2d159e783a51c5c84d362.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/9/9ecda50bd261b9bb4ef2d159e783a51c5c84d362.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ecda50bd261b9bb4ef2d159e783a51c5c84d362_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">IdentifyPrimaryObjects_Red</span><span class=\"informations\">1000\u00d7563 170 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>As you can see in the IPO step, all of the interstitial space between the tissue is being called \u201cRed.\u201d Do I need to do some kind of other processing to pull just the tissue out from the background before I run the pipeline as I have it set now? I\u2019ve tried various methods (which essentially are shots in the dark for me since I\u2019m a newbie at understanding all this image processing), which includes trying to \u201cunmix colors\u201d to ID the background (which was problematic), inverting the image, excluding objects touching edges, etc. before IPO. All seem to be to no avail.</p>\n<p>FWIW I have separate side-by-side BF images that I have a separate pipeline for to calculate total tissue area that I\u2019m pretty happy with, but I\u2019m just having issues with these particular polarized PSR images being on a black background.</p>\n<p>Thanks in advance for your wisdom, and for all the help you\u2019ve already given me!</p>", "<p>I honestly don\u2019t have the experience to know what \u201cshould\u201d be being pulled out in that image- can you explain more what you want identified?</p>", "<p>So in the original \u201cRAW_Gray\u201d image the polarized collagen fibers are on the dark background but are not inclusive of the entire tissue area. In the \u201cMask_Red\u201d image essentially all the dark parts are the tissue and the black background has become gray since I did an \u201cinvert mask.\u201d My issue is that since the original image is on a black background, when I invert it in the masking step and it creates this gray background in the inverted, masked image, it calls all that gray background \u201cred\u201d since (I assume) it falls within the color range I\u2019ve specified for my red pixels.</p>\n<p>If you look at the \u201cRed Objects Outlines\u201d output, I\u2019d essentially like ONLY the objects that are dark on the inside and outlined in green, but it is outlining the entire tissue slice so it calls all the gray parts \u201cRed_Objects.\u201d What ends up happening then is that the output for red objects is over 100% of the total tissue area, since it is calling Red Objects both in the tissue itself and the slice background, and I find the actual tissue area excluding the background using a different Brightfield image and pipeline.</p>\n<p>So I guess I need to figure out a way to pull JUST the tissue out to quantify the red area. For reference, the original, unmanipulated image is this:</p>\n<p><img src=\"//cdck-file-uploads-global.s3.dualstack.us-west-2.amazonaws.com/business4/uploads/imagej/original/2X/d/dd72b86b43d1879d08bb9f01a5a3d8faf5f8115f.png\" alt=\"Polarized_PSR_image\" width=\"600\" height=\"450\"></p>\n<p>Does that make sense?</p>", "<p>I think you just need to set a higher threshold then if you want to exclude that background area- hover over the background grey pixels to see what their value is and set it higher than that.  You say up-thread you\u2019re defining \u201cred\u201d as anything 1-13 (of 255); many background pixels will indeed have a value of 1/255, so I think setting it to 2 or 3 (or more) to 13 of 255 is more likely to get you the results you want.</p>", "<p>Hi!<br>\nI use to analyze picrosirius red histologies too and i developed the following pipeline to do it:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/lDp5c34jT1AjybReCU1dpkWvvGJ.cpproj\">Sirius_working.cpproj</a> (435.5 KB)<br>\nAs you can see i tried to identify the red as objects, but i realized that measuring the pixel intensity on the \u201cred channel\u201d would be more accurate.<br>\nMaybe this can help you.</p>", "<p>Hi all,</p>\n<p>I am brand new to this forum as well as the CP. I am trying to do the same thing - calculating the fibrosis area percentage of the whole tissue.</p>\n<p>Valerio, did you consider the area without tissues on the slides?</p>\n<p>Thank you!<br>\nBen</p>", "<p>Hi <a class=\"mention\" href=\"/u/cbdai2000\">@cbdai2000</a>,</p>\n<p>Users will typically exclude the non-tissue area using the MaskImage module. If you generate a threshold which separates the sample and background, this can then be used with MaskImage to exclude irrelevant areas of the image. Anything masked off will not be considered when taking measurements.</p>", "<p><a class=\"mention\" href=\"/u/dstirling\">@DStirling</a> Thank you Stirling! I read the following post <a href=\"https://forum.image.sc/t/sirius-red-fast-green-staining/25499\" class=\"inline-onebox\">Sirius red/fast green staining</a> on a similar topic. Do you think I can use their threshold to exclude the background?</p>\n<p><a class=\"mention\" href=\"/u/valerio_laghi\">@Valerio_Laghi</a>, I am a rooster and I was trying your script, many thanks!</p>", "<p>You should be able to threshold out the background, but without seeing your images I can\u2019t say for sure. I\u2019d suggest trying it to see how it goes.</p>"], "79414": ["<p>Hi <a class=\"mention\" href=\"/u/aklemm\">@aklemm</a> <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> ,</p>\n<p>I am using <code>ConvertObjectsToImage</code> and <code>SaveImages</code> to save label masks as 8-bit TIFF.</p>\n<p>My issue is that the saved image does not have the correct label IDs but seems to rescale from 0 to 255.</p>\n<p>Probably I am doing something wrong?</p>", "<p>OK, it seems that one needs to convert to <code>uint16</code> and <strong>not</strong> to <code>GrayScale</code> within <code>ConvertObjectsToImage</code>.</p>\n<p>I probably knew that but somehow I did so much NextFlow now that my CellProfiler skills got a bit rusty <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "75833": ["<p>Hello everyone,</p>\n<p>For my thesis project I am trying to measure the cell alignment in images and I want the data to be quantified. I already found the tool \u201cMeasureObjectSizeShape\u201d and the matching \u201cExporttoSpreadsheet\u201d.  At \u201cSelect measurements\u201d, I clicked on \u201cidentify primary objects\u201d, then \u201cAreaSizeShape\u201d and then\"eccentricity\", because I thought that would be a good indicator to quantify the changed morphology. Unfortunately the results doesn\u00b4t show the alignment that is clearly visible in the images. I think the reason is that currently I am measuring the eccentricity of the primary objects (nucleus) that isn\u00b4t aligned.</p>\n<p>I would like to know how you quantify the cell alignment using cell profiler, if there is a way. Unfortunately I haven\u00b4t found any threads on that topic.</p>\n<p>Thank you!</p>", "<p>Not sure if CellProfiler directly has the Feret Angle measurement, but that\u2019s what I used through Fiji within QuPath. You might be able to do similar through CellProfiler\u2019s integration of Fiji (or maybe it has its own angle measurement). <a href=\"https://forum.image.sc/t/export-cell-boundaries/38196/20\" class=\"inline-onebox\">Export cell boundaries - #20 by Research_Associate</a><br>\nThe Feret angle would ignore any polarity, however (180 degrees left to right would be the same as 180 degrees right to left). So depends on your problem.</p>", "<p>Eccentricity as commonly computed (the ratio of  the lengths of the axes of a fitted ellipse) is a measure of shape and isn\u2019t suitable for measuring orientation. Without more info and example images, it\u2019s hard to know what would be suitable but a classical methods is to use the long axis of the ellipse fitted to the object of interest to compute angles between pairs of objects or angles with some reference axis.</p>", "<p>You may find the Orientation features in CellProfiler more helpful than eccentricity, if I\u2019m understanding your use case correctly! If not, can you go into further detail?</p>"], "77371": ["<p>I have 24-bit 2424x2424 nuclear and cytoplasmic protein-stained images that I am trying to use in this pipeline. The issue I am having is that the \u201cNames and Types\u201d module does not recognize my image set due to its name. For simplicity, I have named the two images \u201cChannel 1\u201d and \u201cChannel 2\u201d pertaining to \u201cOrigProtein\u201d and \u201cOrigDNA\u201d respectively as shown in the example pipeline. However, when I use the \u201cupdate\u201d button, I receive the follow two errors:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/e/ee8fd10d8b83a42054b6ef70bb02d3c71a3536a7.png\" alt=\"image\" data-base62-sha1=\"y2pLlqyGXexURLQYW7BBQw07ncr\" width=\"533\" height=\"339\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/f/ef5c3dfa6c47383d228bda8e560133e491446007.png\" alt=\"image\" data-base62-sha1=\"y9tK4YAaHLfV4ivM0Pob7jrMQd1\" width=\"363\" height=\"82\"></p>\n<p>I have tried changing the folder the images were into the folder where the pipeline was located, and various name matching in the \u201cNames and Types\u201d module. The only thing that allowed the pipeline to run correctly and recognize my images was naming my images with the exact name of the example images that came with the example pipeline (e.g. Channel 1-01-A-01-00). I\u2019m not sure why this is the case. For reference, I have \u201cMetadata\u201d and \u201cGroups\u201d off. Any help pertaining to why the pipeline cannot recognize my naming would be greatly appreciated!</p>", "<p>Hi Tamanna,</p>\n<p>The error is saying it can match the OrigDNA to the OrigProtein image to create an image set, if you scroll all the way down on the NamesAndTypes module there is an \u201cImage set matching method\u201d if you only have those two images you can set it to Order and that should work. If that does not work, could you share your pipeline and a sample image set to try to replicate the error.</p>\n<p>Barbara</p>", "<p>Hi,</p>\n<p>Thank you so much, that worked! I can\u2019t believe I missed that.</p>"], "79931": ["<p>Hi there,</p>\n<p>I\u2019m analyzing the area of cells and would like to filter the objects based on a percentile. In other words, I want to filter out the smallest 5% and the largest 5% of the objects.</p>\n<p>From what I can see, FilterObjects seem to use absolute values without an option to use percentiles - am I missing something?</p>"], "74299": ["<p>Hi all!<br>\nI am trying to segment nuclei (stained with DAPI) and satellite cell-derived myotubes by using MyHC antibody.<br>\nThe goal is to automatically segment and count the number and percentage of MyHC-positive cells (i.e. Differentiation index) and the average number of nuclei per mytube (i.e. Fusion index). Moreover, I would like to classify myotubes depending on their number of nuclei (&lt;2, 2&lt;&lt;5 and &gt;5 nuclei per myotube. Assuming that this can be achieved by using MaskObjects, RelateObjects and ClassifyObjects (with an appropriate bin number), an accurate myotube segmentation is still a problem. Indeed, myotubes sometimes overlap in vitro and the signal/intensity of MyHC is not always uniform.<br>\nThe pipeline I have designed with CellProfiler is perfectly able to segment nuclei, but there are still problems with myotube segmentation.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/4Pqca2xcohaCkqRFy79Nm88DV.tif\">Pipeline_workflow.tif</a> (3.2 MB)<br>\nRobust background threshold method seems to work pretty good for this kind of image. I have tried also to Enhance Line structures and Enhance tube structures (EnhanceOrSuppressFeatures) but they suppress low-intensity signals.<br>\nYou can see below some examples in which is present over- or under-segmentation of objects.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/vvbrbU7iHLZxfJUultaFDa6N0iZ.tiff\">MyHC_Cells_01.tiff</a> (961.3 KB)<br>\nEXAMPLE 1 - In this image I would like to connect two \u201cneighbours\u201d objects (yellow arrows) which have skeletons sharing the same spatial orientation. I know there is a MorphologicalSkeleton module but I don\u2019t know if I can use it for this goal.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/wZvJEqkFNQRoK03YndNplxNoRxu.tiff\">MyHC_Cells_02.tiff</a> (940.9 KB)<br>\nEXAMPLE 2 - In order to improve segmentation, I don\u2019t know if it is possible to take advantage of the intensity of known objects (recognizible by eye; yellow arrows). If I use one of the methods to distinguish clumped objects (in IdentifyPrimaryObjects) it will result in over-segmentation.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/d1uRoDGDGskku471unEabgXToaa.tiff\">MyHC_Cells_03.tiff</a> (885.1 KB)<br>\nEXAMPLE 3 - Increase the segmentation of clumped objects if their skeletons are orthogonal (or close to orthogonality; yellow arrow).</p>\n<p>I fear I need to use a training set + CellProfiler analyst. It looks like my problem is similar to the approach described here: <a href=\"https://cellprofiler.org/wormtoolbox\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">WormToolbox | CellProfiler</a></p>\n<p>Sorry for the very long post.</p>\n<p>Thank you for your attention<br>\nAlessio</p>"], "79422": ["<p>Our company has recently updated its proxy server settings, and <strong>CellProfiler now throws exceptions</strong> when I try to open a pipeline or image list <strong>from a URL in our intranet</strong>.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e30fdfc90d96de3e4541a78ca0db0b737b7a2757.png\" data-download-href=\"/uploads/short-url/woGoyk94v03z9HMzKztIIjJ6SGj.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e30fdfc90d96de3e4541a78ca0db0b737b7a2757.png\" alt=\"image\" data-base62-sha1=\"woGoyk94v03z9HMzKztIIjJ6SGj\" width=\"690\" height=\"408\" data-dominant-color=\"011212\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">700\u00d7414 9.39 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>CellProfiler is running on a Windows 10 workstation and the proxy settings are locked down by policy</p>\n<p>Previously all http/https requests would go through the same proxy server, but now requests to intranet resources must not use the proxy server. I suspect that the failures to open the URLs are directly related to that. Are there any options in CellProfiler to set the no_proxy addresses?</p>\n<p>I have <strong>confirmed that I can open pipelines and image lists from external URLs without any issues</strong></p>", "<p>As an <strong>update (but not complete solution)</strong> I removed 2 environment variables that were pointing to the proxy server HTTP_PROXY and HTTPS_PROXY. I had set these in the past to work with other development tools such as Java, Git etc.</p>\n<p>To remove them I simply opened a console and typed:<br>\nSET HTTP_PROXY=<br>\nSET HTTPS_PROXY=</p>\n<p>After these environment variables were removed, I started CellProfiler (from the same console) and <strong>now the intranet URLs open without an issue</strong> <em>BUT external URLs fail</em>. So the problem is reversed !</p>\n<p>It would be nice to find a setting that allows access to URL resources within the intanet AND the internet. Any suggestions?</p>"], "75841": ["<p>Hello,</p>\n<p>I am new to image analysis. For this project, I am infecting immune cells with bacteria expressing a Timer protein (the protein will fluoresces in the GFP spectra when immature and once folding is complete fluoresces in the RFP spectra). The goal is to assess if the Timer protein can be used to visualize bacterial viability.</p>\n<p>I am trying to accomplish 3 tasks, expand the nuclear stain used (hoechst 33342, channel 2) to cover most of the cell, analyze only bacteria localized within the bounds of a cell, quantify the intensity of the GFP and dsRed signal of these bacteria (dsRed is channel 0, GFP is channel 1).</p>\n<p>I am working on a mac with cell profiler version 4.2.5.</p>\n<p>Below is an example image and my pipeline. A few general questions:</p>\n<ol>\n<li>Does the placement of the MeasureObjectIntensity function within the pipeline make sense?</li>\n<li>Since I am curious about the relationship between the GFP and dsRed signals, should I be relating these objects first in the pipeline?</li>\n<li>Is it best practice to measure off of the denoised images or the original?</li>\n<li>When I measure the GFP and dsRed and objects in the GFP and dsRed images the numbers are different and the integrated intensities are at very different scales, what would cause this?</li>\n<li>Since I am trying to account for signals in different points in the Z-plane, I am planning on looking at a ratio of the GFP to dsRed signal and then comparing that value between conditions. Is this an appropriate approach?</li>\n</ol>\n<p>Thank you!<br>\n<a class=\"attachment\" href=\"/uploads/short-url/tFhfjS93QTrApdK832uQHjcGvfA.tif\">Timer_Example_Image.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/qux4NsZt8dXUvjFsaa0axs3XNep.cpproj\">Timer_Color_Intensity_Test.cpproj</a> (1.8 MB)</p>", "<p>With just your example image, I wasn\u2019t able to get any image sets in your pipeline so I\u2019m evaluating it mostly from reading it:</p>\n<ol>\n<li>Yes, you have it after you create the objects, which makes sense</li>\n<li>It depends on what you mean by \u201crelationship\u201d You could use a module like MeasureColocalization to get a per-pixel measure of colocalization across the channels within your objects. If you want to look at colocalization at a per-object level, you could do this a few ways. For GFP objects where the question is 'how much DsRed is inside of GFP+ objects, you could 1) MeasureObjectIntensity as you do in your pipeline of DsRed within the GFP+ objects, 2) make a binary mask from the DsRed channel and measure the \u201cintensity\u201d of GFP+ objects on the binary mask. Average intensity then corresponds to the fraction of the GFP+ object that overlaps with the DsRed signal. There are also other ways, but I\u2019d need to know more about the final measurement you want.</li>\n<li>In general, we\u2019d recommend original or illumination-corrected for measuring</li>\n<li>I\u2019m kind of confused by the question. Could you show me what you mean?</li>\n<li>The image you sent only has 1 z plane. In general looking at a ratio makes sense to me (you can get this using CalculateMath) but this really depends on what your ultimate goal is. If you imagine your results section, what would it say? GFP and DsRed were correlated? GFP objects contained X amount of DsRed?</li>\n</ol>"], "80969": ["<p>Hello,</p>\n<p>I am hoping to get some advice. I am trying to use the <code>Groups</code> module to separate images based on the folder names which indicate the plate so that when I use <code>ExportToDatabase</code>, it will output one file for each plate.</p>\n<p>I do not see a way to do this in the GUI.</p>\n<p>Does anyone know a way to do this in the GUI that I am missing?</p>\n<p>Thank you in advance!</p>"], "80973": ["<p>I am new to CellProfiler and cannot figure out how to run an analysis that will compare my experimental cells to my control cells. What pipeline should I use? I also don\u2019t know if specific photo parameters are needed, I put my photos in black and white then inverted the color, as the original photos were taken on a compound microscope after the cells were stained with crystal violet.</p>", "<p>Hi Gavie,<br>\nYou could use Unmixtutorial (<a href=\"https://tutorials.cellprofiler.org/#unmixcolors-tutorial\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Introduction to tutorials | Tutorials</a>) to segment out the crystal violet stained cells and then use the measurement module \u2018MeasureObjectSizeShape\u2019 if you are interested in knowing the size and shape difference between your control cells and experimental cells. You could also have a look at other measurement modules available in cellprofiler if you would like to know the difference in other morphological aspects of the cell.</p>"], "78415": ["<p>Does anyone have any experience setting up Cellprofiler pipelines to use in Orbit? I am trying to analyze intensity of a slide with 3 fluorescence channels. I\u2019m not entirely sure what I am doing wrong, as I am not even getting errors. I have followed the documentation on Orbit\u2019s website. My measurements should output to a csv file, and it never appears. The tiles are created by Orbit and 6 folders are created with various sections of the ROI. Alternatively, does anyone have experience processing wholes slides using Cellprofiler? Thanks in advance!</p>", "<p>Dear <a class=\"mention\" href=\"/u/hroberts49\">@HRoberts49</a> ,<br>\nplease make sure that you selected the cp features according to the orbit documentation and use cp 2.x.x .<br>\nI recommend to start with the cp pipeline provided at the orbit doc.</p>\n<p>Regards,<br>\nManuel</p>"], "79446": ["<p>Hi everyone! I am a newbie to bioimaging analysis. I am lost on how to analyze my stack of images. The tutorials I have seen have not helped me much. I have a lot of images to analyze in colors: blue (DAPI), red (Cyanine 3) and green (EGFP). I also have images that are merges of the three filters above. The microscope I used to obtain the images is a ZOE Fluorescent Cell Imager. I saved them in JPG format in 4 different folders depending on the filter used.</p>\n<p>Basically I have images of HeLa cells transfected under the following conditions:</p>\n<ol>\n<li>pHSV-IRES-EGFP (C) (plasmid only)</li>\n<li>pHSV-OH1-WT-IRES-EGFP</li>\n<li>pHSV-OH1-C15S-IRES-EGFP</li>\n<li>pHSV-OH1-C112S-IRES-EGFP (inactive OH1)</li>\n<li>pHSV-OH1-Y151F-IRES-EGFP</li>\n<li>pHSV-OH1-Y152F-IRES-EGFP</li>\n</ol>\n<p>C1: Non-INF-gamma induced transfected control.<br>\nC2: Non-transfected non-induced control.<br>\nC3: Non-transfected IFN-gamma induced control.</p>\n<p>The experiment aims to determine whether STAT1 subcellular localization in HeLa cells stimulated with IFN-gamma is affected by OH1 (i.e. cytoplasmic vs nuclear, or an intermediate form). With respect to the wild-type OH1, it can sequester STAT1 and prevent its translocation to the nucleus. Therefore, an overlap between STAT1 (red color) and OH1 (green color) is expected to be observed. For the C112S inactive mutant, a predominantly nuclear localization of STAT1 is expected to be observed. My group and I are particularly interested in the behavior of the Y151F and Y152F tyrosine mutants. Which type of colocalization, pixel-based or object-based, would work in this type of experiment?</p>\n<p>Any help would be appreciated.</p>\n<p>Best,<br>\nFelipe</p>", "<p>Hello Felipe, welcome!</p>\n<p>Just in case you haven\u2019t seen, there is a CellProfiler tutorials page that covers a few examples, found here: <a href=\"https://tutorials.cellprofiler.org/\">https://tutorials.cellprofiler.org/</a>. For you, I think the <strong>Translocation</strong> tutorial would be an ideal start. The included PDF should hopefully demonstrate to you how you can process your DAPI, Cy3 and eGFP images together, so that images taken at the same position are treated as one image set. That said, it will require some tinkering of the regular expression in the metadata input module to capture the right information from whatever your image file names may be.</p>\n<p>As for your image format, it\u2019s not recommended for you to perform quantitative analysis using JPG images. This is because JPG is a lossy compression format, meaning the image quality is sacrificed to decrease file size. When you decrease image quality like this, any quantitative measurement you make in CellProfiler will be hindered. Looking at the manual for the ZOE Fluorescent Cell Imager I can see that it supports saving images in the tiff format (likely with lossless compression) which is the typical format images are saved in for bioimage analysis.</p>\n<p>Now, for actual bioimage analysis recommendations. The aforementioned <strong>Translocation</strong> tutorial shows how to quantify a GFP signal that translocates between the cytoplasm and nucleus under different drug treatments. So, I think this can be directly applied to your own problem as it would tell you how STAT1 localisation changes in response to OH1. This will be done using the object-based colocalization that you mentioned. However, in a CellProfiler pipeline you could also alter the <strong>MeasureColocalization</strong> module in the translocation CellProfiler pipeline to measure correlation for <em>Both</em>, instead of just <em>Within objects</em>. As with anything bioimage analysis related, it\u2019s good to play around with things and record as much data as possible!</p>"], "77399": ["<p>Hey everyone,<br>\nI would like to open a tif file on ImageJ but I got this error message \u201cunsupported format or file not found\u201d<br>\nThe error message and the file is below.<br>\nThanks.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e5d66888bfff50c094b2e2bf8920303e580f31c.jpeg\" alt=\"error\" data-base62-sha1=\"6Ca4xTz8cLsGSfuZ5RnKE50bEHa\" width=\"480\" height=\"182\"></p>\n<aside class=\"onebox googledrive\" data-onebox-src=\"https://drive.google.com/file/d/1bTvvYCsiTrSOM93xXanibxEmILOW9Bca/view?usp=sharing\">\n  <header class=\"source\">\n\n      <a href=\"https://drive.google.com/file/d/1bTvvYCsiTrSOM93xXanibxEmILOW9Bca/view?usp=sharing\" target=\"_blank\" rel=\"noopener nofollow ugc\">drive.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https://drive.google.com/file/d/1bTvvYCsiTrSOM93xXanibxEmILOW9Bca/view?usp=sharing\" target=\"_blank\" rel=\"noopener nofollow ugc\"><span class=\"googledocs-onebox-logo g-drive-logo\"></span></a>\n\n\n\n<h3><a href=\"https://drive.google.com/file/d/1bTvvYCsiTrSOM93xXanibxEmILOW9Bca/view?usp=sharing\" target=\"_blank\" rel=\"noopener nofollow ugc\">Deprem_Sonra.tif</a></h3>\n\n<p>Google Drive file.</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Hi <a class=\"mention\" href=\"/u/ferit\">@Ferit</a>, I tested the file with a fresh FIJI install and was able to open it ok (though the JPEG decompression took a long time). Perhaps updating to the latest version may solve the problem?</p>"], "73302": ["<p>Hi Forum!</p>\n<p>I am definitely new on CellProfiler 3D.<br>\nI am preparing a pipeline to collect shape and size form a set of 3D objects.</p>\n<p>My dataset is composed of TIFF stacks form a monolayer of cells stained for nuclei and mitochondria. Nuclear and mitochondrial signal are merged to guess the whole cell.</p>\n<p>Segmentation of the three objects works pretty well.<br>\nThe pipeline can retrieve shape/size measures for nuclei and cells, but for mitochondria it returns the error:<br>\n\u201cError while processing MeasureObjectSizeShape: math domain error\u201d.</p>\n<p>Does anybody have any suggestion about how to solve the issue? I attach pipeline and test images\u2026</p>\n<p>Thanks!<br>\nMassimo</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/ueKWGY0YV4QkoGgvMwUAhTWtQwf.zip\">3Dtest.zip</a> (14.7 MB)</p>", "<p>Hi Massimo,</p>\n<p>Sorry to hear about this! I can indeed reproduce, and there\u2019s nothing obvious in your pipeline that would be triggering it. I\u2019ve filed a <a href=\"https://github.com/CellProfiler/CellProfiler/issues/4663\">bug report</a>, and we will do our best to track it down!</p>\n<p>~Beth</p>", "<p>I am also receiving this math domain error message with CP4.2.4 version when processing 3D images.  I tracked it down to the ConvertImagetoObjects module when the Preserve original labels is set to No.  The issue went away when setting preserve labels to yes.  However, this fix does not work on objects that were generated by the threshold module since they all get the same object number when preserve labels is set to yes.  Bug is definitely specific to version 4.2.4 since 4.2.1 works as intended and runs without error.</p>"], "77918": ["<p>Dear all,</p>\n<p>I would like to use CellProfiler segment membrane as what Harmony can do: shrink cells based on cytoplasm stain several pixels and select the shrinked pixel as membrane.</p>\n<p>With CellProfiler I can only use nuclei expand-N expand ring with for example 5 pixels, then identiy tertiary object from Cytoplasm minus ring to try to get membrane, however the results are not optimal.</p>\n<p>Do you have other recommendations?<br>\nThank you<br>\nShu</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a.png\" data-download-href=\"/uploads/short-url/qZbKpNBZYq0poAJQZ4jnuwxICRY.png?dl=1\" title=\"Bild1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a_2_578x500.png\" alt=\"Bild1\" data-base62-sha1=\"qZbKpNBZYq0poAJQZ4jnuwxICRY\" width=\"578\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a_2_578x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a_2_867x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/d/bd233e153a049dbcad399413ae73ac81cd79db5a.png 2x\" data-dominant-color=\"5B5E5C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Bild1</span><span class=\"informations\">1061\u00d7917 750 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Shu, similar to the Harmony you can use the cell objects to create a new object for the membrane, there is a module in CellProfiler under ObjectProcessing \u201cExpandOrShrinkObjects\u201d there you can use the cell objects shrink them the \u201c5 pixels\u201d, and then use the IdentifyTertiaryObjects just like you did for the cytoplasm but using he Cell minus the ShrunkCells that will create the Membrane object.</p>\n<p>Barbara</p>", "<p>Hi Barbara,</p>\n<p>thank you very much! It works perfect.<br>\nAnother question. For ring detection, I would like to define instead of a fest pixel number, 50% of the distance between Nuclei border to Cell border. Do you know method to do this?</p>\n<p>Best<br>\nShu</p>"], "63073": ["<p>Thanks so much, <a class=\"mention\" href=\"/u/dstirling\">@DStirling</a> ! I am having trouble finding the RunCellPose plugin anywhere on my system so that I can move it to my plugins directory specified in the CP preferences. I sucessefully updated CellProfiler to 4.2.1 and ran pip install cellpose. I am working in Ubuntu 16.04 LTS, and Python 3.8.12. Any help would be much appreciated!!</p>", "<p>Hi <a class=\"mention\" href=\"/u/rjk\">@rjk</a>,</p>\n<p>You\u2019ll need to download the plugin file from <a href=\"https://github.com/CellProfiler/CellProfiler-plugins/blob/1537dee3d44510164b7a3ac8a9b36ae6d671e7c6/runcellpose.py\">here</a> and drop it into your CellProfiler plugins directory.</p>", "<p>Hi <a class=\"mention\" href=\"/u/dstirling\">@DStirling</a></p>\n<p>First off thanks for everything you are doing for the community.</p>\n<p>I have downloaded the entire plugins folder from the github link above and set that folders as my plugins directory on CellProfiler.</p>\n<p>I can see multiple new plugins have appeared, but alas RunCellPose is not one of them.</p>\n<p>Any idea how i can fix this?</p>", "<p>I would advise against adding the entire folder as each plugin has different dependencies.</p>\n<p>Chances are something went wrong when loading the plugin. If you run from the command line do you see any error messages in the console?</p>", "<p>Hi David,</p>\n<p>Thanks for getting back to me, you\u2019re right, I can clearly see that the program cant find cellpose.</p>\n<pre><code class=\"lang-auto\">(base) alexrimmer@Alexs-MacBook-Pro ~ % pythonw -m cellprofiler\n11:19:04: Debug: Adding duplicate image handler for 'Windows bitmap file'\n11:19:04: Debug: Adding duplicate animation handler for '1' type\n11:19:04: Debug: Adding duplicate animation handler for '2' type\nMySQL could not be loaded.\nTraceback (most recent call last):\n  File \"/Users/alexrimmer/CellProfiler/cellprofiler/modules/exporttodatabase.py\", line 154, in &lt;module&gt;\n    import MySQLdb\n  File \"/Users/alexrimmer/opt/anaconda3/lib/python3.9/site-packages/MySQLdb/__init__.py\", line 18, in &lt;module&gt;\n    from . import _mysql\nImportError: dlopen(/Users/alexrimmer/opt/anaconda3/lib/python3.9/site-packages/MySQLdb/_mysql.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_mysql_affected_rows'\nCould not load runcellpose\nTraceback (most recent call last):\n  File \"/Users/alexrimmer/core/cellprofiler_core/utilities/core/modules/__init__.py\", line 71, in add_module\n    m = __import__(mod, globals(), locals(), [\"__all__\"], 0)\n  File \"/Users/alexrimmer/Desktop/Cellpose/runcellpose.py\", line 3, in &lt;module&gt;\n    from cellpose import models\nModuleNotFoundError: No module named 'cellpose'\ncould not load these modules: runcellpose\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:19:05.169 python[13089:364615] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:19:05.170 python[13089:364615] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:19:05.170 python[13089:364615] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:19:05.170 python[13089:364615] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:19:05.170 python[13089:364615] nil host used in call to allowsAnyHTTPSCertificateForHost:\n</code></pre>\n<p>If I move runcellpose.py to a new folder and set that to my default plugin folder i get a similar message:</p>\n<pre><code class=\"lang-auto\">(base) alexrimmer@Alexs-MacBook-Pro ~ % pythonw -m cellprofiler\n11:16:43: Debug: Adding duplicate image handler for 'Windows bitmap file'\n11:16:43: Debug: Adding duplicate animation handler for '1' type\n11:16:43: Debug: Adding duplicate animation handler for '2' type\nMySQL could not be loaded.\nTraceback (most recent call last):\n  File \"/Users/alexrimmer/CellProfiler/cellprofiler/modules/exporttodatabase.py\", line 154, in &lt;module&gt;\n    import MySQLdb\n  File \"/Users/alexrimmer/opt/anaconda3/lib/python3.9/site-packages/MySQLdb/__init__.py\", line 18, in &lt;module&gt;\n    from . import _mysql\nImportError: dlopen(/Users/alexrimmer/opt/anaconda3/lib/python3.9/site-packages/MySQLdb/_mysql.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_mysql_affected_rows'\nCould not load runcellpose\nTraceback (most recent call last):\n  File \"/Users/alexrimmer/core/cellprofiler_core/utilities/core/modules/__init__.py\", line 71, in add_module\n    m = __import__(mod, globals(), locals(), [\"__all__\"], 0)\n  File \"/Users/alexrimmer/Desktop/Cellpose/runcellpose.py\", line 3, in &lt;module&gt;\n    from cellpose import models\nModuleNotFoundError: No module named 'cellpose'\ncould not load these modules: runcellpose\n2022-02-10 11:16:44.765 python[13059:363252] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:16:44.765 python[13059:363252] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:16:44.767 python[13059:363252] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:16:44.767 python[13059:363252] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:16:44.767 python[13059:363252] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:16:44.767 python[13059:363252] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:16:44.767 python[13059:363252] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:16:44.767 python[13059:363252] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:16:44.767 python[13059:363252] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:16:44.767 python[13059:363252] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:16:44.768 python[13059:363252] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:16:44.768 python[13059:363252] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:16:44.768 python[13059:363252] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:16:44.768 python[13059:363252] nil host used in call to allowsAnyHTTPSCertificateForHost:\n2022-02-10 11:16:44.768 python[13059:363252] nil host used in call to allowsSpecificHTTPSCertificateForHost\n2022-02-10 11:16:44.768 python[13059:363252] nil host used in call to allowsAnyHTTPSCertificateForHost:\nExiting the pipeline validation thread\n</code></pre>\n<p>Any idea what I should be doing differently?</p>\n<p>Thanks!</p>", "<p>It looks like you\u2019re running Python 3.9, which isn\u2019t fully supported. Switching to Python 3.8 may help here, but more importantly it doesn\u2019t look like Cellpose is installed. Could you try running <code>pythonw -m pip freeze</code> to check whether Cellpose is present?</p>", "<p>Seems to be installed?</p>\n<pre><code class=\"lang-auto\">(base) alexrimmer@Alexs-MacBook-Pro ~ % pythonw -m pip freeze\nabsl-py==1.0.0\nalabaster @ file:///home/ktietz/src/ci/alabaster_1611921544520/work\nanaconda-client @ file:///opt/concourse/worker/volumes/live/866d4dd0-ff5b-4d0b-718d-0267a3b10e06/volume/anaconda-client_1635342573767/work\nanaconda-navigator==2.1.1\nanaconda-project @ file:///tmp/build/80754af9/anaconda-project_1626085644852/work\nanyio @ file:///opt/concourse/worker/volumes/live/96440bbe-d2f1-4a9e-5edf-600248ff38bd/volume/anyio_1617783321037/work/dist\nappdirs==1.4.4\napplaunchservices @ file:///Users/ktietz/demo/mc3/conda-bld/applaunchservices_1630511705208/work\nappnope @ file:///opt/concourse/worker/volumes/live/6ca6f098-d773-4461-5c91-a24a17435bda/volume/appnope_1606859448531/work\nappscript @ file:///opt/concourse/worker/volumes/live/00049ed6-6263-4a6e-72b9-9d990f6e2f07/volume/appscript_1611427000595/work\nargh==0.26.2\nargon2-cffi @ file:///opt/concourse/worker/volumes/live/38e8fb2b-1295-4bdf-4adf-b20acbe4d91b/volume/argon2-cffi_1607022498041/work\narrow @ file:///opt/concourse/worker/volumes/live/1c202787-83f7-4b70-6d98-b40769f597f4/volume/arrow_1617737667847/work\nasn1crypto @ file:///tmp/build/80754af9/asn1crypto_1596577642040/work\nastroid @ file:///opt/concourse/worker/volumes/live/5aff3c6b-d8ac-4e74-4846-0f446794397d/volume/astroid_1628063157520/work\nastropy @ file:///opt/concourse/worker/volumes/live/dac790a5-ee97-4520-5b55-f2cc50d275e6/volume/astropy_1629829220593/work\nastunparse==1.6.3\nasync-generator @ file:///home/ktietz/src/ci/async_generator_1611927993394/work\natomicwrites==1.4.0\nattrs @ file:///tmp/build/80754af9/attrs_1620827162558/work\nautopep8 @ file:///tmp/build/80754af9/autopep8_1620866417880/work\nBabel @ file:///tmp/build/80754af9/babel_1620871417480/work\nbackcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\nbackports.functools-lru-cache @ file:///tmp/build/80754af9/backports.functools_lru_cache_1618170165463/work\nbackports.shutil-get-terminal-size @ file:///tmp/build/80754af9/backports.shutil_get_terminal_size_1608222128777/work\nbackports.tempfile @ file:///home/linux1/recipes/ci/backports.tempfile_1610991236607/work\nbackports.weakref==1.0.post1\nbeautifulsoup4 @ file:///tmp/build/80754af9/beautifulsoup4_1631874778482/work\nbinaryornot @ file:///tmp/build/80754af9/binaryornot_1617751525010/work\nbitarray @ file:///opt/concourse/worker/volumes/live/8f2b2fa2-f7cd-4343-4d8f-3cc38b632e33/volume/bitarray_1629132852828/work\nbkcharts==0.2\nblack==19.10b0\nbleach @ file:///tmp/build/80754af9/bleach_1628110601003/work\nbokeh @ file:///opt/concourse/worker/volumes/live/278130e0-6bd5-4375-72c8-87158eba9408/volume/bokeh_1635324480391/work\nboto==2.49.0\nboto3==1.20.47\nbotocore==1.23.47\nBottleneck @ file:///opt/concourse/worker/volumes/live/ac8c8ef3-2ed0-42e9-6ec0-5bb05ad938f6/volume/bottleneck_1607575111469/work\nbrotlipy==0.7.0\ncached-property @ file:///tmp/build/80754af9/cached-property_1600785575025/work\ncachetools==5.0.0\ncellh5==1.3.0\ncellpose==1.0.0\n-e git+https://github.com/CellProfiler/CellProfiler.git@6dee40ca804806731e2c3d0a16dd1675a4b7687a#egg=CellProfiler\n-e git+https://github.com/CellProfiler/core.git@7eb9630f2bac957c626c69f030823f59bd79ae4f#egg=cellprofiler_core\ncentrosome==1.2.0\ncertifi==2021.10.8\ncffi @ file:///opt/concourse/worker/volumes/live/976f8942-f51d-4f0e-7352-2a10f0820d0e/volume/cffi_1625814703974/work\nchardet @ file:///opt/concourse/worker/volumes/live/7e1102c4-8702-40f2-63d6-f260ce5f85e4/volume/chardet_1607706831384/work\ncharset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work\nclick==8.0.3\ncloudpickle @ file:///tmp/build/80754af9/cloudpickle_1632508026186/work\nclyent==1.2.2\ncolorama @ file:///tmp/build/80754af9/colorama_1607707115595/work\nconda==4.11.0\nconda-build==3.21.5\nconda-content-trust @ file:///tmp/build/80754af9/conda-content-trust_1617045594566/work\nconda-pack @ file:///tmp/build/80754af9/conda-pack_1611163042455/work\nconda-package-handling @ file:///opt/concourse/worker/volumes/live/8fb3e065-760b-4a9d-4cd9-aca7fc8baf53/volume/conda-package-handling_1618262145611/work\nconda-repo-cli @ file:///tmp/build/80754af9/conda-repo-cli_1620168426516/work\nconda-token @ file:///tmp/build/80754af9/conda-token_1620076980546/work\nconda-verify==3.4.2\ncontextlib2 @ file:///Users/ktietz/demo/mc3/conda-bld/contextlib2_1630668244042/work\ncookiecutter @ file:///tmp/build/80754af9/cookiecutter_1617748928239/work\ncryptography @ file:///opt/concourse/worker/volumes/live/3143e751-d0f4-457e-7dc5-b7eaa48a56a8/volume/cryptography_1633520383659/work\ncycler==0.10.0\nCython @ file:///opt/concourse/worker/volumes/live/090b3344-25bd-4e30-5dde-5f77abae4b7a/volume/cython_1636035875931/work\ncytoolz==0.11.0\ndaal4py==2021.3.0\ndask==2021.10.0\ndebugpy @ file:///opt/concourse/worker/volumes/live/1a15daf1-2a67-4a51-6aa1-c3bca01f7577/volume/debugpy_1629222706040/work\ndecorator @ file:///tmp/build/80754af9/decorator_1632776554403/work\ndefusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\ndeprecation==2.1.0\ndiff-match-patch @ file:///Users/ktietz/demo/mc3/conda-bld/diff-match-patch_1630511840874/work\ndistributed @ file:///opt/concourse/worker/volumes/live/34c9adba-e95e-473d-7a32-ecdf958a8844/volume/distributed_1635968220957/work\ndocopt==0.6.2\ndocutils==0.15.2\nentrypoints==0.3\net-xmlfile==1.1.0\nfastcache @ file:///opt/concourse/worker/volumes/live/8356601f-d9bc-4d13-4017-1a58ebea6849/volume/fastcache_1607571270986/work\nfastremap==1.12.2\nfilelock @ file:///tmp/build/80754af9/filelock_1635402558181/work\nflake8 @ file:///tmp/build/80754af9/flake8_1620776156532/work\nFlask @ file:///home/ktietz/src/ci/flask_1611932660458/work\nflatbuffers==2.0\nfonttools==4.25.0\nfsspec @ file:///tmp/build/80754af9/fsspec_1632413898837/work\nfuture @ file:///opt/concourse/worker/volumes/live/f456638c-86a7-4060-7f5f-d499a051219b/volume/future_1607571337593/work\ngast==0.5.3\ngevent @ file:///opt/concourse/worker/volumes/live/014e1366-b455-480b-61a8-9a69e4909791/volume/gevent_1628273687151/work\nglob2 @ file:///home/linux1/recipes/ci/glob2_1610991677669/work\ngmpy2==2.0.8\ngoogle-auth==2.6.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngreenlet @ file:///opt/concourse/worker/volumes/live/b27b4e9e-4697-4d57-403b-f82d36a391ca/volume/greenlet_1628888146890/work\ngrpcio==1.43.0\nh5py @ file:///opt/concourse/worker/volumes/live/e7503571-c7b1-45ac-4bb7-37b5178cb0df/volume/h5py_1622088436205/work\nHeapDict @ file:///Users/ktietz/demo/mc3/conda-bld/heapdict_1630598515714/work\nhmmlearn==0.2.6\nhtml5lib @ file:///Users/ktietz/demo/mc3/conda-bld/html5lib_1629144453894/work\nidna @ file:///tmp/build/80754af9/idna_1622654382723/work\nimagecodecs @ file:///opt/concourse/worker/volumes/live/ab9fe69e-f7d4-471a-6199-cb95a745fb88/volume/imagecodecs_1635529117386/work\nimageio @ file:///tmp/build/80754af9/imageio_1617700267927/work\nimagesize @ file:///Users/ktietz/demo/mc3/conda-bld/imagesize_1628863108022/work\nimglyb==2.0.0\nimportlib-metadata @ file:///opt/concourse/worker/volumes/live/b1c84e32-1519-4554-50d3-980dc7c220d9/volume/importlib-metadata_1631916711680/work\ninflect==5.4.0\ninflection==0.5.1\niniconfig @ file:///home/linux1/recipes/ci/iniconfig_1610983019677/work\nintervaltree @ file:///Users/ktietz/demo/mc3/conda-bld/intervaltree_1630511889664/work\nipykernel @ file:///opt/concourse/worker/volumes/live/6f6caad4-5c02-4b4a-5243-1eece346c27b/volume/ipykernel_1633545433252/work/dist/ipykernel-6.4.1-py3-none-any.whl\nipython @ file:///opt/concourse/worker/volumes/live/c0526798-0817-46b3-68c1-bb9ffefe344a/volume/ipython_1635944197798/work\nipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\nipywidgets @ file:///tmp/build/80754af9/ipywidgets_1634143127070/work\nisort @ file:///tmp/build/80754af9/isort_1628603791788/work\nitsdangerous @ file:///tmp/build/80754af9/itsdangerous_1621432558163/work\njavabridge==1.0.16\njdcal @ file:///Users/ktietz/demo/mc3/conda-bld/jdcal_1630584345063/work\njedi @ file:///opt/concourse/worker/volumes/live/4b03e428-2d0c-4635-502c-16df884971e8/volume/jedi_1611333763457/work\njgo==1.0.3\nJinja2 @ file:///tmp/build/80754af9/jinja2_1612213139570/work\njinja2-time @ file:///tmp/build/80754af9/jinja2-time_1617751524098/work\njmespath==0.10.0\njoblib @ file:///tmp/build/80754af9/joblib_1635411271373/work\nJPype1==1.3.0\nJs2Py==0.71\njson5 @ file:///tmp/build/80754af9/json5_1624432770122/work\njsonschema @ file:///Users/ktietz/demo/mc3/conda-bld/jsonschema_1630511932244/work\njupyter @ file:///opt/concourse/worker/volumes/live/5d55c245-fcac-42f9-4a6c-7e147c07785b/volume/jupyter_1607700866889/work\njupyter-client @ file:///tmp/build/80754af9/jupyter_client_1616770841739/work\njupyter-console @ file:///tmp/build/80754af9/jupyter_console_1616615302928/work\njupyter-core @ file:///opt/concourse/worker/volumes/live/ade0d1fb-3680-4893-7a79-579c9d86c1c1/volume/jupyter_core_1633420119353/work\njupyter-server @ file:///opt/concourse/worker/volumes/live/c0b6c5cd-8b5f-482c-6789-42a64b3d2acc/volume/jupyter_server_1616084049292/work\njupyterlab @ file:///tmp/build/80754af9/jupyterlab_1635799997693/work\njupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\njupyterlab-server @ file:///tmp/build/80754af9/jupyterlab_server_1633419203660/work\njupyterlab-widgets @ file:///tmp/build/80754af9/jupyterlab_widgets_1609884341231/work\nkeras==2.8.0\nKeras-Preprocessing==1.1.2\nkeyring @ file:///opt/concourse/worker/volumes/live/effa77ff-08c0-456d-548e-ecd9681ace1e/volume/keyring_1629321568005/work\nkiwisolver @ file:///opt/concourse/worker/volumes/live/f8867fdb-2fa2-4145-73d9-4d6f6dad5f7c/volume/kiwisolver_1612282424136/work\nlazy-object-proxy @ file:///opt/concourse/worker/volumes/live/a169db40-97bf-4f51-6893-dc751f705b7b/volume/lazy-object-proxy_1616529067444/work\nlibarchive-c @ file:///tmp/build/80754af9/python-libarchive-c_1617780486945/work\nlibclang==13.0.0\nllvmlite==0.37.0\nlocket==0.2.1\nlxml @ file:///opt/concourse/worker/volumes/live/e9a191ad-c8f1-4bb7-7955-c333b8ab6c55/volume/lxml_1616443232489/work\nmahotas==1.4.12\nMarkdown==3.3.6\nMarkupSafe @ file:///opt/concourse/worker/volumes/live/fac44ebc-60de-4182-7f89-29bb8796c554/volume/markupsafe_1607027351541/work\nmatplotlib==3.1.3\nmatplotlib-inline @ file:///tmp/build/80754af9/matplotlib-inline_1628242447089/work\nmccabe==0.6.1\nmicroscopeimagequality==0.1.0.dev5\nmistune @ file:///opt/concourse/worker/volumes/live/4217afd5-dad1-438d-6f79-e4992ccda0e5/volume/mistune_1607364880245/work\nmkl-fft==1.3.1\nmkl-random @ file:///opt/concourse/worker/volumes/live/0cda23d8-7460-44b2-7e5d-3c76a8a0ca7e/volume/mkl_random_1626186083266/work\nmkl-service==2.4.0\nmock @ file:///tmp/build/80754af9/mock_1607622725907/work\nmore-itertools @ file:///tmp/build/80754af9/more-itertools_1635423142362/work\nmpmath==1.2.1\nmsgpack @ file:///opt/concourse/worker/volumes/live/ccdd6ca0-523a-4fde-5a76-cdd4f47c445e/volume/msgpack-python_1612287158191/work\nmultipledispatch @ file:///opt/concourse/worker/volumes/live/ae29ad0f-3a64-4ff5-7393-0aa95f2c9f85/volume/multipledispatch_1607574242710/work\nmunkres==1.1.4\nmypy-extensions==0.4.3\nmysqlclient==1.4.6\nnatsort==8.1.0\nnavigator-updater==0.2.1\nnbclassic @ file:///tmp/build/80754af9/nbclassic_1616085367084/work\nnbclient @ file:///tmp/build/80754af9/nbclient_1614364831625/work\nnbconvert @ file:///opt/concourse/worker/volumes/live/ad745f9a-647c-4095-6b46-a8b45a735914/volume/nbconvert_1624479072790/work\nnbformat @ file:///tmp/build/80754af9/nbformat_1617383369282/work\nnest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1613680548246/work\nnetworkx @ file:///tmp/build/80754af9/networkx_1633639043937/work\nnltk==3.6.5\nnose @ file:///tmp/build/80754af9/nose_1606773131901/work\nnotebook @ file:///opt/concourse/worker/volumes/live/600e39a3-70b3-4ed1-4986-4cc3d1a9be7c/volume/notebook_1635411664246/work\nnumba @ file:///Users/builder/miniconda3/envs/prefect/conda-bld/numba_1635176853722/work\nnumexpr @ file:///opt/concourse/worker/volumes/live/8a490d79-0e07-4fed-40fc-a78f896e4811/volume/numexpr_1618856522733/work\nnumpy==1.22.2\nnumpydoc @ file:///tmp/build/80754af9/numpydoc_1605117425582/work\noauthlib==3.2.0\nolefile @ file:///Users/ktietz/demo/mc3/conda-bld/olefile_1629805411829/work\nopencv-python-headless==4.5.5.62\nopenpyxl @ file:///tmp/build/80754af9/openpyxl_1632777717936/work\nopt-einsum==3.3.0\npackaging @ file:///tmp/build/80754af9/packaging_1625611678980/work\npandas==1.3.4\npandocfilters @ file:///opt/concourse/worker/volumes/live/d8ef4635-066d-4ffe-5341-12ebf01bd094/volume/pandocfilters_1605120459573/work\nparso @ file:///tmp/build/80754af9/parso_1617223946239/work\npartd @ file:///tmp/build/80754af9/partd_1618000087440/work\npath @ file:///opt/concourse/worker/volumes/live/ada87e48-bd34-49ac-6489-f51a7259db05/volume/path_1623603892072/work\npathlib2 @ file:///opt/concourse/worker/volumes/live/9cb7a8e6-768d-4b83-4c58-1af1d6394b3e/volume/pathlib2_1625585698767/work\npathspec==0.7.0\npatsy==0.5.2\npep8==1.7.1\npexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\npickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\nPillow==8.4.0\npipwin==0.5.1\npkginfo==1.7.1\npluggy @ file:///opt/concourse/worker/volumes/live/a2630e20-8422-46fc-7347-f73294368851/volume/pluggy_1615976601840/work\nply==3.11\npoyo @ file:///tmp/build/80754af9/poyo_1617751526755/work\nprokaryote==2.4.4\nprometheus-client @ file:///tmp/build/80754af9/prometheus_client_1623189609245/work\nprompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1633440160888/work\nprotobuf==3.19.4\npsutil @ file:///opt/concourse/worker/volumes/live/da41f1b1-060b-47fa-4c17-557e069ead1d/volume/psutil_1612298011002/work\nptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\npy @ file:///tmp/build/80754af9/py_1607971587848/work\npyasn1==0.4.8\npyasn1-modules==0.2.8\npycodestyle @ file:///tmp/build/80754af9/pycodestyle_1615748559966/work\npycosat==0.6.3\npycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\npycurl==7.44.1\npydocstyle @ file:///tmp/build/80754af9/pydocstyle_1621600989141/work\npyerfa @ file:///opt/concourse/worker/volumes/live/fef1f482-1dec-42b9-4439-b8031d24ea69/volume/pyerfa_1621560786048/work\npyflakes @ file:///tmp/build/80754af9/pyflakes_1617200973297/work\nPygments @ file:///tmp/build/80754af9/pygments_1629234116488/work\npyimagej==1.0.2\npyjsparser==2.7.1\nPyJWT @ file:///opt/concourse/worker/volumes/live/bd094316-1935-4bf0-5028-b889d4e7967c/volume/pyjwt_1619682501859/work\npylint @ file:///opt/concourse/worker/volumes/live/110f293a-445f-477b-4041-2e00ae784083/volume/pylint_1627536795404/work\npyls-spyder==0.4.0\npyodbc===4.0.0-unsupported\npyOpenSSL @ file:///tmp/build/80754af9/pyopenssl_1635333100036/work\npyparsing @ file:///tmp/build/80754af9/pyparsing_1635766073266/work\nPyPrind==2.11.3\npyrsistent @ file:///opt/concourse/worker/volumes/live/76cffa60-bd33-4155-4e83-ea03c38b1294/volume/pyrsistent_1636111020441/work\npySmartDL==1.3.4\nPySocks @ file:///opt/concourse/worker/volumes/live/112288ac-9cb0-4e73-768b-13baf4ca6419/volume/pysocks_1605305820043/work\npytest==6.2.4\npython-bioformats==4.0.5\npython-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\npython-javabridge==4.0.3\npython-lsp-black @ file:///tmp/build/80754af9/python-lsp-black_1634232156041/work\npython-lsp-jsonrpc==1.0.0\npython-lsp-server==1.2.4\npython-slugify @ file:///tmp/build/80754af9/python-slugify_1620405669636/work\npytz==2021.3\npytz-deprecation-shim==0.1.0.post0\nPyWavelets @ file:///opt/concourse/worker/volumes/live/3cbb6155-7383-45e0-55bb-8641a92939f6/volume/pywavelets_1607645526758/work\nPyYAML==6.0\npyzmq==18.0.1\nQDarkStyle @ file:///tmp/build/80754af9/qdarkstyle_1617386714626/work\nqstylizer @ file:///tmp/build/80754af9/qstylizer_1617713584600/work/dist/qstylizer-0.1.10-py2.py3-none-any.whl\nQtAwesome @ file:///tmp/build/80754af9/qtawesome_1615991616277/work\nqtconsole @ file:///tmp/build/80754af9/qtconsole_1632739723211/work\nQtPy @ file:///tmp/build/80754af9/qtpy_1629397026935/work\nregex @ file:///opt/concourse/worker/volumes/live/6d0c2188-eef5-45ba-7cde-604a70bbd9f2/volume/regex_1628063357190/work\nrequests @ file:///tmp/build/80754af9/requests_1629994808627/work\nrequests-oauthlib==1.3.1\nrope @ file:///tmp/build/80754af9/rope_1623703006312/work\nrsa==4.8\nRtree @ file:///opt/concourse/worker/volumes/live/18283f9b-719e-45b1-7f3a-937f49358be4/volume/rtree_1618420836397/work\nruamel-yaml-conda @ file:///opt/concourse/worker/volumes/live/e81cf0fe-611a-498e-6e69-a7320057c1ac/volume/ruamel_yaml_1616016689696/work\ns3transfer==0.5.1\nscikit-image==0.18.3\nscikit-learn @ file:///opt/concourse/worker/volumes/live/83f46a43-da2a-467b-5581-ed2cbd128684/volume/scikit-learn_1621370403088/work\nscikit-learn-intelex==2021.20210714.100439\nscipy @ file:///opt/concourse/worker/volumes/live/8e481b15-67b8-4160-7a8b-26cd6e8fe54f/volume/scipy_1630606804826/work\nscyjava==1.4.1\nseaborn @ file:///tmp/build/80754af9/seaborn_1629307859561/work\nSend2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work\nsentry-sdk==0.18.0\nsimplegeneric==0.8.1\nsingledispatch @ file:///tmp/build/80754af9/singledispatch_1629321204894/work\nsip==4.19.13\nsix @ file:///tmp/build/80754af9/six_1623709665295/work\nsniffio @ file:///opt/concourse/worker/volumes/live/38ca9e9e-09d1-4d43-5a0f-b546422e7807/volume/sniffio_1614030472707/work\nsnowballstemmer @ file:///tmp/build/80754af9/snowballstemmer_1611258885636/work\nsortedcollections @ file:///tmp/build/80754af9/sortedcollections_1611172717284/work\nsortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work\nsoupsieve @ file:///tmp/build/80754af9/soupsieve_1616183228191/work\nSphinx==4.2.0\nsphinxcontrib-applehelp @ file:///home/ktietz/src/ci/sphinxcontrib-applehelp_1611920841464/work\nsphinxcontrib-devhelp @ file:///home/ktietz/src/ci/sphinxcontrib-devhelp_1611920923094/work\nsphinxcontrib-htmlhelp @ file:///tmp/build/80754af9/sphinxcontrib-htmlhelp_1623945626792/work\nsphinxcontrib-jsmath @ file:///home/ktietz/src/ci/sphinxcontrib-jsmath_1611920942228/work\nsphinxcontrib-qthelp @ file:///home/ktietz/src/ci/sphinxcontrib-qthelp_1611921055322/work\nsphinxcontrib-serializinghtml @ file:///tmp/build/80754af9/sphinxcontrib-serializinghtml_1624451540180/work\nsphinxcontrib-websupport @ file:///tmp/build/80754af9/sphinxcontrib-websupport_1597081412696/work\nspyder @ file:///opt/concourse/worker/volumes/live/5ce630f9-babe-4f53-755a-cd7324c300d9/volume/spyder_1636480249527/work\nspyder-kernels @ file:///opt/concourse/worker/volumes/live/72f0c5be-8b9e-43d2-67e5-f038915937d7/volume/spyder-kernels_1634236950410/work\nSQLAlchemy @ file:///opt/concourse/worker/volumes/live/454d700c-0b8b-469e-7add-023d0df6ee3d/volume/sqlalchemy_1626948440911/work\nstatsmodels @ file:///opt/concourse/worker/volumes/live/237cf070-cf87-4ff8-6149-8213b5eb40a5/volume/statsmodels_1614023802257/work\nsympy @ file:///opt/concourse/worker/volumes/live/5ed26665-a223-4942-7dc1-66b2b3830ab1/volume/sympy_1635237064989/work\ntables @ file:///opt/concourse/worker/volumes/live/daa73f70-754b-4f28-73ce-6f96c40f4b9d/volume/pytables_1607975400838/work\nTBB==0.2\ntblib @ file:///Users/ktietz/demo/mc3/conda-bld/tblib_1629402031467/work\ntensorboard==2.8.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.8.0\ntensorflow-io-gcs-filesystem==0.24.0\ntermcolor==1.1.0\nterminado==0.9.4\ntestpath @ file:///tmp/build/80754af9/testpath_1624638946665/work\ntext-unidecode @ file:///Users/ktietz/demo/mc3/conda-bld/text-unidecode_1629401354553/work\ntextdistance @ file:///tmp/build/80754af9/textdistance_1612461398012/work\ntf-estimator-nightly==2.8.0.dev2021122109\nthreadpoolctl @ file:///Users/ktietz/demo/mc3/conda-bld/threadpoolctl_1629802263681/work\nthree-merge @ file:///tmp/build/80754af9/three-merge_1607553261110/work\ntifffile @ file:///tmp/build/80754af9/tifffile_1627275862826/work\ntinycss @ file:///tmp/build/80754af9/tinycss_1617713798712/work\ntoml @ file:///tmp/build/80754af9/toml_1616166611790/work\ntoolz @ file:///home/linux1/recipes/ci/toolz_1610987900194/work\ntorch==1.10.2\ntornado @ file:///opt/concourse/worker/volumes/live/2c1a63a2-006b-48ee-56b9-0cfe8b4927f9/volume/tornado_1606942321278/work\ntqdm @ file:///tmp/build/80754af9/tqdm_1635330843403/work\ntraitlets @ file:///tmp/build/80754af9/traitlets_1632522747050/work\ntyped-ast @ file:///opt/concourse/worker/volumes/live/c2210c24-bb2d-4474-77f5-1bc9bf1cb79f/volume/typed-ast_1624953683856/work\ntyping-extensions @ file:///tmp/build/80754af9/typing_extensions_1631814937681/work\ntzdata==2021.5\ntzlocal==4.1\nujson @ file:///opt/concourse/worker/volumes/live/258fa87e-ec76-445c-5f9c-fc9523993cd7/volume/ujson_1611259511951/work\nunicodecsv==0.14.1\nUnidecode @ file:///tmp/build/80754af9/unidecode_1614712377438/work\nurllib3==1.26.7\nwatchdog @ file:///opt/concourse/worker/volumes/live/3aa0e7d9-c795-4854-41fd-f0b2492c886a/volume/watchdog_1624955010200/work\nwcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\nwebencodings==0.5.1\nWerkzeug @ file:///tmp/build/80754af9/werkzeug_1635505089296/work\nwhichcraft @ file:///tmp/build/80754af9/whichcraft_1617751293875/work\nwidgetsnbextension @ file:///opt/concourse/worker/volumes/live/42762d5a-6520-4b06-68a2-619dfed6a007/volume/widgetsnbextension_1607531500933/work\nwrapt @ file:///opt/concourse/worker/volumes/live/1381ca3b-f984-4f5c-4f46-a67ca4eeffa5/volume/wrapt_1607574524589/work\nwurlitzer @ file:///opt/concourse/worker/volumes/live/f0d9880a-118b-4ba3-7b92-3c5bb0f6c00b/volume/wurlitzer_1626947802781/work\nwxPython==4.1.0\nxarray==0.21.1\nxlrd @ file:///tmp/build/80754af9/xlrd_1608072521494/work\nXlsxWriter @ file:///tmp/build/80754af9/xlsxwriter_1628603415431/work\nxlwings==0.24.9\nxlwt==1.3.0\nxmltodict @ file:///Users/ktietz/demo/mc3/conda-bld/xmltodict_1629301980723/work\nyapf @ file:///tmp/build/80754af9/yapf_1615749224965/work\nzict==2.0.0\nzipp @ file:///tmp/build/80754af9/zipp_1633618647012/work\nzope.event==4.5.0\nzope.interface @ file:///opt/concourse/worker/volumes/live/8c2d4bd1-6406-47f0-45ce-890992bafbf6/volume/zope.interface_1625036159007/work\n(base) alexrimmer@Alexs-MacBook-Pro ~ % \n</code></pre>", "<p>That is really weird. It might be an issue with Python 3.9 compatibility, otherwise you should try <code>cellpose==0.7.2</code></p>", "<p>Ok, so still having an issue using runcellpose. It appears that there might be a back compatability issue between CellProfiler 4.2.0 and cellpose. I get \u201cImportError: Numba needs Numpy 1.21 or less\u201d.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/5/e550a07030b3e7717eed4cb1e5048af5aeed65d7.jpeg\" data-download-href=\"/uploads/short-url/wIC57LI7EQdx4nlC639BlgqMriL.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/5/e550a07030b3e7717eed4cb1e5048af5aeed65d7_2_690x199.jpeg\" alt=\"image\" data-base62-sha1=\"wIC57LI7EQdx4nlC639BlgqMriL\" width=\"690\" height=\"199\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/5/e550a07030b3e7717eed4cb1e5048af5aeed65d7_2_690x199.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/5/e550a07030b3e7717eed4cb1e5048af5aeed65d7_2_1035x298.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/5/e550a07030b3e7717eed4cb1e5048af5aeed65d7_2_1380x398.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/5/e550a07030b3e7717eed4cb1e5048af5aeed65d7_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1736\u00d7502 109 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>If I then downgrade to Numpy 1.21, CellProfiler will not open from terminal and I get this error \u201cnumpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/d/fd781cbf0df1a976188a85c5a23d72556b827838.jpeg\" data-download-href=\"/uploads/short-url/Aai8DVmTIunLj5MlDAStHj6xz1S.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd781cbf0df1a976188a85c5a23d72556b827838_2_690x248.jpeg\" alt=\"image\" data-base62-sha1=\"Aai8DVmTIunLj5MlDAStHj6xz1S\" width=\"690\" height=\"248\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd781cbf0df1a976188a85c5a23d72556b827838_2_690x248.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd781cbf0df1a976188a85c5a23d72556b827838_2_1035x372.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd781cbf0df1a976188a85c5a23d72556b827838_2_1380x496.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/d/fd781cbf0df1a976188a85c5a23d72556b827838_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1738\u00d7626 135 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>You may need to reinstall javabridge as it has to be built against a specific version of numpy.</p>", "<p>Thanks for the suggestion, <a class=\"mention\" href=\"/u/dstirling\">@DStirling</a>! I tried this without success, but I might be missing something. I reinstalled javabridge and then reinstalled cellpose, but I am still having the same problem. Are there any details I can provide that would make the current issues clearer? Thanks for all your help!</p>", "<p>I think the best thing we could try is making a fresh virtual environment, installing CellPose, then installing CellProfiler?</p>", "<p>Sorry it\u2019s been a while, another command to try:</p>\n<p>First ensure you have the final desired version of numpy installed, then run<br>\n<code>pip install javabridge --no-cache-dir --no-build-isolation</code></p>\n<p>You may need to also do the same with the centrosome package</p>", "<p>Hello, I was wondering if you managed to install cellpose. I\u2019m having the same problem: I need to downgrade to numpy 1.21 for numba to work but then CellProfiler does not open. Any suggestions?</p>", "<p>You probably need to re-install python javabridge - this has worked in the past in our hands (the last two shouldn\u2019t be necessary if you already have cellprofiler and cellpose pip installed)</p>\n<pre><code class=\"lang-auto\">pip install numpy==1.21\npip install --force --no-deps python-javabridge\npip install cellprofiler\npip install cellpose\n</code></pre>", "<p>Hi there,</p>\n<p>Unfortunately, the problem persists after javabridge installation.</p>", "<p>Thank you <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> for the suggestion. Unfortunately, still not working. See the error below.</p>\n<p>Traceback (most recent call last):<br>\nFile \u201cc:\\users\\esperanza\\documents\\github\\cellprofiler\\cellprofiler\\gui\\app.py\u201d, line 69, in OnInit<br>\nfrom .cpframe import CPFrame<br>\nFile \u201cc:\\users\\esperanza\\documents\\github\\cellprofiler\\cellprofiler\\gui\\cpframe.py\u201d, line 25, in <br>\nfrom .<em>workspace_model import Workspace<br>\nFile \u201cc:\\users\\esperanza\\documents\\github\\cellprofiler\\cellprofiler\\gui_workspace_model.py\u201d, line 8, in <br>\nimport cellprofiler.gui.figure<br>\nFile \"c:\\users\\esperanza\\documents\\github\\cellprofiler\\cellprofiler\\gui\\figure_<em>init</em></em>.py\", line 1, in <br>\nfrom ._figure import Figure<br>\nFile \u201cc:\\users\\esperanza\\documents\\github\\cellprofiler\\cellprofiler\\gui\\figure_figure.py\u201d, line 8, in <br>\nimport centrosome.cpmorphology<br>\nFile \u201cC:\\Users\\Esperanza\\My Documents\\python venv\\cellpose\\lib\\site-packages\\centrosome\\cpmorphology.py\u201d, line 12, in <br>\nfrom ._cpmorphology2 import skeletonize_loop, table_lookup_index<br>\nFile \u201ccentrosome_cpmorphology2.pyx\u201d, line 1, in init centrosome._cpmorphology2<br>\nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject<br>\nOnInit returned false, exiting\u2026</p>", "<p>Ah, same theory. Just add centrosome onto the list of things you\u2019re force-reinstalling after setting your numpy version!</p>\n<pre><code class=\"lang-auto\">pip install numpy==1.21\npip install --force --no-deps python-javabridge centrosome\npip install cellprofiler\npip install cellpose\n</code></pre>", "<p>Thanks! Alas, same story.</p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"c:\\users\\radoslawgrochowski\\documents\\github\\cellprofiler\\cellprofiler\\gui\\app.py\", line 69, in OnInit\n    from .cpframe import CPFrame\n  File \"c:\\users\\radoslawgrochowski\\documents\\github\\cellprofiler\\cellprofiler\\gui\\cpframe.py\", line 25, in &lt;module&gt;\n    from ._workspace_model import Workspace\n  File \"c:\\users\\radoslawgrochowski\\documents\\github\\cellprofiler\\cellprofiler\\gui\\_workspace_model.py\", line 8, in &lt;module&gt;\n    import cellprofiler.gui.figure\n  File \"c:\\users\\radoslawgrochowski\\documents\\github\\cellprofiler\\cellprofiler\\gui\\figure\\__init__.py\", line 1, in &lt;module&gt;\n    from ._figure import Figure\n  File \"c:\\users\\radoslawgrochowski\\documents\\github\\cellprofiler\\cellprofiler\\gui\\figure\\_figure.py\", line 8, in &lt;module&gt;\n    import centrosome.cpmorphology\n  File \"C:\\ProgramData\\Anaconda3\\envs\\cellprofiler_pose\\lib\\site-packages\\centrosome\\cpmorphology.py\", line 12, in &lt;module&gt;\n    from ._cpmorphology2 import skeletonize_loop, table_lookup_index\n  File \"centrosome\\_cpmorphology2.pyx\", line 1, in init centrosome._cpmorphology2\nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\nOnInit returned false, exiting...\n</code></pre>\n<pre><code class=\"lang-auto\">pip freeze\n\nboto3==1.21.42\nbotocore==1.24.42\ncellpose==2.0.4\n-e CellProfiler==4.2.1\n-e cellprofiler-core==4.2.1\ncentrosome==1.2.0\ncertifi==2021.10.8\ncharset-normalizer==2.0.12\ncolorama==0.4.4\ncycler @ file:///tmp/build/80754af9/cycler_1637851556182/work\nCython==0.29.28\ndeprecation==2.1.0\ndocutils==0.15.2\nfastremap==1.12.2\nfuture==0.18.2\nh5py==3.6.0\nidna==3.3\nimageio==2.16.2\ninflect==5.5.2\nJinja2==3.1.1\njmespath==1.0.0\njoblib==1.1.0\nkiwisolver @ file:///C:/ci/kiwisolver_1644962567532/work\nllvmlite==0.38.0\nmahotas==1.4.12\nMarkupSafe==2.1.1\nmatplotlib==3.1.3\nmkl-fft==1.3.1\nmkl-random @ file:///C:/ci/mkl_random_1626186184278/work\nmkl-service==2.4.0\nmysqlclient==1.4.6\nnatsort==8.1.0\nnetworkx==2.8\nnumba==0.55.1\nnumpy==1.21.0\nopencv-python-headless==4.5.5.64\npackaging==21.3\nPillow==9.1.0\nprokaryote==2.4.4\npsutil==5.9.0\npycocotools==2.0.4\npyparsing @ file:///tmp/build/80754af9/pyparsing_1635766073266/work\npython-bioformats==4.0.5\npython-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\npython-javabridge==4.0.3\nPyWavelets==1.3.0\npyzmq==22.3.0\nrequests==2.27.1\ns3transfer==0.5.2\nscikit-image==0.19.2\nscikit-learn==1.0.2\nscipy==1.8.0\nsentry-sdk==0.18.0\nsip==4.19.13\nsix @ file:///tmp/build/80754af9/six_1644875935023/work\nthreadpoolctl==3.1.0\ntifffile==2022.4.8\ntorch==1.11.0\ntornado @ file:///C:/ci/tornado_1606942392901/work\ntqdm==4.64.0\ntyping_extensions==4.1.1\nurllib3==1.26.9\nwincertstore==0.2\nwxPython==4.1.0\n</code></pre>", "<p>Hello,</p>\n<p>I was stuck at the same point just now.<br>\nHad a virtual environment where the runCellpose with cellpose==0.6.5 ran on Cellprofiler 4.2.1<br>\nI tried to upgrade the Cellpose version, ran into the issue <a class=\"mention\" href=\"/u/radee2k\">@radee2k</a> describes.<br>\nI might have tried to upgrade the numpy version <img src=\"https://emoji.discourse-cdn.com/twitter/thinking.png?v=12\" title=\":thinking:\" class=\"emoji\" alt=\":thinking:\" loading=\"lazy\" width=\"20\" height=\"20\">.</p>\n<p><strong>Solution</strong>:</p>\n<pre><code class=\"lang-auto\">pip uninstall numpy\npip unistall python-javabridge\npip uninstall centrosome\npip install numpy==1.21\npip install python-javabridge --no-cache-dir --no-build-isolation\npip install centrosome --no-cache-dir --no-build-isolation\n</code></pre>\n<p>Whatever the reason behind all this is, but now it works and Cellprofiler starts from the command line (in the virtual environment).</p>\n<p>Best of luck</p>\n<p>Jens</p>"], "76899": ["<p>Hi,</p>\n<p>I\u2019m trying to use CellProfiler to apply spillover correction to some stacked images. For each image I have stacks with 37 channels and a spillover matrix. Process is:</p>\n<p>CorrectSpilloverApply - apply matrix to image.<br>\nSaveImages - save output from step above.</p>\n<p>The correction is applied, but when the image is saved the output images seem to have the dimensions altered - each image is now 37 pixels wide and has the same number of channels that should be the pixels in the x dimension. Basically, I think the x and z dimensions are getting mixed up.</p>\n<p>Only seems to happen when I use CorrectSpilloverApply. I\u2019m using CellProfiler 4.2.5. Is there any way to fix/avoid this?</p>\n<p>EDIT: Have tried the same pipeline in version 4.1.3, and it behaves as expected there.</p>", "<p>Hi,</p>\n<p>CellProfiler <a href=\"https://github.com/CellProfiler/CellProfiler/issues/4401\">had issues</a> in 4.0 and 4.1 saving images of certain channel sizes that caused us to change file saving behavior and explicitly require designation of images with &gt;3 channels in CP4.2. I\u2019m not familiar with the plugin you\u2019re referencing, but all that should be required is to add <code>channelstack=True</code> (<a href=\"https://github.com/CellProfiler/CellProfiler/pull/4406/files#diff-cb6a5e43ca8e3f62cae14dabfefd130b7bd132bbe0bd74a484cec330633deb5eR594\">see example here</a>) to the <code>Image</code> created in that plugin and it should be 4.2 - compatible. Good luck!</p>", "<p>Hi Beth,</p>\n<p>Thanks for that. The plugin is from the Bodenmiller groups IMC plugins, and it looks like the method is no longer supported. But I\u2019ve noticed that this also seems to happen when I mask an image stack as well:</p>\n<ul>\n<li>Load image stacks and binary masks</li>\n<li>Save image stacks (output has correct dimensions)</li>\n<li>Mask image stack using binary mask</li>\n<li>Save masked stack (output has incorrect dimensions)</li>\n</ul>\n<p>Thanks,<br>\nMike</p>", "<p>Yes, the only two modules that explicitly set channelstack are the image loading modules and GrayToColor, so I suspect in CellProfiler 4.2 that saving any image that doesn\u2019t come directly from one of those two sources (a directly loaded image or that module) is going to have the same exact problem. If you\u2019re running from source, you can add the same property to any module where you need it; otherwise, you\u2019re probably best off staying in CP4.1, sorry to say.</p>\n<p>In general, most use cases of CellProfiler aren\u2019t designed for mass-operating on multichannel images - it\u2019s assumed that you\u2019re splitting out your channels and operating on them independently. Especially as IMC/CODEX/etc become more commonly adopted, it\u2019s obvious why in some circumstances it\u2019s HELPFUL to be able to do so, but the ability to do it just wasn\u2019t \u201cdesigned\u201d into CellProfiler\u2019s DNA 20 years ago when it was first written, and it\u2019s not fully integrated into the libraries we use to save out images.</p>\n<p>I\u2019ve made a <a href=\"https://github.com/CellProfiler/CellProfiler/issues/4729\">GitHub issue</a> for us to try to make sure to address these issues as we work on CellProfiler 5 - apologies for the complications to your current workflows!</p>"], "76388": ["<p>Hi everyone,</p>\n<p>I am having a recurring error when using Cell Profiler Analyst, where I want to create a filter in the Classifier module, for selecting for example DMSO control wells (information contained in the Test_Per_Image table, Image_Metadata_Compound column). This returns an error saying that the _Per_image and _Per_Object tables are not linked.</p>\n<p>In the past, I was able to use this feature, but I am not sure what I am missing/doing wrong now.</p>\n<p>Included are a sample Cell Profiler pipeline and the generated properties file that reproduce this error.</p>\n<p>I am grateful to anyone who can shed some light on this issue!</p>\n<p>Thanks so much,</p>\n<p>Martin</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/d7JL0ygvhjWH5C93JLqTPL3zsgS.cpproj\">ChromaLive_Test_CPA.cpproj</a> (1.7 MB)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/4Kv1NecydrHZ22IYAy7CsKIP9fT.properties\">20221024_48h_TestCPA_Results_TestCPA.properties</a> (9.4 KB)</p>", "<p>Hi Martin,</p>\n<p>Very sorry to hear that! I don\u2019t see anything that strikes me as out of the ordinary in either file, so we likely need to dig a bit deeper. Can you share the database itself (either directly here or link it out)? Can you tell us a little more about how this was run - OS, CellProfiler version, etc? When you say \u201cin the past\u201d it worked, was that with the same computer and version of CellProfiler that is now generating the error?</p>\n<p>Thanks!</p>", "<p>Hi Beth, thanks for the prompt reply!</p>\n<p>Here is a link to the small batch of images used to recreate the error, as well as the database:</p><aside class=\"onebox googledrive\" data-onebox-src=\"https://drive.google.com/file/d/127gMkyzPdMY430bgRSBzgCRsFj6qTYsw/view?usp=share_link\">\n  <header class=\"source\">\n\n      <a href=\"https://drive.google.com/file/d/127gMkyzPdMY430bgRSBzgCRsFj6qTYsw/view?usp=share_link\" target=\"_blank\" rel=\"noopener nofollow ugc\">drive.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https://drive.google.com/file/d/127gMkyzPdMY430bgRSBzgCRsFj6qTYsw/view?usp=share_link\" target=\"_blank\" rel=\"noopener nofollow ugc\"><span class=\"googledocs-onebox-logo g-drive-logo\"></span></a>\n\n\n\n<h3><a href=\"https://drive.google.com/file/d/127gMkyzPdMY430bgRSBzgCRsFj6qTYsw/view?usp=share_link\" target=\"_blank\" rel=\"noopener nofollow ugc\">Test CPA.zip</a></h3>\n\n<p>Google Drive file.</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nFiles generated on macOS Ventura (also the case with macOS Mojave), Cell Profiler 4.2.4, Cell Profiler Analyst 3.0.4</p>\n<p>Here is the properties file corresponding to a working database, generated also on a Mac, with an up-to-date Cell Profiler (at the time, August 2022)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/fNw6KskmoakqwbPgltXzYkciQl.properties\">MRTx_ChromaLive_Test.properties</a> (8.9 KB)</p>\n<p>I\u2019ve been looking at whatever could have been done differently between the analysis, but can\u2019t find anything\u2026</p>"], "81003": ["<p>Hello,</p>\n<p>We are using Cellprofiler 4.2.1 and 4.2.5 and are very happy with the functionality this gives us.<br>\nFor the Trends in Microscopy Workshop Conference a colleague of mine developed a jupyter notebook to draw images from OMERO inject them into a headless Cellprofiler pipeline and upload the resulting images and measurements.<br>\nThis opened up some interesting possibilities.<br>\nThe next step for us would be to get the generated/filtered objects from Cellprofiler and upload them as ROIs to OMERO.<br>\nI have tested around a bit with the Cellprofiler Python code/documentation and found no easy way to get the objects out of the pipeline.<br>\nWe have the following starting parameters:</p>\n<ul>\n<li>a pipeline *.cppipe</li>\n<li>a folder with images</li>\n</ul>\n<p>Is there an elegant way to run the pipeline with these images and somehow access the generated objects in the end or at some other point?</p>\n<p>Best,</p>\n<p>Jens</p>"], "74859": ["<p>Hello, I am new here and also new to this image analysis thing. So at my training lab, fluorescence intensity from different channels is measured manually for each cell. I have been told to develop a pipeline (if possible) that automatically measures blue and red intensity in a single go using CellProfiler. And so, I have made a pipeline and would appreciate it if some experts could validate the basic idea of my pipeline. So the pipeline is as follows: Identify primary objects (nucleus/DAPI stained DNA from channel 0) &gt; identify primary objects (Cy5 red signals from channel 1, around the nucleus) &gt; relate the objects [DNA as a parent and PLA signals as a child] &gt; Merge the PLA signals &gt; measure the mean fluorescence intensity of DNA and PLA &gt; display the intensity on image/export it in spreadsheet format.<br>\nMy input is an oir file, so I use FIJI to take the maximum intensity projection (Z-stack) of both the channels followed by merging it and saving it as RGB file. I edit diameter range, threshold values/strategy, etc as per the image or results I get. Thank you! Any suggestions/improvements?</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/rJEJPij7dvDks7WFF7v6QSH7Gm8.cppipe\">Final_PLA_pipeline.cppipe</a> (14.9 KB)</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/8/28c90e7499755803de9145c561f50cc809a380dc.png\" data-download-href=\"/uploads/short-url/5ONRl2ewR3lHM4cZjHrjxJwIkEs.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/8/28c90e7499755803de9145c561f50cc809a380dc_2_537x500.png\" alt=\"image\" data-base62-sha1=\"5ONRl2ewR3lHM4cZjHrjxJwIkEs\" width=\"537\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/8/28c90e7499755803de9145c561f50cc809a380dc_2_537x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/8/28c90e7499755803de9145c561f50cc809a380dc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/8/28c90e7499755803de9145c561f50cc809a380dc.png 2x\" data-dominant-color=\"5A5B5A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">622\u00d7579 101 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/d/0d9fb94d8509755c4b01a200dab7520d988a7dcb.png\" alt=\"image\" data-base62-sha1=\"1WwpET70CHatSoe7KU2Yhc79aUH\" width=\"397\" height=\"395\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8c80cf43364213754bcdf92a29977a3d497f3de2.png\" alt=\"image\" data-base62-sha1=\"k2WON0aVYXgj7nDmSfgnTI62mqK\" width=\"429\" height=\"441\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/98b05b7bcc4caec7ffd2b4c1bf692a4211047751.jpeg\" alt=\"image\" data-base62-sha1=\"lMKq0bS71CiVkPUMrGZGVTD51M5\" width=\"429\" height=\"446\"></p>", "<aside class=\"quote no-group\" data-username=\"sha11ot\" data-post=\"1\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n Amaan:</div>\n<blockquote>\n<p>Hello, I am new here and also new to this image analysis thing.</p>\n</blockquote>\n</aside>\n<p>Welcome to the forum! <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<aside class=\"quote no-group\" data-username=\"sha11ot\" data-post=\"1\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n Amaan:</div>\n<blockquote>\n<p>I have been told to develop a pipeline (if possible) that automatically measures blue and red intensity in a single go using CellProfiler.</p>\n</blockquote>\n</aside>\n<p>Seems very doable!</p>\n<p>In general, your strategy mostly makes sense. If your question is just about measuring Cy5 signal in nuclei, I\u2019m not sure you even need to segment Cy5. You could just segment nuclei then MeasureObjectIntensity in the Cy5 channel. Be very careful when you\u2019re measuring intensity values. For instance, make sure you don\u2019t have saturation in your images. Measuring intensity is very easy (clicking a button usually) but doing it <em>right</em> is actually really hard!</p>\n<aside class=\"quote no-group\" data-username=\"sha11ot\" data-post=\"1\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n Amaan:</div>\n<blockquote>\n<p>merging it and saving it as RGB file</p>\n</blockquote>\n</aside>\n<p>You should not need to save this as an RGB file. I would recommend you don\u2019t do this as it might result in a change in bit depth in your image depending on how you do it. Save as a tiff and then use that in Cellprofiler (grayscale image).</p>\n<p>I\u2019m not 100% sure what you mean when you say you edit the settings depending on the image/result, but in general you want to pick 1 pipeline and 1 set of settings for all your images to produce the most fair and reproducible workflow.</p>", "<p>Oh hey, thank you very much <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"> !!</p>\n<aside class=\"quote no-group\" data-username=\"Rebecca_Senft\" data-post=\"2\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rebecca_senft/40/48617_2.png\" class=\"avatar\"> Rebecca Senft:</div>\n<blockquote>\n<p>You could just segment nuclei then MeasureObjectIntensity in the Cy5 channel.</p>\n</blockquote>\n</aside>\n<p>I am not exactly sure how this can be done. Can you please elaborate on this? As far as I know, if I segment nuclei then only their intensities can be measured\u2026It would be great if you could just link any other post/tutorial if already available.</p>\n<aside class=\"quote no-group\" data-username=\"Rebecca_Senft\" data-post=\"2\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rebecca_senft/40/48617_2.png\" class=\"avatar\"> Rebecca Senft:</div>\n<blockquote>\n<p>You should not need to save this as an RGB file</p>\n</blockquote>\n</aside>\n<p>Oh shit, by RGB file\u2026I meant, saving the merged file, not as a composite, but as a RGB tiff and for grayscaling it, I have added the module.</p>\n<aside class=\"quote no-group\" data-username=\"Rebecca_Senft\" data-post=\"2\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rebecca_senft/40/48617_2.png\" class=\"avatar\"> Rebecca Senft:</div>\n<blockquote>\n<p>in general you want to pick 1 pipeline and 1 set of settings for all your images to produce the most fair and reproducible workflow</p>\n</blockquote>\n</aside>\n<p>Yessss, but right now I am just testing my pipeline with different cells of varying size/shape, in clumps or individually, with different intensities, and hence thus changing diameter/threshold or other parameters.</p>\n<p>And again, thanks a lot.</p>", "<aside class=\"quote no-group\" data-username=\"Amaan\" data-post=\"3\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/amaan/40/65788_2.png\" class=\"avatar\"> Amaan:</div>\n<blockquote>\n<p>I am not exactly sure how this can be done. Can you please elaborate on this? As far as I know, if I segment nuclei then only their intensities can be measured\u2026It would be great if you could just link any other post/tutorial if already available.</p>\n</blockquote>\n</aside>\n<p>Take a look at the example pipeline (the Try Example button on the Welcome to CellProfiler screen that pops up when you open CellProfiler) for an example of this. Generally speaking. Use DAPI when you IdentifyPrimaryObjects, then use MeasureObjectIntensity and choose the image corresponding to the channel you want to measure.</p>\n<aside class=\"quote no-group\" data-username=\"Amaan\" data-post=\"3\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/amaan/40/65788_2.png\" class=\"avatar\"> Amaan:</div>\n<blockquote>\n<p>Oh shit, by RGB file\u2026I meant, saving the merged file, not as a composite, but as a RGB tiff and for grayscaling it, I have added the module.</p>\n</blockquote>\n</aside>\n<p>I\u2019m not 100% exactly what you did, but here\u2019s an example to show you what I mean. This is with the example neuron image built into Fiji. This is with just 1 channel for clarity. On the left, I just have the channel as a 16 bit tiff (no modifications). The histogram of image intensities is shown (you can see how it trails off at higher intensity values). On the right is the image and histogram if I were to select Image &gt; Type &gt; RGB. This image looks the same to our eyes, but we can use the histogram to see how it is different. The image is now 8bit and we can see the range of intensities has changed. This is not great! We\u2019re kind of truncating our data as you can see in the histogram. In general there is no reason for you to do this. Just save the tiff and split the channels in CellProfiler. Like I said, I\u2019m not 100% sure how you went about saving the image, but definitely make sure you\u2019re not chopping off your data like this!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d2b33c0ba2d68cdc0ce0839388cad82064bce2ba.jpeg\" data-download-href=\"/uploads/short-url/u3WhaZhbpT2QTALCJWnHIrJTmlk.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d2b33c0ba2d68cdc0ce0839388cad82064bce2ba_2_582x499.jpeg\" alt=\"image\" data-base62-sha1=\"u3WhaZhbpT2QTALCJWnHIrJTmlk\" width=\"582\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d2b33c0ba2d68cdc0ce0839388cad82064bce2ba_2_582x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d2b33c0ba2d68cdc0ce0839388cad82064bce2ba_2_873x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d2b33c0ba2d68cdc0ce0839388cad82064bce2ba.jpeg 2x\" data-dominant-color=\"736161\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1079\u00d7926 79.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"Rebecca_Senft\" data-post=\"4\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rebecca_senft/40/48617_2.png\" class=\"avatar\"> Rebecca Senft:</div>\n<blockquote>\n<p>Like I said, I\u2019m not 100% sure how you went about saving the image, but definitely make sure you\u2019re not chopping off your data like this!</p>\n</blockquote>\n</aside>\n<p>The file provided to me is a Z-stack of blue and red channels in a single OIR format file. So, when I open the oir file in fiji, 2 windows open up. One of blue channel and one of red channel. What I do is basically get the maximum intensity projection (image &gt; stacks &gt; z project &gt; max intensity) of both image. And then I merge the maximum intensity images (image &gt; color &gt; merge channels [composite unchecked]) to get a single RGB file. And then I save it as tiff. Then I just load this image into CellProfiler and then use a module ColortoGray to split the 2 channels followed by intensity measurement of both, DAPI and PLA as mentioned in the original message.  So thanks to you, now I know that I am losing information in the conversion step <img src=\"https://emoji.discourse-cdn.com/twitter/smiling_face_with_tear.png?v=12\" title=\":smiling_face_with_tear:\" class=\"emoji\" alt=\":smiling_face_with_tear:\" loading=\"lazy\" width=\"20\" height=\"20\">.<br>\nI can\u2019t even add screenshots right now or do the tutorial of finding PLA intensity directly without segmenting Cy5 channel since I don\u2019t have access to my institute computer on the weekend. Will try your advice on Monday morning definitely.<br>\nThank you very much!</p>", "<p>This sounds ok. When you open the OIR file in Fiji, what bit depth is above the image?:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/c/fc555507c4440230c2e7a217d307a24874226928.png\" data-download-href=\"/uploads/short-url/A0f92BXoPcTXwSQkJT1ozBaKC9i.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/c/fc555507c4440230c2e7a217d307a24874226928_2_690x76.png\" alt=\"image\" data-base62-sha1=\"A0f92BXoPcTXwSQkJT1ozBaKC9i\" width=\"690\" height=\"76\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/c/fc555507c4440230c2e7a217d307a24874226928_2_690x76.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/c/fc555507c4440230c2e7a217d307a24874226928.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/c/fc555507c4440230c2e7a217d307a24874226928.png 2x\" data-dominant-color=\"DCDDDC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">756\u00d784 15.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Just projecting, merging and saving should not change your bit depth. Does it actually say RGB above the image before you save? My guess is either your image files are RGB from the start or that the bit depth is preserved in the final image.</p>", "<p>Thank you for your post, this helped me with an issue I\u2019ve been facing as well as I haven\u2019t been sure how to save images properly while saving their bit depth. If you save a tiff file with multiple channels from FIJI, how do I then go about splitting those channels in cellprofiler as separate images?<br>\nThanks for your help!<br>\nJason</p>", "<p>No problem Jason. There are a couple of ways to do this. You could save separate tiff files for each channel in Fiji and then assemble them afterward. But you can also go from multichannel tiff to separate channels in CellProfiler. Just make sure to extract metadata from image file headers in Metadata and then you should be able to assign different channels to different image names in NamesAndTypes. Note that there is not currently support for multichannel 3D images and these have to be split into 1 image per channel before being opened in CellProfiler.</p>", "<p>Hey Rebecca,</p>\n<aside class=\"quote no-group\" data-username=\"Rebecca_Senft\" data-post=\"6\" data-topic=\"74859\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rebecca_senft/40/48617_2.png\" class=\"avatar\"> Rebecca Senft:</div>\n<blockquote>\n<p>Does it actually say RGB above the image before you save?</p>\n</blockquote>\n</aside>\n<p>I can save them as a 16-bit composite images instead of saving them as RGB.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/3/234ee1c6cd04a88e06cb950da24da397c12df618.png\" alt=\"image\" data-base62-sha1=\"52lIlodIVS0aLXu7yyl1WIHSpG8\" width=\"380\" height=\"370\"><br>\nAnd yes, when the composite is not checked while merging the channels it automatically merges the channel and opens a RGB file.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/2472043c1c99eddfc394797fc06dccb996e90f14.png\" alt=\"image\" data-base62-sha1=\"5cpt1Xos95z8XdgdUvdteaAm3Mo\" width=\"518\" height=\"71\"></p>\n<p>Thanks for all the help!!</p>", "<p>For loading in multi-channel images, you may want to check out our tutorial/blog post on the <a href=\"https://forum.image.sc/t/input-modules-tutorial/63911#importing-2d-images-521-3\">CellProfiler input modules</a>; depending on how you have the images saved, you want either example 2 or example 3 in the linked section. The blog also links out to timestamps in a YT video, depending on if you\u2019re a text-first or video-first person.</p>\n<p>Hope that helps!</p>", "<p>oh Hey Beth,<br>\nThanks for the reply. I am definitely checking out all the tutorials in the free time. Thank you and have a nice day!</p>"], "78963": ["<p>Hi, I would like to download Cellpose plugin and try it for my images.<br>\nHowever when I click on the download link it refers me to Github repository.<br>\nI am not into programming so much, I would like to have this very basic question that how I can add it to my cell profiler?<br>\nthanks a lot</p>", "<h2>\n<a name=\"sara-i-found-it-easier-to-create-a-small-python-script-python-at-least-399-installation-install-cuda-i-am-using-cuda-116-httpsdocsnvidiacomcudacuda-installation-guide-microsoft-windowsindexhtml-pip-install-cellpose-pip-uninstall-torch-pip-install-torch-f-httpsdownloadpytorchorgwhltorch_stablehtml-1\" class=\"anchor\" href=\"#sara-i-found-it-easier-to-create-a-small-python-script-python-at-least-399-installation-install-cuda-i-am-using-cuda-116-httpsdocsnvidiacomcudacuda-installation-guide-microsoft-windowsindexhtml-pip-install-cellpose-pip-uninstall-torch-pip-install-torch-f-httpsdownloadpytorchorgwhltorch_stablehtml-1\"></a>Sara,<br>\nI found it easier to create a small Python script.<br>\npython at least 3.9.9<br>\nInstallation:<br>\nInstall CUDA (I am using Cuda 11.6)<br>\n<a href=\"https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">cuda-installation-guide-microsoft-windows 12.1 documentation</a><br>\npip install cellpose<br>\npip uninstall torch<br>\npip install torch -f <a href=\"https://download.pytorch.org/whl/torch_stable.html\" rel=\"noopener nofollow ugc\">https://download.pytorch.org/whl/torch_stable.html</a>\n</h2>\n<p>import cv2  # pip install opencv-python if needed<br>\nimport skimage  # pip install scikit-image if needed<br>\nfrom skimage.color import rgb2gray, label2rgb<br>\nfrom skimage.measure import label, regionprops, regionprops_table<br>\nimport cellpose  # pip install cellpose<br>\nimport cellpose.models<br>\nimport matplotlib.pyplot as plt  # pip install matplotlib<br>\nimport matplotlib.patches as mpatches<br>\nfrom matplotlib.patches import Rectangle<br>\nimport torch</p>\n<p>image = cv2.imread(filename)<br>\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)<br>\nchannels = [0, 0]  # IF YOU HAVE GRAYSCALE<br>\nlabels, flows, style, diams = model_cyto.eval(gray, diameter=average_diameter_pixs, channels=channels)<br>\n\u2026</p>", "<p><a class=\"mention\" href=\"/u/keesh\">@keesh</a> Thank you so much! I finally installed runcellpose to mu mac, but the problem is I only have it when I open cell profiler from code source, unfortunately when I directly open Cell profiler I still don\u2019t have it.<br>\n<a class=\"mention\" href=\"/u/dstirling\">@DStirling</a> I also would like to hear your comment on it please <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Sara,<br>\nPlease read this thread as it may help you.<br>\nthx e.-<br>\n<a href=\"https://forum.image.sc/t/runcellpose-plugin-installation/63073?page=2\">RunCellpose Plugin Installation - Usage &amp; Issues - Image.sc Forum</a></p>"], "79476": ["<p>Dear all!</p>\n<p>I am pretty new to image data analysis and I finally got some nice confocal images. With the help of this forum and youtube I tried for the first time ever to analyze some of my images. However, often there are only single cell pictures in the demos and I have some troubles with the identification of my cell membrane on a tissue staining. My DAPI is sometimes weak so I tried to segment my cell membrane via IdentifyPrimaryObjects, which worked somehow but I think far away from really good. Next, I identify Spot Counts again with IdentifyPrimaryObjects and relate the Objects to the Cell membrane because I want to count it inside the cells and this works really good with CellProfiler. But again, my cell membrane is not so good-looking. I stained the cell membrane with EpCAM and my nucleus with DAPI.</p>\n<p>So my question is, if somebody has some good tips for me to improve my pipeline to identify my cell membrane and to count some proteins inside for further analysis?</p>\n<p>Thanks in advance!</p>\n<p>here is my pipeline:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/4P0JvpYHyIaQiaMXPs1AFnYemTV.cpproj\">pipeline_cellmembrane.cpproj</a> (141.8 KB)</p>\n<p>and my pictures:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/wRDWyuslXiqu1gMfT4HymQ6jqQw.tif\">AA3_AF647.tif</a> (1.1 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/xQlDr0B1Gb1VMUQEylZy55msllo.tif\">AA3_DAPI.tif</a> (1.1 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/3B34CKBW3Qcj3M7XP0n5hqvBZXR.tif\">AA3_EPCAM488.tif</a> (1.1 MB)</p>", "<p>Welcome to image.sc!</p>\n<p>I had a look at your images. If you want a membrane segmentation for your first image <a class=\"attachment\" href=\"https://forum.image.sc/uploads/short-url/wRDWyuslXiqu1gMfT4HymQ6jqQw.tif\">AA3_AF647.tif</a> then maybe try <a href=\"https://github.com/hci-unihd/plant-seg/blob/b025d95df308261b393ef474e8f40436d2c27fa0/plantseg\" rel=\"noopener nofollow ugc\">PlantSeg</a> from Heidelberg University. There are some very good models for confocal images. I\u2019m not involved in this work, but you may cite <a href=\"https://github.com/hci-unihd/plant-seg#citation\" rel=\"noopener nofollow ugc\">this</a>.</p>\n<p>If you actually want others to have a look at your images, please upload a screenshot along with your TIFFs:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/1/01701d1e8c29dfe96e1be76b054ef3d69bb200fd.jpeg\" data-download-href=\"/uploads/short-url/cIGaaRn61aLkaM3EWhjVuOLd4F.jpeg?dl=1\" title=\"AA3_EPCAM488\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01701d1e8c29dfe96e1be76b054ef3d69bb200fd_2_500x500.jpeg\" alt=\"AA3_EPCAM488\" data-base62-sha1=\"cIGaaRn61aLkaM3EWhjVuOLd4F\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01701d1e8c29dfe96e1be76b054ef3d69bb200fd_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01701d1e8c29dfe96e1be76b054ef3d69bb200fd_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/1/01701d1e8c29dfe96e1be76b054ef3d69bb200fd_2_1000x1000.jpeg 2x\" data-dominant-color=\"333333\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">AA3_EPCAM488</span><span class=\"informations\">1024\u00d71024 86 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>This is PlantSeg:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/725be8a58903b45cccc2ec3bd0a92a737ba49751.jpeg\" data-download-href=\"/uploads/short-url/gjFlpeMiGxDI57D6H2KVvFk6kwh.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/725be8a58903b45cccc2ec3bd0a92a737ba49751_2_690x257.jpeg\" alt=\"image\" data-base62-sha1=\"gjFlpeMiGxDI57D6H2KVvFk6kwh\" width=\"690\" height=\"257\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/725be8a58903b45cccc2ec3bd0a92a737ba49751_2_690x257.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/725be8a58903b45cccc2ec3bd0a92a737ba49751_2_1035x385.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/725be8a58903b45cccc2ec3bd0a92a737ba49751_2_1380x514.jpeg 2x\" data-dominant-color=\"99A298\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2158\u00d7806 282 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "80503": ["<p>Hello,</p>\n<p>I am studying cell motility in fibroblasts and am hoping to measure cell protrusions of multiple cells as they are tracked over ~100 frames. My approach is to binarize the image, use Morph with the center of cells as a seed, and then MeasureObjectSkeleton. I am mainly interested in the end points of branches on the perimeter of the cell, as opposed to the vertices and skeletons inside the cell. I would like to know how many end points there are, how they change in position and distance from the center of the cell, etc. However, I have not been able to figure out how to correlate the data output of MeasureObjectSkeleton (the vertices and edges csv files) to the tracked cells in my images in a straightforward way. For example, in my main output csv, each row has an image number and cell number. However, in the edges and vertices csvs, there is only an image number, no cell number. Is there a way to add cell number to these output files? If not, would the best way to do this be to write a code that takes in the vertices positions and find what cell center it matches best?</p>\n<p>Many thanks.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4262cf656b7a06bc7e91060fd5578fd87ee1a8da.jpeg\" data-download-href=\"/uploads/short-url/9thdc0rezAxYI4iXnPgc1DGlxB8.jpeg?dl=1\" title=\"skel_example\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4262cf656b7a06bc7e91060fd5578fd87ee1a8da_2_690x335.jpeg\" alt=\"skel_example\" data-base62-sha1=\"9thdc0rezAxYI4iXnPgc1DGlxB8\" width=\"690\" height=\"335\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4262cf656b7a06bc7e91060fd5578fd87ee1a8da_2_690x335.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/2/4262cf656b7a06bc7e91060fd5578fd87ee1a8da_2_1035x502.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/2/4262cf656b7a06bc7e91060fd5578fd87ee1a8da.jpeg 2x\" data-dominant-color=\"555555\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">skel_example</span><span class=\"informations\">1250\u00d7608 86.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d91f8f6f1d999930a6856146379814f4a47aa12c.jpeg\" data-download-href=\"/uploads/short-url/uYLeb6utoyXQc85qQIk2mDiAFoo.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d91f8f6f1d999930a6856146379814f4a47aa12c_2_690x172.jpeg\" alt=\"image\" data-base62-sha1=\"uYLeb6utoyXQc85qQIk2mDiAFoo\" width=\"690\" height=\"172\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d91f8f6f1d999930a6856146379814f4a47aa12c_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d91f8f6f1d999930a6856146379814f4a47aa12c_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d91f8f6f1d999930a6856146379814f4a47aa12c.jpeg 2x\" data-dominant-color=\"0A1508\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1035\u00d7258 33.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Lilianna,<br>\nif you have two or more object classes you may try to use the module \u201cRelateObjects\u201d to identify how many children objects are contained in every parent cell.</p>\n<p>I would use the option \u201cRetain the branchpoint image\u201d in \u201cMeasureObjectSkeleton\u201d, then try to split the image of the Branchpoints, apply a threshold to R/G/B, and get different object classes (Branchpoints, Branches, Endpoints).</p>\n<p>Finally, use \u201cRelateObjects\u201d with segmented cells as parent objects. If you include all the measurements, the final .csv files should contain the parent cells for every branchpoint, etc., and the number of children branchpoints for every parent cell.</p>\n<p>Good luck!<br>\nRocco</p>", "<p>Thank you!</p>", "<p>Hi Rocco,</p>\n<p>I\u2019m not sure how to get CellProfiler to identify objects by color, specifically the following step:</p>\n<aside class=\"quote no-group\" data-username=\"RoccoDAntuono\" data-post=\"2\" data-topic=\"80503\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/roccodantuono/40/28724_2.png\" class=\"avatar\"> Rocco D'Antuono:</div>\n<blockquote>\n<p>apply a threshold to R/G/B, and get different object classes (Branchpoints, Branches, Endpoints)</p>\n</blockquote>\n</aside>\n<p>I have the branchpoint image, and used ColorToGray to r/b/g split it. However, for each color, it turns pixels of the other colors black and does not effect the while \u201cbones\u201d of the skeleton. I hope the images below illustrate this. So far, I have not figured out how to go from this step to having Branchpoint, Branch, Endpoint objects. Any suggestions would be appreciated. Thanks.</p>\n<p>Original branchpoint image:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/5/45ce0b85a75284f1fe3fa3eb12a06be6fd6bfd21.png\" alt=\"oBranchpoint\" data-base62-sha1=\"9XwotOZWVOfLg6tYG5ZGfiW6lR7\" width=\"546\" height=\"339\"></p>\n<p>Red branchpoint image:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1515c2544716ad63a7d10be7ea4cdcb970343eb.png\" alt=\"rBranchpoint\" data-base62-sha1=\"tRI6sxAxhzcaiCsmL333QEDjN7t\" width=\"546\" height=\"339\"></p>\n<p>Green branchpoint:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1e0b71f8a9f7b592899315770b75fec5f28bbeb3.png\" alt=\"gBranchpoint\" data-base62-sha1=\"4hMQpuO1wXLL6BRx7cXC419f3fd\" width=\"546\" height=\"339\"></p>", "<p>Hi Lilianna, I see, it is very unfortunate.</p>\n<p>A think a workaround might be:</p>\n<ol>\n<li>get the overall skeleton (threshold the branchpoint image with a very low value, after having converted it to grayscale). Let\u2019s call it \u201cTh_sk\u201d</li>\n<li>threshold the separated images from the branchpoint image. Let\u2019s call them \u201cTh_R\u201d, \u201cTh_G\u201d, \u201cTh_B\u201d</li>\n<li>get the partial skeleton (just white pixels) with the \u201cAnd\u201d operation in the \u201cImageMath\u201d module, using \u201cTh_R\u201d, \u201cTh_G\u201d, and \u201cTh_B\u201d (obtained in point 2). Let\u2019s call it \u201cTh_sk_part\u201d</li>\n<li>subtract the partial skeleton ( \u201cTh_sk_part\u201d from point 3) from the overall skeleton (\u201cTh_sk\u201d point 2), to get a binary image of just the R/G/B pixels (branching points pixels, \u201cTh_br\u201d).</li>\n<li>get the images with pure R, G, B pixels as a result of the \u201cAnd\u201d operation in the \u201cImageMath\u201d module between the image of branching points pixels \u201cTh_br\u201d from point 4 and images \u201cTh_R\u201d, \u201cTh_G\u201d, \u201cTh_B\u201d from point 2.</li>\n</ol>\n<p>Best regards,<br>\nRocco</p>"], "76407": ["<p><a class=\"attachment\" href=\"/uploads/short-url/2fgpoBDu0vZU99DYcw1995bo19Z.tif\">\u6a19\u672cAB_2262.tif</a> (13.5 MB)</p>", "<p>It would be helpful to know what you want to measure.</p>", "<p>Thanks for response. I want to measure cell sizes and area. Is it possible to detect this kind of image by using imageJ or Cell profiler?</p>", "<p>What kind of image is this? Are the cells stained with any dyes? This doesn\u2019t look like the kind of image CellProfiler can work with. FIJI seems more likely. Also, your image is an RGB color image. Are the original, acquired data in grayscale?</p>", "<p>These are tissue like structure from hipsc and this is a raw image with RGB and I didn\u2019t stain the cells yet. How can I manage this kind of image with FIJI? I appreciate your guidance.</p>", "<p>Hi Mosa,</p>\n<p>Realistically, I think segmenting these cells, let alone accurately measuring area, is going to be not possible here, because a) it seems like in general the cells are overlapping, so at minimum I think you\u2019d need to acquire in 3D and b) in this image, it\u2019s REALLY hard to even see where the cell boundaries are between cells (one thing I often ask collaborators with tricky images to segment is to sit down and draw the cell boundaries manually on one image - if your \u201cneural net\u201d is having a hard time with it, likely a computer\u2019s won\u2019t be able to either). If that\u2019s the kind of information that you need, I think you need to look into different imaging methods. Is there someone at your institution that can help you with that? This image just unfortunately I don\u2019t think holds the information that you need to measure what you want to measure.</p>", "<p>Hi Beth, thank you for your reply and your opinion.<br>\nI will look for someone who can help me this kind of image in my institute.</p>", "<p>Frankly speaking. These cells are over confluent and as Beth mentioned, there are no clear borders visible. This makes it impossible to segment these cells.</p>"], "77946": ["<p>I\u2019m trying to make the switch from Fiji/ImageJ to Cell profiler, but whenever I try to upload one of my images onto CellProfiler (v4.2.5), I keep getting the same error message: \u201cCould not load the file as an image (see log for details)\u201d</p>\n<p>Any ideas on what I could be doing wrong?</p>", "<p>Hi Steve, could yo provide a sample image? or tell us more about what file type was the image stored as? How are you loading it into CellProfiler?</p>"], "62611": ["<p>I have EM images of sarcomeres that are labeled with gold particles and I would like to be able to measure the minimal distance of the gold particles within a given ROI to another manually selected object. As shown in the attached image, I can successfully use cellprofiler to identify the gold particles as primary objects within the ROI and I can successfully perform a manual selection of the second object, but I haven\u2019t come up with a way to measure the minimal distance of all of the primary objects from the manually selected object. The measureobjectneighbors function works but only produces distance measurements for the closest and second closest primary objects. Any thoughts on how to get minimal distance measurements for all of the identified primary objects?</p>\n<p>Thanks in advance for any help that can be provided</p>", "<p>I found a solution to this question in a previous forum</p><aside class=\"quote quote-modified\" data-post=\"2\" data-topic=\"58131\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/schmiedc/40/14633_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/finding-a-shortest-distance-to-the-object-and-handling-large-image-files/58131/2\">Finding a shortest distance to the object and handling large image files</a> <a class=\"badge-wrapper  bullet\" href=\"/c/image-analysis/6\"><span class=\"badge-category-bg\" style=\"background-color: #25AAE2;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for questions about image processing and analysis.\">Image Analysis</span></a>\n  </div>\n  <blockquote>\n    In terms of the core image analysis this is a relatively easy problem to solve. \n\n\nDetect each individual particle. Which can be done with a Maximum detection at best: <a href=\"https://imagej.nih.gov/ij/docs/menus/process.html#find-maxima\" rel=\"noopener nofollow ugc\">https://imagej.nih.gov/ij/docs/menus/process.html#find-maxima</a> \nIf the data is a bit noisy apply a Gaussian filter first. To be more robust and size selective use a Difference of Gaussian or Laplacian of Gaussian filter. The maximum detection will give you regions of interest (ROIs). \n\n\nThen you need to get the distance from the mu\u2026\n  </blockquote>\n</aside>\n\n<p>Briefly, generate a Euclidean distance map in ImageJ and then determine the intensity of gray that resides under your objects of interest  <a href=\"https://imagej.nih.gov/ij/docs/guide/146-29.html#sub:Distance-Map\" rel=\"noopener nofollow ugc\">https://imagej.nih.gov/ij/docs/guide/146-29.html#sub:Distance-Map </a></p>", "<p>Hi Troy, there are a couple of ways to do this in CellProfiler also - most analogously, you can generate a distance transform using the \u201cMorph\u201d module. But I think both of these should also work</p>\n<ul>\n<li>Using MeasureObjectNeighbors using your gold particles as your object and the manual object as the neighbor (since it sounds like you typically only have one manual object per image?), you should be able to get this measurement with the \u201cExpand until adjacent\u201d option.</li>\n<li>Using MeasureObjectNeighbors with your manual object as the object and the gold particles as the neighbor, the \u201cObject Relationships\u201d spreadsheet that you can add in ExportToSpreadsheet should provide this info, if you use the Within distance option and set the distance particularly large.</li>\n</ul>"], "76436": ["<p><a class=\"attachment\" href=\"/uploads/short-url/3WKHcDFBPdXoGCjQ2hCgZMKdoDU.tif\">Olig2_image_1.tif</a> (528.4 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/1XC0CAMhsgW9TRAIo9YK4Bx8vV7.tif\">Olig2_image_2.tif</a> (528.4 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/kU5KPhnKEesGg2AU6WojGWS0Fqe.tif\">Olig2_image_3.tif</a> (528.4 KB)</p>\n<p>The attached images show the labeled nuclei for a single cell type (oligodendrocytes, using antibody Olig2). I\u2019m trying to quantify the number of labeled nuclei in the tissue images using a CellProfiler pipeline I created.</p>\n<p>Since I\u2019m using a mouse antibody to label these nuclei in mouse cortical tissue, a lot of debris/background (mainly blood vessels, I\u2019m guessing) is also being labeled. My pipeline is counting the debris/background as objects, and the amount of debris/background varies between images so I\u2019m getting a lot of variability between different images. I\u2019m also staining with other nuclear markers (like NeuN) and DAPI; these images don\u2019t have as much debris/background as the Olig2 images so the pipeline is more precise with labeling and counting only nuclei as objects.</p>\n<p>Of the images I posted, image_1 is a pretty good image (little debris/background), image_2 is an okay image (more debris/background), and image_3 is a poor image (lots of debris/background).</p>\n<p>I\u2019ve tried adjusting the diameter of primary objects and filtering objects all kinds of ways but can\u2019t seem to find the right combination.</p>\n<p>Here is my pipeline: <a class=\"attachment\" href=\"/uploads/short-url/w6CDxPIqw6rFzTs4pJRNbQhdks7.cpproj\">Glia Cell Counting.cpproj</a> (118 KB)</p>\n<p>I\u2019m using CellProfiler4.2.4 on a MacBook Pro.</p>\n<p>Thanks in advance!</p>", "<p>Hi <a class=\"mention\" href=\"/u/aeaker\">@aeaker</a> ,</p>\n<p>I haven\u2019t checked your pipeline, but I tried the worst case (image 3) with Cellpose and looks to me that it is doing a reasonably good job?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/b/0bcf74ba9dd5552c65e9461663764c9f0cefc81d.jpeg\" data-download-href=\"/uploads/short-url/1GtIV9EuSjTuQ7BcgAx0D8rCWkt.jpeg?dl=1\" title=\"Olig2_image_3_3X_withROIs\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bcf74ba9dd5552c65e9461663764c9f0cefc81d_2_690x331.jpeg\" alt=\"Olig2_image_3_3X_withROIs\" data-base62-sha1=\"1GtIV9EuSjTuQ7BcgAx0D8rCWkt\" width=\"690\" height=\"331\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bcf74ba9dd5552c65e9461663764c9f0cefc81d_2_690x331.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bcf74ba9dd5552c65e9461663764c9f0cefc81d_2_1035x496.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/b/0bcf74ba9dd5552c65e9461663764c9f0cefc81d_2_1380x662.jpeg 2x\" data-dominant-color=\"6D616D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Olig2_image_3_3X_withROIs</span><span class=\"informations\">1920\u00d7923 182 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "77464": ["<p>Hello,</p>\n<p>I was analyzing if two proteins, which were respectively stained as Red(EP300) and Green(ROCK2), are colocalized using the pixel based pipeline(an attached file below), which I created according to the colocalization analysis in CellProfiler tutorial.The algorithm seemed to work well for dimly red channel image in which the red signals are dominantly located in nuclei. However, it did not go well with strongly green channel image as the green signals distributed in both cytoplasms and nuclei were filtered out and lost post Illumination correction ( attached 4 images). I have tried all algorithms equipped with modules for correcting green channel image. \u201cBackground\u201d method barely corrected background and various combinations of \" Regular\" algorithms failed in my hands.</p>\n<p>Could someone please help me sort out:1.the cause for the failure to illumination correction of green channel image; 2. how to correct color switch of the images3,4, in which green nuclei are supposed to be red.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/wp3zyP7WFugrmS3nzegftLQ1YZ7.cpproj\">R2_EP Colocalization.cpproj</a> (895.0 KB)<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/2/52d1e6ee73d690bb89cb7fc5b47ae2d5365e5d50.png\" alt=\"image1\" data-base62-sha1=\"bOERFKGMUXrQlfwbJMtU2wIRhio\" width=\"640\" height=\"440\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/5/05419360c60deb437ce565311e951c8a6cad7c22.png\" alt=\"image2\" data-base62-sha1=\"KuSIPQgmmlrqCltvthMcJOQYlY\" width=\"640\" height=\"440\"><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b78db330859f87876cddcf6d48311e0cd3a38cdf.png\" alt=\"image3\" data-base62-sha1=\"qbMY0qP0zaKpJtCiFa2IWp28SZ1\" width=\"640\" height=\"440\"><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/071e946b4e38edc5aaaf1ab6eac69fa22cb64f19.png\" data-download-href=\"/uploads/short-url/10YRgvauUL64gTwZAbIEDIJ4Xhv.png?dl=1\" title=\"image4\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/071e946b4e38edc5aaaf1ab6eac69fa22cb64f19_2_690x395.png\" alt=\"image4\" data-base62-sha1=\"10YRgvauUL64gTwZAbIEDIJ4Xhv\" width=\"690\" height=\"395\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/071e946b4e38edc5aaaf1ab6eac69fa22cb64f19_2_690x395.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/071e946b4e38edc5aaaf1ab6eac69fa22cb64f19_2_1035x592.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/071e946b4e38edc5aaaf1ab6eac69fa22cb64f19.png 2x\" data-dominant-color=\"A7A7A7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image4</span><span class=\"informations\">1070\u00d7614 79.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks</p>\n<p>Bin.</p>"], "79002": ["<p><a>Uploading: \u5c4f\u5e55\u622a\u56fe 2023-03-24 102325.png\u2026</a><br>\n<a>Uploading: \u5c4f\u5e55\u622a\u56fe 2023-03-24 102226.png\u2026</a></p>\n<p>run(\u201cSplit Channels\u201d);<br>\nselectWindow(\u201ctitle.jpg (red)\u201d);<br>\nsetAutoThreshold(\u201cDefault dark\u201d);<br>\n//run(\u201cThreshold\u2026\u201d);<br>\n//setThreshold(70, 255);<br>\nsetOption(\u201cBlackBackground\u201d, false);<br>\nrun(\u201cConvert to Mask\u201d);</p>\n<p>Can you help me see what\u2019s wrong with my script? Why always report an error:<br>\nNo window with the title \"title.jpg(red)\"found.</p>", "<p>Hi <a class=\"mention\" href=\"/u/cary\">@Cary</a>, welcome to the forum!</p>\n<p>The error message is quite informative. In order to select a window you need to know its name. There is a separate function that allows you to find the name of the currently active window: <code>getTitle()</code>. Here is a short example of how you might use it:</p>\n<pre><code class=\"lang-auto\">//-- Open a new image\nnewImage(\"example\", \"RGB noise\", 100, 100, 1);\n//-- find the name of the window and record it into a variable\ntitle=getTitle();\n//print(title);\nrun(\"Split Channels\");\n//-- Select the Red window\nselectWindow(title + \" (red)\");\n</code></pre>\n<p>Note how you can concatenate the original name with a string (in this case <code>\" (red)\"</code>) using a plus sign (<code>+</code>).</p>\n<p>It also helps others when you add code to a post, if you can use the toolbar button to format your code (or use triple back ticks <code>```</code> as above), and this really helps with readability.</p>\n<p>Hope that helps!</p>", "<p><strong>Special thanks to you, the issue has been resolved.</strong></p>"], "80538": ["<p>Hello,<br>\nI am running a CellProfiler v4.2.5 built as source on Windows 10. I have a Cell Painting pipeline. The .csv outputs produced by the ExportToSpreadsheet duplicate some Metadata columns. The duplicated columns are the same as the ones chosen in the \u2018NamesAndTypes\u2019 module in the \u2018Match metadata\u2019 part. In example - I am matching the Plate, Well and Field, and these are in the .csv files as well, adding another metadata columns (ie. \u2018Plane\u2019) duplicates them in the .csv as well. Extra metadata columns are also visible in the ExportToDatabase module in the metadata selection of the .properties file part.<br>\nI include the pipeline as well as images for testing.<br>\nThank you<br>\nViktor</p>", "<p>pipeline<br>\n<a class=\"attachment\" href=\"/uploads/short-url/v3GnYuWSJkmgFmklWqXwBJCToMO.cpproj\">CellPainting analyze_screen_properties.cpproj</a> (1.3 MB)</p>\n<p>files<br>\n<a class=\"attachment\" href=\"/uploads/short-url/9gYBngemhRxpQ1m66tQOu3ZYdbd.zip\">Viktor.zip</a> (4.7 MB)</p>"], "79005": ["<p>Hi CellProfiler community,</p>\n<p>I am trying to count the number of co-positive nuclei. However, some speckles in my DAPI channel are being detected as nuclei, which is skewing my co-positive percentages.<br>\nThere is a stark difference in intensity (but not size) between nuclei and speckles. I have tried setting pixel intensity thresholds within the \u2018IdentifyPrimaryObject\u2019 function, but this does not filter out the DAPI speckles.<br>\nHowever, I have found that the \u2018FindMaxima\u2019 function is able to pick up these speckles very well (blue particles in image). Is there any way to filter out all the particles that are detected using \u2018FindMaxima\u2019?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d.png\" data-download-href=\"/uploads/short-url/2l2DhxTkMVAnpDhOYqgstblX4Ul.png?dl=1\" title=\"image3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_234x500.png\" alt=\"image3\" data-base62-sha1=\"2l2DhxTkMVAnpDhOYqgstblX4Ul\" width=\"234\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_234x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d_2_351x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/106547ffc47d11ae81e52f46d3ad25040d12020d.png 2x\" data-dominant-color=\"191919\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image3</span><span class=\"informations\">385\u00d7820 90.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>If not, any other suggestions for removing speckles based on their higher intensity are very much welcome.</p>\n<p>Thank you in advance.</p>"], "75421": ["<p>Hi everyone, I am using CellProfiler 3.1.9 to analyze immune-fluorescent spots on cells and got an error at OverlayOutlines steps in my pipeline: \u201cError while processing OverlayOutlines: shape mismatch: value array of shape (3,) could not be broadcast to indexing result of shape (25432,2)\u201d<br>\nCan anyone help to solve this issue?<br>\nThanks, Brigitte</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a960644d95fcc26f095e200d6570cf2f4f8a9e59.png\" alt=\"image\" data-base62-sha1=\"oand5691nXOSCFj9eo0Yf9Pr2OZ\" width=\"373\" height=\"203\"></p>\n<p>font search path [\u2018C:\\Users\\wolf\\AppData\\Local\\Temp\\3\\_MEI15~2\\mpl-data\\fonts\\ttf\u2019, \u2018C:\\Users\\wolf\\AppData\\Local\\Temp\\3\\_MEI15~2\\mpl-data\\fonts\\afm\u2019, \u2018C:\\Users\\wolf\\AppData\\Local\\Temp\\3\\_MEI15~2\\mpl-data\\fonts\\pdfcorefonts\u2019]<br>\ngenerated new fontManager<br>\nC:\\Users\\wolf\\AppData\\Local\\Temp\\3_MEI15~2\\cellprofiler\\utilities\\hdf5_dict.py:539: FutureWarning: Conversion of the second argument of issubdtype from <code>int</code> to <code>np.signedinteger</code> is deprecated. In future, it will be treated as <code>np.int32 == np.dtype(int).type</code>.<br>\nC:\\Users\\wolf\\AppData\\Local\\Temp\\3_MEI15~2\\cellprofiler\\utilities\\hdf5_dict.py:541: FutureWarning: Conversion of the second argument of issubdtype from <code>float</code> to <code>np.floating</code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type</code>.<br>\nC:\\Users\\wolf\\AppData\\Local\\Temp\\3_MEI15~2\\matplotlib\\axes_base.py:1428: MatplotlibDeprecationWarning: The \u2018box-forced\u2019 keyword argument is deprecated since 2.2.<br>\nC:\\Users\\wolf\\AppData\\Local\\Temp\\3_MEI15~2\\centrosome\\cpmorphology.py:4209: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use <code>arr[tuple(seq)]</code> instead of <code>arr[seq]</code>. In the future this will be interpreted as an array index, <code>arr[np.array(seq)]</code>, which will result either in an error or a different result.<br>\nC:\\Users\\wolf\\AppData\\Local\\Temp\\3_MEI15~2\\centrosome\\cpmorphology.py:416: FutureWarning: Conversion of the second argument of issubdtype from <code>float</code> to <code>np.floating</code> is deprecated. In future, it will be treated as <code>np.float64 == np.dtype(float).type</code>.<br>\nC:\\Users\\wolf\\AppData\\Local\\Temp\\3_MEI15~2\\skimage\\util\\arraycrop.py:175: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use <code>arr[tuple(seq)]</code> instead of <code>arr[seq]</code>. In the future this will be interpreted as an array index, <code>arr[np.array(seq)]</code>, which will result either in an error or a different result.<br>\nC:\\Users\\wolf\\AppData\\Local\\Temp\\3_MEI15~2\\skimage\\util\\arraycrop.py:177: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use <code>arr[tuple(seq)]</code> instead of <code>arr[seq]</code>. In the future this will be interpreted as an array index, <code>arr[np.array(seq)]</code>, which will result either in an error or a different result.<br>\nFailed to run module OverlayOutlines<br>\nTraceback (most recent call last):<br>\nFile \u201cCellProfiler\\cellprofiler\\gui\\pipelinecontroller.py\u201d, line 2919, in do_step<br>\nFile \u201cCellProfiler\\cellprofiler\\pipeline.py\u201d, line 2034, in run_module<br>\nFile \u201cCellProfiler\\cellprofiler\\modules\\overlayoutlines.py\u201d, line 231, in run<br>\nFile \u201cCellProfiler\\cellprofiler\\modules\\overlayoutlines.py\u201d, line 334, in run_color<br>\nFile \u201cCellProfiler\\cellprofiler\\modules\\overlayoutlines.py\u201d, line 355, in draw_outlines<br>\nFile \u201csite-packages\\skimage\\segmentation\\boundaries.py\u201d, line 230, in mark_boundaries<br>\nValueError: shape mismatch: value array of shape (3,) could not be broadcast to indexing result of shape (25432,2)</p>", "<p>Hi Brigitte - can you upload your pipeline and an image set that generates this issue? It will be difficult for us to help troubleshoot this error from just this. Thank you so much!</p>", "<p>Hi Beth, thanks for your answer, here are the pipeline and some images.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/7x24rnPOGxBruPPA6zMsnJ4stuD.cpproj\">20221027_DNAdamage_SW480_20221228.cpproj</a> (431.8 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/g2vzuj3zm6163Zi3iwEGQYjYMHP.czi\">20221027_SW480_3hCis20_mAF488gH2AX_63_1.czi</a> (2.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/qsGZpFiVn80Taf3SHKCBBOgC7zo.czi\">20221027_SW480_3hCis20_mAF488gH2AX_63_2.czi</a> (2.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/c5cuHxQwMO8LjkykaKX2jLOjOcX.czi\">20221027_SW480_3hCis20_mAF488gH2AX_63_3.czi</a> (2.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/hBy23rpp7Pnb2TEyFV2AQgKaY63.czi\">20221027_SW480_3hOx50_mAF488gH2AX_63_1.czi</a> (2.2 MB)</p>", "<p>Hi <a class=\"mention\" href=\"/u/brigitte\">@Brigitte</a>,</p>\n<p>Thanks for sharing your pipeline and images.</p>\n<p>I saw you\u2019re using czi files as input, and it\u2019s perfect, it works, but when you try to overlay outline objects in the top of these czi files there is a shape mismatch between the czi file and the objects, this is why you\u2019re receiving the error.</p>\n<p>To solve that you can add to your pipeline a GrayToColor module to create a RGB image that you can use to overlay the outline of your objects.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6db4f64ff513c68efece05a2e3b623724ed04a90.png\" data-download-href=\"/uploads/short-url/fEvKK2FMioO30wt4TpiuFpM1Iw8.png?dl=1\" title=\"Screen Shot 2023-01-09 at 3.30.08 PM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6db4f64ff513c68efece05a2e3b623724ed04a90_2_690x304.png\" alt=\"Screen Shot 2023-01-09 at 3.30.08 PM\" data-base62-sha1=\"fEvKK2FMioO30wt4TpiuFpM1Iw8\" width=\"690\" height=\"304\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6db4f64ff513c68efece05a2e3b623724ed04a90_2_690x304.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6db4f64ff513c68efece05a2e3b623724ed04a90.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6db4f64ff513c68efece05a2e3b623724ed04a90.png 2x\" data-dominant-color=\"EBECED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-01-09 at 3.30.08 PM</span><span class=\"informations\">844\u00d7373 107 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/6/d61114fd7ad2afaf4f49014c1e28639e907d3b90.png\" data-download-href=\"/uploads/short-url/uxIM9xQpCgoE9A1QHxCaZeCVi4U.png?dl=1\" title=\"Screen Shot 2023-01-09 at 3.31.02 PM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d61114fd7ad2afaf4f49014c1e28639e907d3b90_2_690x343.png\" alt=\"Screen Shot 2023-01-09 at 3.31.02 PM\" data-base62-sha1=\"uxIM9xQpCgoE9A1QHxCaZeCVi4U\" width=\"690\" height=\"343\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d61114fd7ad2afaf4f49014c1e28639e907d3b90_2_690x343.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/6/d61114fd7ad2afaf4f49014c1e28639e907d3b90.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/6/d61114fd7ad2afaf4f49014c1e28639e907d3b90.png 2x\" data-dominant-color=\"E9E9EE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-01-09 at 3.31.02 PM</span><span class=\"informations\">938\u00d7467 127 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Like here:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/1716851605b56bf0de158f8e2a230711ebc49eea.png\" data-download-href=\"/uploads/short-url/3ifehHYKCB66ySUheygt8PCOrTY.png?dl=1\" title=\"Screen Shot 2023-01-09 at 3.31.20 PM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/1716851605b56bf0de158f8e2a230711ebc49eea_2_690x347.png\" alt=\"Screen Shot 2023-01-09 at 3.31.20 PM\" data-base62-sha1=\"3ifehHYKCB66ySUheygt8PCOrTY\" width=\"690\" height=\"347\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/1716851605b56bf0de158f8e2a230711ebc49eea_2_690x347.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/1716851605b56bf0de158f8e2a230711ebc49eea.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/1716851605b56bf0de158f8e2a230711ebc49eea.png 2x\" data-dominant-color=\"415F3F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-01-09 at 3.31.20 PM</span><span class=\"informations\">935\u00d7471 325 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best,<br>\nMario</p>", "<p>Hi Mario, thanks a lot for this helpful input!<br>\nI am just a bit confused, because I never had this problem before, for example, the following pipeline works perfectly:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/9vMT6FmMWb0Y8gnMS8HA4Yn890x.cpproj\">20220222_DNAdamage_SW480_20220302_20230110.cpproj</a> (434.6 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/wRm3XnVJhvO3bXOs9XIHTuo3hp5.czi\">20220222_SW480_1hOx75_mAF488gH2AX_rAF546YB1_63_2.czi</a> (3.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/oGDRAC81sSfGYbfiHB8RZxceYaR.czi\">20220222_SW480_1hOx75_mAF488gH2AX_rAF546YB1_63_3.czi</a> (3.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/7WwPITdejDL54LhnpA6Q9ZRglLy.czi\">20220222_SW480_1hOx75_mAF488gH2AX_rAF546YB1_63_1.czi</a> (3.2 MB)</p>", "<p>Hi <a class=\"mention\" href=\"/u/brigitte\">@Brigitte</a>,</p>\n<p>Sure, In this case (2nd pipeline and images) the images you\u2019re using as input have 3 channels (RGB), so in these images we have an array of shape (3,). Your first post images (czi files) just have two channels (Red and Green) and because of that the shape of your image is (2,).</p>\n<p>Best,<br>\nMario</p>", "<p>Thanks a lot! Best, Brigitte</p>"], "76447": ["<p>Hi,<br>\nI am new to cell profiler and am having difficulty using the Erben_2017 pipeline in the newest version of Cell Profiler as the Measure Image Intensity feature is different and results in some of the following modules having unavailable images (see picture) . I tried installing the earlier version 2.0 which the pipeline was created in but wasn\u2019t able to get the software to open so I would like to be able to use this pipeline in the newest version.</p>\n<p>Can anyone suggest how to fix the 3 identify primary objects modules that follow the measure image intensity to run correctly?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/5/85359409a58ae6c9a1fd6b04b42e61331a787407.jpeg\" data-download-href=\"/uploads/short-url/j0qig0QWl3OcUFItCR1uVap21z9.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/5/85359409a58ae6c9a1fd6b04b42e61331a787407_2_690x387.jpeg\" alt=\"image\" data-base62-sha1=\"j0qig0QWl3OcUFItCR1uVap21z9\" width=\"690\" height=\"387\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/5/85359409a58ae6c9a1fd6b04b42e61331a787407_2_690x387.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/5/85359409a58ae6c9a1fd6b04b42e61331a787407_2_1035x580.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/5/85359409a58ae6c9a1fd6b04b42e61331a787407_2_1380x774.jpeg 2x\" data-dominant-color=\"EEEEEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71078 130 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "79520": ["<p>Hi <a class=\"mention\" href=\"/u/aklemm\">@aklemm</a> <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> ,</p>\n<p>I have XYT TIFF stacks, so each plane a frame in a timelapse movie.<br>\nIdeally I would like to segment those images and then save XXT label mask TIFF stacks from within CellProfiler. Is that possible at all?</p>\n<p>Or do I need to work with TIFF sequences, like: <code>im_t0.tif, im_t1.tif, ...</code>?</p>", "<p>Hi,</p>\n<p>We figured something out:</p>\n<p>In <code>SaveImages</code> one can select <code>Movie/Stack</code> and then use the <code>T</code> metadata for the grouping!</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/e/9ef825893b80738e529c627d805d95294061b435.jpeg\" data-download-href=\"/uploads/short-url/mGj5HJNC2t5zGaIJ94E2bY76u57.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/e/9ef825893b80738e529c627d805d95294061b435_2_690x450.jpeg\" alt=\"image\" data-base62-sha1=\"mGj5HJNC2t5zGaIJ94E2bY76u57\" width=\"690\" height=\"450\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/e/9ef825893b80738e529c627d805d95294061b435_2_690x450.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/e/9ef825893b80738e529c627d805d95294061b435.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/e/9ef825893b80738e529c627d805d95294061b435.jpeg 2x\" data-dominant-color=\"CCCCCC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">990\u00d7646 70.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>CellProfiler is so awesome <img src=\"https://emoji.discourse-cdn.com/twitter/heart.png?v=12\" title=\":heart:\" class=\"emoji\" alt=\":heart:\" loading=\"lazy\" width=\"20\" height=\"20\">!</p>\n<p>However, now another little issue, we do not manage the save the path to the created <code>Movie/Stack</code> in the <code>Image.txt</code> output table. We are selecting it here (<code>WormLabel</code>):</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6fbab0185945ea58a1095ad294a7cb32bda72c3d.png\" alt=\"image\" data-base62-sha1=\"fWoYwhtz6HluMxun7bd8fPk8Sx7\" width=\"456\" height=\"332\"></p>\n<p>But the Image table does not contain the column.</p>\n<p>My suspicion: CellProfiler is confused as there is now only one file for many timepoints and it somehow does not know how to fill all the rows in the table\u2026it would be great if that could be fixed (or if we find out what we are doing wrong).</p>", "<p>ping <a class=\"mention\" href=\"/u/gnodar\">@gnodar</a> <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "81576": ["<p>Hello there,</p>\n<p>Is Cellpose plugin for Cellprofiler is limited to <a href=\"https://github.com/CellProfiler/CellProfiler-plugins/tree/master\" rel=\"noopener nofollow ugc\">version 1.0</a>?  Also, are those plugins (Cellpose and Stardist) are limited to Python 3.8?  Basically, what is the latest in using Cellpose and Stardist in Cellprofiler?</p>\n<p>Thanks a lot,<br>\nJama.</p>"], "80552": ["<p>Hello,</p>\n<p>it seems there is an issue with the MeasureImageSkeleton module. It reports what seems to be a wrong number of branches and end-points in a clean skeleton image. Here is a fragment showing its strange behavior:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/0/90a73f3c99a5d4f6cb223cc0f2df870752c3cad4.png\" data-download-href=\"/uploads/short-url/kDF50HPHFByBtHqAQtbjGVbnnne.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/0/90a73f3c99a5d4f6cb223cc0f2df870752c3cad4_2_690x237.png\" alt=\"image\" data-base62-sha1=\"kDF50HPHFByBtHqAQtbjGVbnnne\" width=\"690\" height=\"237\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/0/90a73f3c99a5d4f6cb223cc0f2df870752c3cad4_2_690x237.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/0/90a73f3c99a5d4f6cb223cc0f2df870752c3cad4.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/0/90a73f3c99a5d4f6cb223cc0f2df870752c3cad4.png 2x\" data-dominant-color=\"6B6B6A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">795\u00d7274 13.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>In the picture above, you can see that it reports the node correctly, but for some reason finds a sequence of end-points.</p>\n<p>But in the following image, everything is OK (another fragment of the same image).</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/57a7301b2c043e7931a72f722e61c7a5cd3cba3c.png\" alt=\"image\" data-base62-sha1=\"cvpJMlBYcdmpm9Wxc1wE1Y5MwxK\" width=\"682\" height=\"252\"></p>\n<p>What is wrong?<br>\nAdditionally, it would be nice to get a report on the number of nodes. The present module does not report it at all.</p>\n<p>I use Cellprofiler 4.2.5.</p>\n<p>I\u2019m attaching the entire project that demonstrates the issue including the image.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/eYF2xC9z5OChHUy4Y9Sj330ZypT.zip\">issue.zip</a> (22.8 KB)</p>\n<p>Warm regards to everyone,<br>\nAlex</p>"], "79529": ["<p>.  I have tried several variations of the worm pipeline but I just can\u2019t get it to work properly on my image.  I am very new to this stuff but have spent several days toying with Cell-profiler but have gotten nowhere.  I just wanted to straighten out the worms and get fat measurements<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/e/5e118872bacf4914958b4349f9e324634601fd38.jpeg\" data-download-href=\"/uploads/short-url/dqarGm7QyzznpiivruQFSZxmFAs.jpeg?dl=1\" title=\"470s#1_res\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5e118872bacf4914958b4349f9e324634601fd38_2_690x388.jpeg\" alt=\"470s#1_res\" data-base62-sha1=\"dqarGm7QyzznpiivruQFSZxmFAs\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5e118872bacf4914958b4349f9e324634601fd38_2_690x388.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5e118872bacf4914958b4349f9e324634601fd38_2_1035x582.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5e118872bacf4914958b4349f9e324634601fd38_2_1380x776.jpeg 2x\" data-dominant-color=\"F2F0F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">470s#1_res</span><span class=\"informations\">1920\u00d71080 40.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n.  I simply don\u2019t understand what is going wrong.  I will gladly pay someone for their time.</p>\n<p>I tried this on both cell profiler 2.1 and 2.4 using the standard pipelines and trying to build my own but nothing worked</p>"], "78506": ["<p><a class=\"attachment\" href=\"/uploads/short-url/jQePmHyYW6r8Evkll47Ttihr2ZQ.tif\">DIESTRUS All runs.lif_Run 3 F55 Diestrus A 10_ch03.tif</a> (616.6 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/7DHjXTkuinN7E1ikMtkE3wzaqct.tif\">DIESTRUS All runs.lif_Run 3 F55 Diestrus B 32_ch00.tif</a> (878.4 KB)</p>\n<p>Background:<br>\nMarked sections with four stains: Nuclei, Dnmt, Vasopressin and Cre (The last three are just three different proteins). Images attached show two of these stains on the same section as an example.</p>\n<p>Analysis goal:<br>\nI\u2019m trying to understand whether these proteins coexpress with eachother. For example, the cells that are expressing Dnmt, do they also express Cre? Do they also express Vasopressin? The nuclei stain is just to mark cells.</p>\n<p>Challenges:<br>\nI dont know which cell profiler modules I should use for this type of analysis. I tried doing MaskImages and then MeasureObjectIntensity, but I don\u2019t understand what the resulting intensity values mean. Should I use Relate objects instead? Is Cell profiler the right software for this type of analysis?</p>"], "80044": ["<p>Hi Everyone,</p>\n<p>I am curious how cellprofiler scales intensities and what the proper procedure is for what I\u2019m looking to do. I am looking to take measurements on single frames with red and green channels. I understand if I understand correctly, cellprofiler normalizes intensity values from 0-1 and scales to the mean? This is not optimal when trying to compare cells that should have different fluorescence intensities. Is the correct way to handle these images setting the intensity range manually during import?</p>\n<p>Normally, I would open up multiple images in Fiji and adjust the brightness/contrast then \u201cpropagate to all open images\u201d. Is there an analogous function in cellprofiler?</p>\n<p>The HistogramMatching tool looks like what I would want except it only applies to frames not across different images. Am I missing something?</p>\n<p>Thanks in advance!</p>\n<p>Ryan</p>", "<p>Hello Ryan,</p>\n<p>You are right in that CellProfiler converts intensities to be between 0 and 1. Then, CellProfiler display contrast stretches images <strong>for display only</strong>. However, all analysis is performed on the <strong>non-contrast stretched values</strong>.</p>\n<p>If you would like to change how CellProfiler displays your pixel data, right-click on a displayed image, select <strong>Adjust Contrast</strong> and then change the <strong>Normalization Mode</strong> to <strong>raw</strong>.</p>\n<p>If you would like to rescale the intensity of an image (for example, <code>0</code> - <code>1</code> becomes <code>0.1</code> - <code>0.9</code>), you can use the <strong>RescaleIntensity</strong> module. In this module, you can set custom low and high intensity ranges for each image independently, too.</p>", "<p>This is very helpful thank you!</p>\n<p>Do you have any suggestions for normalizing the raw intensity values in cellprofiler? Am I correct in doing so by using NamesAndTypes \u2192 \u201cSet intensity range from\u201d = Manual, then choosing an arbitrary value determined by looking at the images in ImageJ? Is there a more standardized way to do this in cellprofiler?</p>\n<p>Best regards,</p>\n<p>Ryan</p>", "<p>What is the purpose of changing the intensity range as you suggest? If it\u2019s simply the case for uniformly making the objects within your image easier to see (not analyse), this is handled automatically by CellProfiler\u2019s display normalization, as mentioned above. If it\u2019s to make particular objects (for example a nucleus) easier to segment, modules like <strong>IdentifyPrimaryObjects</strong> should still be able to segment from the image without you adjusting the image histogram manually.</p>\n<p>You originally mentioned that different cells in the same image may have different fluorescence intensities. Are there any markers for these cells that do not change that could be used to segment them (eg a nuclear stain)? Alternatively, you could run two <strong>IdentifyPrimaryObjects</strong> modules, one to identify the bright cells and the other to identify the dim cells. It\u2019s hard to recommend a strategy without seeing the images themselves, though.</p>", "<p>Apologies for the confusion. My goal is to have all images from the same channel across different samples scaled to the same range. This way cells with disrupted localization show a dim diffuse signal rather than a scaled signal. I\u2019ve attached an example image where the wild type on the left looks how it should but the mutant on the right has been scaled so that the signal appears brighter.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/nrzKIe9veWQb5m0qejgGWIlGFyG.pdf\">exampleimage.pdf</a> (3.3 MB)</p>\n<p>From your previous response it sounds like changing the normalization mode to raw will fix the issue for visualization purposes.</p>\n<p>I typed the questions below but I think I figured it out before replying. The raw images have a range of 0-65535. The default setting in ImageJ is to autoscale. This is where my confusion was coming from. Since you mentioned the measurements use the raw images I believe that answers my questions below.</p>\n<blockquote>\n<p>What isn\u2019t clear to me though is how colocalization with a marker in a different channel would be affected if the maximum intensity isn\u2019t set to the same value across all green channel images (please forgive my ignorance of the statistics)</p>\n<p>I could in theory normalize the green channels relative to the red channel but then wouldn\u2019t I still want to have the same intensity range across all of the red channel images?</p>\n</blockquote>\n<p>Thank you!</p>\n<p>Ryan</p>", "<p>Great! There\u2019s a tutorial on colocalization analysis that you can find here: <a href=\"https://cellprofiler.org/examples\" class=\"inline-onebox\">Examples | CellProfiler</a>, which will hopefully include some solutions that you can apply to your images too.</p>", "<p>Hi,</p>\n<p>just to add one thing, that wasn\u2019t mentioned so far \u2013 the Cellprofiler help actually describes very well what happens in the NamesAndTypes section:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/c/9c323c6d8c5ed6f4a4a8b77dc32bfbcbdf144cf1.png\" data-download-href=\"/uploads/short-url/mhM7asiZJpLkHvf0b5eSIxCaxTH.png?dl=1\" title=\"cp_range_help\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/c/9c323c6d8c5ed6f4a4a8b77dc32bfbcbdf144cf1.png\" alt=\"cp_range_help\" data-base62-sha1=\"mhM7asiZJpLkHvf0b5eSIxCaxTH\" width=\"690\" height=\"255\" data-dominant-color=\"E8E8E8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">cp_range_help</span><span class=\"informations\">855\u00d7317 13.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><a class=\"mention\" href=\"/u/ryan_feathers\">@Ryan_Feathers</a> when your raw images have a 16bit-range, they will be simply divided by 65535 when choosing \u201cImage metadata\u201d which is the default. Therefore all images and pixels will be handled equally, preserving any differences in intensity. For later data processing/ statistics, you can simply multiply the intensity values by 65535 if you for some reason need to show the original intensity AU in your results.</p>\n<p>There is only one module I would like to highlight in terms of being careful with intensity rescaling and that is the EnhanceOrSuppressFeatures module and the \u201cRescale result image\u201d option. Setting this to \u201cyes\u201d may cause issues if you have images that are overall very dim or almost empty. I think this setting works mainly well, when the intensity distribution is equal across images. At least I usually set this to \u201cNo\u201d because it can produce non-sense data when images are almost empty (e.g. during screening because of cell death).</p>\n<p>Regards, Anna</p>", "<p>Hi Anna,</p>\n<p>Thank you for sharing, this is really helpful!</p>\n<p>One of the challenges I\u2019m still facing is that my wild type cells have fairly uniform fluorescence and the mutants are highly variable. This is common for yeast plasmid expression and we will usually just ignore the bright outliers (1-2 per image out of 20 or more cells).</p>\n<p>I\u2019ve attempted a couple different strategies to exclude the outliers from analysis (various thresholding workflows, masking, etc.) but I haven\u2019t been able to get it working optimally.</p>\n<p>Since my data is somewhat low throughput my next idea was to crop out individual cells of interest. I made an ImageJ plugin to do this quickly and reproducibly so my plan now is to feed boxed images of cells into cellprofiler.</p>\n<p>Now that I know cellprofiler uses raw intensities, as long as my cropped images are saved as raw 16 bit images then I should not need any intensity normalization for measuring colocalization correct?</p>\n<p>Regarding the normalization for display: Changing it the normalization mode to Raw resulted in a black image at all values for the sliders. I\u2019m guessing this is because the signal occupies such a small range and the sliders are 0-255? I tried HistogramMatching to normalize the green channel (highly variable fluorescence) to the red channel (mostly uniform and roughly equivalent fluorescence across all samples) and this displays results that look pretty reasonable.</p>\n<p>My only concern with this approach is if the mutant that\u2019s in the green channel affects the fluorescence of the red marker this will be masked by how the red channel is normalized.</p>\n<p>Thank you both again for the help and suggestions!</p>\n<p>Best,</p>\n<p>Ryan</p>"], "78504": ["<p>Hi,</p>\n<p>I am wondering what is the reason behind very long analysis times in CP when analysing multiple folders in one run. A single plate takes 30-40 minutes, but when I put all 27 folders (=plates), the analysis is much longer than 40 min * 27. It goes on for. 3-4 days, instead &lt;1 day.</p>\n<p>Anyone knows any explanation for this?</p>\n<p>Best,<br>\nBartek</p>", "<p>It looks like you\u2019re running out of memory. Check if this is the case by monitoring memory usage while running your analysis. If that\u2019s not the case, you\u2019ll have to provide more info.</p>", "<p>Thanks for your reply!</p>\n<p>I\u2019ve restarted the analysis to now monitor the memory usage. When I saw the analysis running slow today, there was still plenty memory left (out of 512 GB, less than 5% was being used). I\u2019ll report back how does the resource utilisation look like when it\u2019s actively processing the entire set.</p>\n<p>What kind of info will be useful in this case?</p>", "<p>Within the first 30 minutes, the high point of usage was 99% for CPU and 17% memory (around 70 GB). My pipeline isn\u2019t very complex (attached screenshot), but I wonder whether the ExportToSpreadhseet isn\u2019t the cause for slow processing. It\u2019s over 35000 image sets in the end\u2026</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b.jpeg\" data-download-href=\"/uploads/short-url/dvn0zXIQZZEXD6tHvJQ3hJQSLEL.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_690x340.jpeg\" alt=\"image\" data-base62-sha1=\"dvn0zXIQZZEXD6tHvJQ3hJQSLEL\" width=\"690\" height=\"340\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_690x340.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_1035x510.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/e/5ea8168ef2e55f82238866082182b9e007c2be4b_2_1380x680.jpeg 2x\" data-dominant-color=\"EEEEED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7948 153 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<blockquote>\n<p>What kind of info will be useful in this case?</p>\n</blockquote>\n<p>Operating system and CP version but also possibly some details of the workflow.</p>", "<blockquote>\n<p>whether the ExportToSpreadhseet isn\u2019t the cause for slow processing</p>\n</blockquote>\n<p>If that\u2019s the case, try exporting to database instead.</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5.jpeg\" data-download-href=\"/uploads/short-url/5HG9zIs3zlq9G1doLBjgQ2hXdbv.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_690x199.jpeg\" alt=\"image\" data-base62-sha1=\"5HG9zIs3zlq9G1doLBjgQ2hXdbv\" width=\"690\" height=\"199\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_690x199.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_1035x298.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/7/27fae44a096c3b072af42ceef4dea0d7f2636ff5_2_1380x398.jpeg 2x\" data-dominant-color=\"F8F8F9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1830\u00d7528 111 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>CP 4.2.1</p>\n<p>Workflow screenshot in the previous post.</p>", "<p>Tried it. Usage of resources was the same, but in this case \u2018CellProfiler.exe stopped working\u2019 after 10-15 minutes from the moment it started running modules</p>", "<p>If the database set up is fine then it looks like CP may have some issue writing to disk. Unfortunately I don\u2019t know much about Windows to help with troubleshooting this kind of issue. Maybe someone more knowledgeable will come along.</p>"], "81072": ["<p>Hi CellProfiler Community,</p>\n<p>I am adapting a published pipeline ( Yeast colony classification) to count colonies in petridishes taken in rgb format. The modified pipeline is working well on my plates but however the pipeline stops after analyzing the first image in the list.  I am using a common phrase in the images to direct the pipeline to analyze all the images in the folder or list and export a spreadsheet with analysis from all plates.<br>\nI have around 100  images to analyze and I am trying to use my workstation (used for NGS analysis) for local processing. Do i still need to add createbatchfiles module to the end of pipeline or it can handle processingmultiple files with the \u201ccreatebatchfiles\u201d module? If so, what could be issue in the pipeline where I am using a single phrase common to all files? I have attached the modified pipeline and the few images</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/cBu5xqKU7QXznfqSZPSomXWdQZR.tif\">Binary_4.tif</a> (1.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/x3rIpCUzN3q4BigkUUX6P6SWedk.cpproj\">KountStrePula_ver3.cpproj</a> (840.6 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/pGscH7OalIvQUXY9i9vlgZGeOHS.tif\">vfr-4-24-0uM 16h08m47s.tif</a> (3.6 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/hQ4CVUVyKRNTWa5QqWRmKbSxj6m.tif\">vfr-4-24-0uM 16h11m30s.tif</a> (3.6 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/j9fUpnpYHfZigRyREnV7AdXO0vf.tif\">vfr-4-24-1uM 16h22m46s.tif</a> (3.6 MB)</p>\n<p>Thanks</p>"], "81078": ["<aside class=\"onebox googledrive\" data-onebox-src=\"https://drive.google.com/file/d/1mFd7Urqw1RGrrVb0VT7iPl-DgP6ZJH_U/view\">\n  <header class=\"source\">\n\n      <a href=\"https://drive.google.com/file/d/1mFd7Urqw1RGrrVb0VT7iPl-DgP6ZJH_U/view\" target=\"_blank\" rel=\"noopener nofollow ugc\">drive.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https://drive.google.com/file/d/1mFd7Urqw1RGrrVb0VT7iPl-DgP6ZJH_U/view\" target=\"_blank\" rel=\"noopener nofollow ugc\"><span class=\"googledocs-onebox-logo g-drive-logo\"></span></a>\n\n\n\n<h3><a href=\"https://drive.google.com/file/d/1mFd7Urqw1RGrrVb0VT7iPl-DgP6ZJH_U/view\" target=\"_blank\" rel=\"noopener nofollow ugc\">POSITION_ IMAGE ANALYST VMCF Prague 2023.pdf</a></h3>\n\n<p>Google Drive file.</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "74422": ["<p>Hello.</p>\n<p>Recently I downloaded CellProfiler.<br>\nHowever, after starting up the application, the following screen appears and the application does not start normally.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6ab3ce31ea58a13c4bd994a7469fe2ec44fcc32.png\" data-download-href=\"/uploads/short-url/wUAH08EvU1sotkgg9ZMRmBodi5Y.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6ab3ce31ea58a13c4bd994a7469fe2ec44fcc32_2_690x359.png\" alt=\"image\" data-base62-sha1=\"wUAH08EvU1sotkgg9ZMRmBodi5Y\" width=\"690\" height=\"359\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6ab3ce31ea58a13c4bd994a7469fe2ec44fcc32_2_690x359.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6ab3ce31ea58a13c4bd994a7469fe2ec44fcc32_2_1035x538.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6ab3ce31ea58a13c4bd994a7469fe2ec44fcc32_2_1380x718.png 2x\" data-dominant-color=\"1B1B1B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1466\u00d7764 20.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I have also checked the environment variable settings with reference to the troubleshooting (<a href=\"https://github.com/CellProfiler/CellProfiler/wiki/Windows-Installation-Troubleshooting\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Windows Installation Troubleshooting \u00b7 CellProfiler/CellProfiler Wiki \u00b7 GitHub</a>), but still no improvement.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/a/ea4249361f4ea33582028c74ce0dfb0a624fc1dd.png\" data-download-href=\"/uploads/short-url/xqlKclktNFlqztk72jg95RTJEgR.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/a/ea4249361f4ea33582028c74ce0dfb0a624fc1dd_2_454x499.png\" alt=\"image\" data-base62-sha1=\"xqlKclktNFlqztk72jg95RTJEgR\" width=\"454\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/a/ea4249361f4ea33582028c74ce0dfb0a624fc1dd_2_454x499.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/a/ea4249361f4ea33582028c74ce0dfb0a624fc1dd_2_681x748.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/a/ea4249361f4ea33582028c74ce0dfb0a624fc1dd.png 2x\" data-dominant-color=\"EAEDEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">879\u00d7968 34.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Please help me.</p>", "<p>Hello <a class=\"mention\" href=\"/u/k.takiguchi\">@K.Takiguchi</a></p>\n<p>Can you confirm that you\u2019ve downloaded Visual C++ Redistributable according to the Cellprofiler website? See here, copied directly from the download page of Cellprofiler:</p>\n<p>If CellProfiler will not open, you may need to install the Visual C++ Redistributable available at <a href=\"https://aka.ms/vs/16/release/VC_redist.x64.exe\" rel=\"noopener nofollow ugc\">this link</a>.</p>", "<p>Thank you for your reply.<br>\nI have already downloaded the file you mentioned,<br>\nbut it did not solve the problem.</p>"], "79032": ["<p>Hello,</p>\n<p>I am analyzing large multiplexed images (14000x14000 pixels and 24 channels). I am counting the number of cells (roughly 250,000 cells) using IdentifyPrimaryObjects in one channel that is stained with DAPI. Then I create a cytoplasm mask using ExpandOrShrinkObjects then subsequently MaskObjects. I am measuring the intensity of all 24 channels using MeasureObjectIntensity for each cell in both cellular compartments (cytoplasm and nucleus). When I run ExportToDatabase, the file is created; however, it fails to ever populate with the data nor does the pipeline ever finish. I can pause and stop Cellprofiler using the buttons in the cellprofiler GUI, so it isn\u2019t actually \u201cfrozen\u201d in the traditional sense. If I instead create 24 cellprofiler pipelines, each of which measures one channel and exports the cytoplasmic and nuclear data for this one channel, the program works fine; however, the problem arises when all channels are analyzed at once. This to me indicates that it isn\u2019t a problem with my data but some limitation of the ExportToDatabase module. I am very interested in getting this issue resolved; however, I have no way to troubleshoot since no error occurs. Is there a limitation for ExportToDatabase that I am not aware of? This is highly disruptive to my workflow as I would like to analyze several sets of these multiplexed images.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/rNcph0XFDsl8fBsKZbBB4d0y9Bq.cpproj\">CyCIF analysis.cpproj</a> (4.5 MB)</p>", "<p>To further understand why exporttodatabase.py seems to \u201cget stuck\u201d, I\u2019ve used <a href=\"https://github.com/benfred/py-spy\" rel=\"noopener nofollow ugc\">Py-Spy</a> to track cell profiler process. I am by no means knowledgeable in Python, but it seems that the process stops making any function calls after several minutes of running the exporttodatabase.py script. This is in contrast to all other modules in which the functions and lines are clearly detected by Py-Spy. I can\u2019t tell the exact line in which the script gets stuck but it seems that the majority of the time is spent on ExportToDatabase.py lines 3850-3853 until the script stops running anything. The process is still using CPU; however, nothing is detected by Py-Spy. I\u2019m not sure how to interpret this\u2013maybe a script written in another language is called? Or perhaps there is truly an issue with the export module.</p>\n<p>Furthermore, I believe this to be an error exclusive to ExportToDatabase, as ExportToSpreadsheet correctly exports the data as expected.</p>", "<p>I\u2019ve noticed another issue with ExportToSpreadsheet with very large (1M cells/image) datasets. When measuring multiple channels, I receive the following error when exporting to a spreadsheet. Since the eof is 2^32-1 I believe this is a limitation in either CellProfiler or Python since it seems HDF5 files do not have size limitations. Hopefully this can be corrected.</p>\n<p>Unable to open file (truncated file: eof = 2147483647, sblock-&gt;base_addr = 0, stored_eof = 2783469115)</p>", "<p>I will continue to post updates here, but I am hoping to leverage the expertise of <a class=\"mention\" href=\"/u/cellprofilerteam\">@CellProfilerTeam</a> on this issue after a recent discussion with the h5py team. As I described in a recent <a href=\"https://github.com/h5py/h5py/issues/2250\" rel=\"noopener nofollow ugc\">Github</a> post for h5py, I originally believed this to be an issue with opening a large h5py file; however, since the h5py team indicated this isn\u2019t an issue with their package, it seems that Cellprofiler fails fully populate the h5py file after 2^31-1 entries, resulting in a truncated file. I would appreciate some guidance on resolving this issue. Perhaps I am too optimistic that it could be resolved by simply changing the variable type to support greater than 2^32-1 entries into the hdf5 file. I\u2019m not sure why this issue is handled differently for exporttodatabase (freezing) versus exporttospreadsheet (truncated file), but I hope resolving the truncated hdf5 file problem could fix both issues.</p>"], "76996": ["<p>I would like to perform cell segmentation on fixed cells, the dyes I know of are Phalloidin (too expensive) and Calcein AM (cannot be fixed), so are there any recommended fluorescent dyes that can be used for fixed cell segmentation?</p>", "<p>How about a membrane dye?</p>", "<p>Hi <a class=\"mention\" href=\"/u/gavyn\">@Gavyn</a>,</p>\n<p><a href=\"https://www.thermofisher.com/order/catalog/product/de/en/C37608\">CellMask</a> might be an option.</p>\n<p>Note the vendors indication: <em>The stain survives fixation, but not permeabilization.</em></p>", "<p>We routinely use this <a href=\"https://www.abcam.com/phalloidin-ifluor-594-reagent-ab176757.html\" rel=\"noopener nofollow ugc\">Phalloidin</a> from Abcam which I have found to be really affordable compared to other vendors (1/4 of the price) - and staining is always perfect! You also use it at 1:1000, so it goes this distance even in dense spheroid samples. We have the 488, 594 and 647 nm versions - just note that you need to reconstitute the 647 product yourself in DMSO.</p>\n<p>You could also look into something like PKH26 - while it\u2019s primarily for use in live cells, stained cells can be fixed if they\u2019re being imaged within a few weeks (although there might be vendor variability on that).</p>\n<p>Seb</p>", "<p>Thank you very much for your suggestions, I believe these suggestions will help my project.</p>", "<aside class=\"quote no-group\" data-username=\"Marie-nkaefer\" data-post=\"2\" data-topic=\"76996\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/marie-nkaefer/40/21605_2.png\" class=\"avatar\"> Marie Held:</div>\n<blockquote>\n<p>How about a membrane dye?</p>\n</blockquote>\n</aside>\n<p>Do you have any recommended membrane dyes?</p>", "<p>It\u2019s not actually SUPPOSED to be a whole-cell dye, but we routinely use Syto14 in the <a href=\"https://www.biorxiv.org/content/10.1101/2022.07.13.499171v2.full\">CellPainting assay</a> to measure RNA, and then also to do cell segmentation on, because you get a small amount of fluorescence throughout the whole cell in addition to the higher concentrations in the places like the nucleoli.</p>", "<p>CellMask<br>\nPKH26</p>"], "79557": ["<p>Hello Everyone,</p>\n<p>I implemented the cell segment function of cellpose 2.2 in my web project.<br>\nBut I am not sure of the meaning of channel input in cellpose.<br>\nAs all of you know, there are 2 channels in cellpose input parameters----chan and --chan2.<br>\nI used the cellpose command line for cell segmentation and it worked properly except for the --chan parameter.<br>\nWhenever I change this parameter, I am not sure what is changed from the result image.<br>\nWishing you\u2019re doing well, I am waiting for your help.</p>\n<p>Thanks.<br>\nJames<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7.jpeg\" data-download-href=\"/uploads/short-url/kujbWkGoDTC7ugyvwr7JHJF8ytF.jpeg?dl=1\" title=\"result\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7_2_690x346.jpeg\" alt=\"result\" data-base62-sha1=\"kujbWkGoDTC7ugyvwr7JHJF8ytF\" width=\"690\" height=\"346\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7_2_690x346.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7_2_1035x519.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7.jpeg 2x\" data-dominant-color=\"332D33\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">result</span><span class=\"informations\">1221\u00d7614 115 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78533": ["<p>Hello everyone,</p>\n<p>I am new at using CellProfiler and I\u2019m trying to quantify signal intensity for p16, a nuclear protein, in an artificial skin construct in 2D. I understand that this may be a simple request but I have tried following several approaches without good results. The following images are the DAPI channel (ch00), the p16 channel (ch01) and the image with all overlaid channels.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/6vY482qrshx8ZujGF1mLaA9ILu1.tif\">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1_ch00.tif</a> (1.3 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/hdLoMpGIe3zPJPRxduYSoXTiywi.tif\">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1_ch01.tif</a> (1.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/f6zNMjhxiXL6WEMSh2wmEzSrKV6.tif\">UG3SEN_02_17_K14_488_p16_594_53BP1_647_SN1_1.tif</a> (2.6 MB)</p>\n<p>My initial idea on how to go on about doing this was the following:</p>\n<ol>\n<li>Covert all channels to greyscale</li>\n<li>Identify Primary Objects, in the DAPI channel, i.e. nuclei</li>\n<li>Mask everything that is not identified as a nucleus in the DAPI channel image</li>\n<li>Apply this mask to the p16 channel image</li>\n<li>Define this as an object</li>\n<li>Measure the signal intensity of this object in the p16 channel</li>\n</ol>\n<p>I could not get this to work so I did the following:</p>\n<ol>\n<li>Covert all channels to greyscale</li>\n<li>Identify Primary Objects in the DAPI channel, i.e. nuclei</li>\n<li>Identify Primary Objects in the p16 channel as \u201cp16 signal\u201d</li>\n<li>Relate nuclei as the \u201cparent\u201d object to the p16 signal \u201cchildren\u201d object and define this as new object named \u201cp16 positive nuclei\u201d</li>\n<li>Measure signal intensity of p16 positive nuclei</li>\n</ol>\n<p>Looking at the images this has not worked as I wanted to, as the object \u201cp16 positive nuclei\u201d does not seem to be defined by the area occupied by the object \u201cnuclei\u201d.</p>\n<p>Essentially, what I want to do is define the area occupied by the nuclei in the DAPI channel and measure the intensity of the p16 signal from the p16 channel in that area, as p16 is a nuclear stain. Below you can find my pipeline.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/mYeZDoco9za6pAjSiy0uOHdftwP.cpproj\">UG3SEN_p16_53BP1.cpproj</a> (1.6 MB)</p>\n<p>I apologize for the long post. I would be grateful for any help.</p>\n<p>Thank you,<br>\nPeriklis</p>"], "72903": ["<p>I tried to clone the repository and was not able to download the sample data due to the following error:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/e/7ef6d9649bd524478dd62124f70ff189b7593d0d.png\" data-download-href=\"/uploads/short-url/i7b23JExKJLbacegIH7PplaXuMd.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/e/7ef6d9649bd524478dd62124f70ff189b7593d0d.png\" alt=\"image\" data-base62-sha1=\"i7b23JExKJLbacegIH7PplaXuMd\" width=\"690\" height=\"258\" data-dominant-color=\"212121\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1407\u00d7528 32.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>It says that \u201cThis repository is over its data quota\u201d. I do have git-lfs installed, what is the problem here?</p>\n<p>Thanks.</p>", "<p>As the error message says, the repository has gone over its allocated data quota on git-lfs (see <a href=\"https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-storage-and-bandwidth-usage\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">About storage and bandwidth usage - GitHub Docs</a>). It\u2019s not something on your side. Raise an issue on the repository so that the owner(s) can find a solution (i.e. pay for more data, move elsewhere or wait until next month for the quota to reset) or maybe you can arrange with the owner(s) to pay for the data yourself ($5/month for an additional 50GB).<br>\nA possible workaround is to fork the repository into your account then archive it (make sure to enable the inclusion of lfs objects in the archive) and download the resulting archive.</p>", "<p>Thank you! I got a response on the repository page.</p>", "<p><a class=\"mention\" href=\"/u/mollyb\">@mollyb</a><br>\nIs there any image file under the \u2018inputs/images\u2019 folder in the example data you downloaded?<br>\nI found that there is no image file under the example data \u2018inputs/images\u2019 folder downloaded on <a href=\"https://zenodo.org/record/7515132#.Y-cGonZBxPY\" rel=\"noopener nofollow ugc\">Zenodo</a></p>"], "72394": ["<p>Hello everyone,<br>\nFor my new job, I got an <strong>Ubuntu 22.04.1 LTS</strong> work laptop and I couldn\u2019t find much information on installing Cellprofiler on this system.<br>\nI\u2019ve tried a few variations of <a href=\"https://gist.github.com/GenevieveBuckley/d17a94eac2293d0540d5881bc358dd6a\" rel=\"noopener nofollow ugc\">this</a> and <a href=\"https://iqbalnaved.wordpress.com/2022/05/22/installing-cellprofiler-4-2-1-on-ubuntu-20-04/\" rel=\"noopener nofollow ugc\">this</a>, but neither worked.</p>\n<p>Before I start posting error messages, do you know if there is proper documentation for this installation that I might have missed?</p>\n<p>If not, did someone manage to install it and is willing to share some instructions?</p>\n<p>Thank you for the help!</p>", "<p>I\u2019m not aware of any installation instructions for Ubuntu 22 specifically - we would be happy to help you troubleshoot getting some working though!</p>", "<p>Hello <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a>!<br>\nThank you for the reply. I will try to describe what went wrong below!</p>\n<p>Following <a href=\"https://gist.github.com/GenevieveBuckley/d17a94eac2293d0540d5881bc358dd6a\" rel=\"noopener nofollow ugc\">this guide</a>:</p>\n<h2>\n<a name=\"at-point-3-1\" class=\"anchor\" href=\"#at-point-3-1\"></a>At point 3</h2>\n<p>after having added the link to the list of sources:</p>\n<p>I get the following issue:</p>\n<pre><code class=\"lang-auto\">Get:12 http://cz.archive.ubuntu.com/ubuntu bionic InRelease [242 kB]                \nErr:12 http://cz.archive.ubuntu.com/ubuntu bionic InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3B4FE6ACC0B21F32\nReading package lists... Done\nW: GPG error: http://cz.archive.ubuntu.com/ubuntu bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3B4FE6ACC0B21F32\nE: The repository 'http://cz.archive.ubuntu.com/ubuntu bionic InRelease' is not signed.\nN: Updating from such a repository can't be done securely, and is therefore disabled by default.\n</code></pre>\n<p>So when I try to install libwebkitgtk-1.0-0 of course I get:</p>\n<pre><code class=\"lang-auto\">E: Package 'libwebkitgtk-1.0-0' has no installation candidate\n</code></pre>\n<h2>\n<a name=\"in-point-7-2\" class=\"anchor\" href=\"#in-point-7-2\"></a>In point 7</h2>\n<p>There seems not to be a GitHub/CellProfiler directory. So I tried to install directly in the CellProfiler directory using:</p>\n<pre><code class=\"lang-auto\">pip install -e .\n</code></pre>\n<h2>\n<a name=\"try-to-import-3\" class=\"anchor\" href=\"#try-to-import-3\"></a>Try to import</h2>\n<p>After points 7, 8 and 9, if I try to import wx I get the following error:</p>\n<pre><code class=\"lang-auto\">&gt;&gt;&gt; import wx\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/wx/__init__.py\", line 17, in &lt;module&gt;\n    from wx.core import *\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/wx/core.py\", line 12, in &lt;module&gt;\n    from ._core import *\nImportError: /lib/x86_64-linux-gnu/libgio-2.0.so.0: undefined symbol: g_module_open_full\n</code></pre>\n<h2>\n<a name=\"calling-cellprofiler-4\" class=\"anchor\" href=\"#calling-cellprofiler-4\"></a>Calling cellprofiler</h2>\n<p>If I try to call cellprofiler from the terminal I get:</p>\n<pre><code class=\"lang-auto\">(cellprofiler-dev) dallavem@PP4-1004-B:~/Downloads$ cellprofiler\nTraceback (most recent call last):\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/bin/cellprofiler\", line 5, in &lt;module&gt;\n    from cellprofiler.__main__ import main\n  File \"/home/dallavem/Downloads/CellProfiler/cellprofiler/__main__.py\", line 22, in &lt;module&gt;\n    from cellprofiler_core.measurement import Measurements\n  File \"/home/dallavem/Downloads/core/cellprofiler_core/measurement/__init__.py\", line 1, in &lt;module&gt;\n    from ._measurements import Measurements\n  File \"/home/dallavem/Downloads/core/cellprofiler_core/measurement/_measurements.py\", line 15, in &lt;module&gt;\n    from ..constants.image import CT_OBJECTS\n  File \"/home/dallavem/Downloads/core/cellprofiler_core/constants/image.py\", line 1, in &lt;module&gt;\n    from bioformats import READABLE_FORMATS\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/bioformats/__init__.py\", line 21, in &lt;module&gt;\n    import javabridge\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/javabridge/__init__.py\", line 38, in &lt;module&gt;\n    from .jutil import start_vm, kill_vm, vm, activate_awt, deactivate_awt\n  File \"/home/dallavem/.conda/envs/cellprofiler-dev/lib/python3.8/site-packages/javabridge/jutil.py\", line 162, in &lt;module&gt;\n    import javabridge._javabridge as _javabridge\n  File \"_javabridge.pyx\", line 1, in init _javabridge\nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n</code></pre>\n<p>I hope this info can help pointing at the problem, thank you!</p>", "<p>General note - I might try instead of cloning CellProfiler from GitHub, just pip installing it, as stuff is subject to change as we move to CP5. But ultimately your call! Definitely do clone and install core from GitHub if installing CellProfiler from GitHub</p>\n<blockquote>\n<p>At point 3</p>\n</blockquote>\n<p>Have you tried <a href=\"https://chrisjean.com/fix-apt-get-update-the-following-signatures-couldnt-be-verified-because-the-public-key-is-not-available/\">this</a>? I\u2019m not certain webkit-gtk 1 is necessary but this should hopefully help you get it.</p>\n<blockquote>\n<p>At point 7</p>\n</blockquote>\n<p>Yup, looks like the person who wrote that guide had cloned inside a folder called GitHub</p>\n<blockquote>\n<p>Try to import</p>\n</blockquote>\n<p>See if the webgtk thing fixes the issue, if not, can you confirm whether or not you installed with the <a href=\"https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-22.04/\">latest wheels</a>, and if not can you try them? (CellProfiler might complain about 4.2.0 being too new a version of wx but it shouldn\u2019t prevent run in my experience)</p>\n<blockquote>\n<p>Calling cellprofiler<br>\nUsually that happens if the version of numpy is changed after the version of python-javabridge is installed; it\u2019s fussy like that. Using the code in the comment below (minus installing cellpose, unless you also wan that!) typically solves it.</p>\n</blockquote>\n<aside class=\"quote\" data-post=\"4\" data-topic=\"68994\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rebecca_senft/40/48617_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/cannot-use-cellpose-plugin-with-cellprofiler/68994/4\">Cannot use cellpose plugin with cellprofiler</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    It\u2019s been a while, but I figured I would update this to say that we do now have a .bat file for easing the install on Windows. Details can be found <a href=\"https://github.com/CellProfiler/CellProfiler-plugins/tree/master/Instructions/Windows_batch_file\">here</a>. \nAlso, have you tried something like this? \npip uninstall -y numpy python-javabridge centrosome\npip install numpy==1.21\npip install --no-cache-dir --no-deps --no-build-isolation centrosome python-javabridge\npip install cellprofiler cellpose\n  </blockquote>\n</aside>\n", "<p>Hello,</p>\n<p>Update your envirnoment by following <a href=\"https://github.com/CellProfiler/CellProfiler/wiki/Conda-Installation\" rel=\"noopener nofollow ugc\">this</a>. It\u2019s work fine to install cellprofiler 4.2.4 on ubuntu 22.04.</p>\n<p>Try this command to update your environment conda :<br>\n<code>conda env update --name myenv --file local.yml --prune</code></p>\n<p>Best,</p>", "<p>Hi,</p>\n<p>the problem is that CellProfiler requires Python3.8 but Ubuntu 22.04 comes with Python3.10. In my case it helped to install Python3.8 and make a local environment with it. I made some new instructions for it, integrating some of the instructions I found on the GitHUB pages. It works, hopefully it will work for others as well. You can find the instructions on our lab web page: <a href=\"http://www.pfistererlab.org/tools.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Pfisterer Laboratory</a></p>\n<p><a href=\"https://www.pfistererlab.org/cubbli22_cellprofiler_4_installation.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https://www.pfistererlab.org/cubbli22_cellprofiler_4_installation.html</a></p>"], "79055": ["<p>Hi guys! I am currently performing nuclei segmentation using DAPI channel of multiplex immunofluorescence tumor biopsy images. My goal is to perform the segmentation with various open source software to compare and contrast them.</p>\n<p>I have a working pipeline with semi-optimized parameters in QuPath but have one issue: For standardization across platforms, I require there to be a space between detected objects (nuclei). Is there a way to do this in QuPath ?</p>\n<p>For example in the screenshot below, it can be seen that the objects are overlapping<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b0b0c95024be35a1437cd6fea316e98469aaeaf.png\" data-download-href=\"/uploads/short-url/aHRtWyBvoqMeu1WKtbpZj4ySCkf.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/b/4b0b0c95024be35a1437cd6fea316e98469aaeaf.png\" alt=\"image\" data-base62-sha1=\"aHRtWyBvoqMeu1WKtbpZj4ySCkf\" width=\"498\" height=\"500\" data-dominant-color=\"545454\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">608\u00d7610 16.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>There is a module that does this object shrinking in CellProfiler and was wondering if there was an equivalent in QuPath, or if someone has any ideas if I can export this to another program to achieve my desired results?</p>\n<p>Thanks!</p>", "<p>If you export the masks from QuPath, you can choose to create a border region, and rather than the usual choice of making that border region a different, third value, you could choose to make it the same as the background value.</p>\n<p>Alternatively you could take the ROI for all of the cells and erode it one pixel, but that seems like way more processing power and time, and still requires exporting the masks.</p>"], "75473": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4c6f7e9886601c3b634f0cf39b55a646164f0bc1.jpeg\" data-download-href=\"/uploads/short-url/aUba94uMkPXm8d98XJo3huLTfKF.jpeg?dl=1\" title=\"siNT w3 ell2004\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c6f7e9886601c3b634f0cf39b55a646164f0bc1_2_656x500.jpeg\" alt=\"siNT w3 ell2004\" data-base62-sha1=\"aUba94uMkPXm8d98XJo3huLTfKF\" width=\"656\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c6f7e9886601c3b634f0cf39b55a646164f0bc1_2_656x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c6f7e9886601c3b634f0cf39b55a646164f0bc1_2_984x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c6f7e9886601c3b634f0cf39b55a646164f0bc1_2_1312x1000.jpeg 2x\" data-dominant-color=\"400916\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">siNT w3 ell2004</span><span class=\"informations\">1344\u00d71024 80.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/af416a3b075e6a19198c849c9505fe00b3438b6e.jpeg\" data-download-href=\"/uploads/short-url/p0nHUtDkiLbj9npJVeNhWlj1COO.jpeg?dl=1\" title=\"sigba w6 w3 ell2006\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af416a3b075e6a19198c849c9505fe00b3438b6e_2_656x500.jpeg\" alt=\"sigba w6 w3 ell2006\" data-base62-sha1=\"p0nHUtDkiLbj9npJVeNhWlj1COO\" width=\"656\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af416a3b075e6a19198c849c9505fe00b3438b6e_2_656x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af416a3b075e6a19198c849c9505fe00b3438b6e_2_984x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/af416a3b075e6a19198c849c9505fe00b3438b6e_2_1312x1000.jpeg 2x\" data-dominant-color=\"250308\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">sigba w6 w3 ell2006</span><span class=\"informations\">1344\u00d71024 60 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><strong>Biology and cell staining:</strong></p>\n<p>Above are two images from a Knockdown experiment using RNAi, so we knock down a gene and check for how it changes cells, here the primary phenotype we look at in the endoplasmic reticulum(ER), in the images they are stained red, while nuclei in blue.</p>\n<p>Condensed ER = diseased\u2019 here caused by inhibiting  a gene, GBA, seen in the image above labelled \u2018sigba\u2019</p>\n<p>Extended ER = \u2018normal\u2019 treated by a no gene inhibiting RNAi, in image above as \u2018siNT\u2019</p>\n<p><strong>Goal</strong><br>\nI would like to create an analysis pipeline that measures how many cells have diseased vs normal phenotype, by looking at ER morphology, and hope in the next step to do Deep learning.</p>\n<p>I am new to image analysis and I know there must be an already existing pipeline somewhere, Cellprofiler, or imageJ.</p>\n<p>However, I think I find it easier and more helpful to write my own code, I did already do some segmentation and masking and on diff images, but I am unable to find a way to make it find what makes it diseased or normal ER? what is the threshold, since it;s biology and RNA inhibitors won;t affect all cells, we expect that it won\u2019t black and white. But we sure should be able to cluster them into a grading system.</p>\n<p><strong>Long-term goal</strong></p>\n<p>I would like to take a loads of images from Control (siNT) vs diseased (sigba) and do cell painting (stain different organelles in the conditions) and let the algorithms give me all the features then do some differential clustering analysis or something to see what features fall together and what can we learn from it?</p>\n<p><strong>Please</strong></p>\n<p>If there are any suggestions on a breakdown of things I need to do or look at, it would be great.</p>\n<p>Abdullah <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>What is the difference you are looking for? The red staining looks fairly similar between the two images, aside from what may be either significant sample to sample variation or image adjustment.</p>\n<p>The upper picture may have a much more constrained brightness/contrast? At least the background appears to be much higher, and the nuclear signal is definitely saturated.</p>", "<p>Hey</p>\n<p>Am looking mainly to see if the ER which is in red seem spread = normal and potentially would have signal of red diluted</p>\n<p>But when ER is condensed, the red would be less and more localised</p>\n<p>Because ER generally tends to be spread out around the Nucleus (blue here, away from body of nucleus)</p>\n<p>So yaah if we identify the blue (the nucleus ) which would give us the number of cells, and then as secondary object would be ER, in Red. which maybe we can use area around nucleus in red and measure it to see if extended/spread or condensed</p>\n<p>This also can be a crit</p>", "<p>I agree with MicroscopyRA here that the staining looks pretty similar between your two samples, aside from differences in potentially overall imaging conditions.</p>\n<p>That being said, you essentially have a couple options here -</p>\n<ol>\n<li>If you think you can confidently look at a cell and define it as \u201ccondensed\u201d or \u201cextended\u201d, you could measure lots of properties of the cells in CellProfiler then use a tool like CellProfiler Analyst to create a machine learning classifier to distinguish the phenotypes and then say \u201cX% of cells are condensed in treatment A vs Y% in controls\u201d.</li>\n<li>You can pick a whole-cell metric (such as the amount of ER signal within X pixels of the nuclear boundary - in CellProfiler this would be found in the MeasureObjectIntensityDistribution module) and then compare it between your treated and your untreated.</li>\n<li>You could try to segment the ER as an object and then make measurements of it, but that seems the most challenging to me.</li>\n</ol>\n<p>You COULD of course write your own code for all of this, but it will likely take you a lot longer than using existing tools, so I would only recommend doing so if there\u2019s a particular other reason to.</p>", "<p>Hello</p>\n<p>Thank you so much for replying.</p>\n<p>definitely these might not be the best images as I am still optimising staining, however it\u2019s not the staining per se, it\u2019s more like the shape of ER around the nucleus.</p>\n<p>Your suggestion are brilliant, thank you.</p>\n<p>I believe I need to get comfortable with CellProfiler and use to progress my project but on the side, write my own code which am already doing.</p>\n<p>I really appreciate it.</p>\n<p>Thanks <img src=\"https://emoji.discourse-cdn.com/twitter/relaxed.png?v=12\" title=\":relaxed:\" class=\"emoji\" alt=\":relaxed:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "76502": ["<p>Hello,<br>\nI have some engineered muscle tissue histology images that I am analyzing. I am using watershed to identify the myotube structures. Most of the images were able to process. However, I have a few images that returned the following error:</p>\n<p>Error while processing Watershed:<br>\n(Worker) IndexError: index 0 is out of bounds for axis 0 with size 0</p>\n<p>Is this a known issue?</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/sXI46GQnKDLD5k3hJDcPIfeg3n4.cpproj\">Myotube quantification v3 troubleshooting.cpproj</a> (81.1 KB)</p>"], "76503": ["<p>Hello everyone,</p>\n<p>I am trying to segment this kind of brightfield WSI to train a model.</p>\n<p>I should have 3 classes. blue, red and brown cells. There is a nuclei marker in light blue.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/0/30cee1218789992f45de43ebd6e8bf2c1725d9eb.jpeg\" alt=\"Picture1\" data-base62-sha1=\"6XM9EtjYBIhbLdIGtHFhgs1SUDV\" width=\"212\" height=\"212\"></p>\n<p>I have tried to use HSV threshold with watershed, but the results are bad. Also tried stardist pretrained model, but not worked well either. So, I think I have to annotated some images well.</p>\n<p>My idea is the following:-</p>\n<ol>\n<li>open the wsi in qupath, and crop a representative 500x500.</li>\n<li>Move the image to napari to annotate and export mask.</li>\n<li>Use the little masks (10) to train a stardist first , then infer the rest of the images\u2019 masks .</li>\n</ol>\n<p>I feel there should be a better workflow, but don\u2019t where to start.</p>\n<p>Suggestions are highly appreciated!<br>\nThanks</p>", "<p>Might not be helpful at this point in the experiment, but I think the main issue is with the staining. You have blue on blue, and the nuclear stain is so weak that it doesn\u2019t show through the other stains. This would be far better as an IF experiment, or with better color separation, maybe yellow, green, red.</p>\n<p>Worse, in some cases the nuclei are the brightest parts of the image, while in others they are the darkest. That won\u2019t make it easy for single channel models like StarDist or even CellPose.</p>\n<p>You could probably segment by area with a pixel classifier though. Maybe others will have ideas.</p>", "<p>Have you tried building a pixel classifier in QuPath? This is a tricky looking image, but I would give that a try especially since the images are already amenable to qupath. You could also try custom stain vectors in QuPath to help separate those channels and perhaps then get a cleaner image of the nuclei that you could feed into something like StarDist.</p>", "<p>I personally have hand labeled tens of thousands of cells. I have tried all the approaches you have suggested. My experience is that hand labeling has to be done far more than anyone wants to. If you have a good network, you should not have to label more than 2,000 to start getting results good enough to then use predictions to get you to 90% for additional training images.</p>", "<p>Want to give this the right answer, but is still don\u2019t know where to quickly annotate and save in a format easily ported to python!</p>", "<p>If you use QuPath for CellPose, you still need to set up the environment to run CellPose in Python, but then you do the rest, annotation etc, in QuPath. So depends on how you approach it.</p>\n<p>You could also do all the training in Python and then just use the model in QuPath. Either way.</p><aside class=\"quote quote-modified\" data-post=\"1\" data-topic=\"58901\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/oburri/40/3464_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/cellpose-in-qupath-qupath-extension-cellpose/58901\">Cellpose in QuPath - QuPath Extension Cellpose</a> <a class=\"badge-wrapper  bullet\" href=\"/c/announcements/10\"><span class=\"badge-category-bg\" style=\"background-color: #AB9364;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for announcements, such as new software releases and upcoming community events.\">Announcements</span></a>\n  </div>\n  <blockquote>\n    <a name=\"cellpose-on-qupath-using-virtual-environments-1\" class=\"anchor\" href=\"#cellpose-on-qupath-using-virtual-environments-1\"></a>Cellpose on QuPath using virtual environments\nHi all, \nAs a follow up to <a href=\"https://forum.image.sc/t/using-cellpose-mask-in-qupath-cell-detection/51010\">a lot</a>-<a href=\"https://forum.image.sc/t/qupath-transferring-objects-from-one-image-to-the-other/48122/3\">of discussions</a>-<a href=\"https://forum.image.sc/t/using-python-command-line-from-groovy-qupath-cellpose/47684\">all over</a>: \n\n<a name=\"we-have-released-a-cellpose-wrapper-for-qupath-030-2\" class=\"anchor\" href=\"#we-have-released-a-cellpose-wrapper-for-qupath-030-2\"></a>We have released a Cellpose wrapper for QuPath 0.3.0\nFueled by a burning desire to have a (relatively) simple way to get Cellpose results into QuPath and with the help of new functionalities included into QuPath 0.3.0 including the new new modular form of extensions, I am happy to annouce the first working version of the Cellpose QuPath Extension \n\n\n<a name=\"what-this-extension-does-3\" class=\"anchor\" href=\"#what-this-extension-does-3\"></a>What this extension does\nBorrowing from <a class=\"mention\" href=\"/u/petebankhead\">@\u2026</a>\n  </blockquote>\n</aside>\n\n<p>CellProfiler may have similar? Not sure.</p>", "<p>Can I reiterate. So, you mean to use cellpose as initial model then refine the annotations.</p>\n<p>The second approch, did you mean to create python model then port it to qupath? Let\u2019s say I have a pytorch model, is there is an easy way to ship it to qupath?</p>", "<p>Um, as far as I know, CellPose only has one type of model, and that is what is read by the CellPose plugin.</p>\n<p>You can refine CellPose models either through QuPath retraining or through CellPose. But if you want to do the final analysis in QuPath, it may be easier to just do everything there.</p>\n<p>Still not confident CellPose will do a great job of cells with that kind of staining, but it\u2019s the best suggestion I\u2019ve got other than an pixel classifier to detect areas within the (maybe?) pancreatic islets.</p>", "<p>Maybe to clear something up. All the qupath cellpose extension does is call the cellpose code, which is in python. So there is no need to \u201cport\u201d. It\u2019s merely a wrapper for the python command that launches cellpose with some glue that makes reimporting label images into QuPath objects simple, especially with tiles.</p>"], "75483": ["<p>Hi everyone,<br>\nI am using the trackobjects pipeline from the Cellprofiler site to track nuclei over time.<br>\nhere is the pipeline I use (the same as in the site but with some parameters changed):<br>\n<a class=\"attachment\" href=\"/uploads/short-url/3i5mWcEc6nF4ojdCKM90sRsfXk1.cpproj\">ExampleTrackObjects.cpproj</a> (563.0 KB)<br>\nwhen I export the data, I see that in the resulting excel output, some of the nuclei are not saved.<br>\nfor example here I attached screen shot for the resulted file where the nuclei number that are identified are shown in the column \u2018object number\u2019. I sorted the values in the column from the largest to the smallest. the number 84 is not seen there (the max is 81), while in one of the pictures there is nucleus#84.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0aabe577d41066c6fe2aaa693a76708defbf38eb.png\" data-download-href=\"/uploads/short-url/1wp3MjFce44lDy7gqYYBRtUnco3.png?dl=1\" title=\"Capture_error\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aabe577d41066c6fe2aaa693a76708defbf38eb_2_690x421.png\" alt=\"Capture_error\" data-base62-sha1=\"1wp3MjFce44lDy7gqYYBRtUnco3\" width=\"690\" height=\"421\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aabe577d41066c6fe2aaa693a76708defbf38eb_2_690x421.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aabe577d41066c6fe2aaa693a76708defbf38eb_2_1035x631.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aabe577d41066c6fe2aaa693a76708defbf38eb_2_1380x842.png 2x\" data-dominant-color=\"616669\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Capture_error</span><span class=\"informations\">2377\u00d71451 329 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nI attached a folder with images for analysis as example:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/tXy86dOKDHOXobTtcIESf5Oxjbs.zip\">example images (2).zip</a> (8.0 MB)</p>\n<p>I would appreciate any help!<br>\nhappy new year:)</p>", "<p>Hi Nadeen,</p>\n<p>The numbers that are shown in that picture are not the ObjectNumbers - the column in your spreadsheet that they correspond to will be TrackObjects_Label_SomeNumber. Hope that helps!</p>", "<p>thank you very much Beth !!! <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>"], "78559": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f.png\" data-download-href=\"/uploads/short-url/4DgXc2GoljqBABpBGWtgWoia7G7.png?dl=1\" title=\"qUPATH\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_690x15.png\" alt=\"qUPATH\" data-base62-sha1=\"4DgXc2GoljqBABpBGWtgWoia7G7\" width=\"690\" height=\"15\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_690x15.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_1035x22.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/207933e7573c684c3cdc48c1b535b081a35c5e9f_2_1380x30.png 2x\" data-dominant-color=\"C9C9C9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">qUPATH</span><span class=\"informations\">1419\u00d732 20.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nHello everyone, I need your help in the sense that I do not fully understand the different Qupath parameters, what do they mean exactly<br>\nMax caliper/Min caliper<br>\nOD std dev<br>\nOD Range<br>\nAnd especially in term of interpretation (that is a good circularity of the core =1, but for the other parameters how can I know the reference values)<br>\nThank you for your HEEELP ! <span class=\"hashtag\">#URGENT</span></p>", "<p>I believe the calipers are the min and max lengths of the object, as measured by calipers, see <a href=\"https://en.wikipedia.org/wiki/Feret_diameter\" class=\"inline-onebox\">Feret diameter - Wikipedia</a><br>\nStd dev is Standard Deviation and Range is\u2026 range from min to max. OD ranges tend to be from 0 to ~1.5-3, but can change depending on the color vectors for specific stains.</p>"], "74464": ["<p>Hello,</p>\n<p>I am having issues with Cellprofiler when trying to analyze fluorescent antibody-stained tissue samples on 25x75mm slides. I am interested in the spatial relationship between adjacent cells, so I am currently analyzing entire stitched images (~25,000x45,000 px) at once, which contains about 1M unique cells after segmentation. I believe my computer is sufficient for this task (24C, 128GB RAM) and doesn\u2019t consume all of my memory at any point so I don\u2019t think this is the main cause of my issues. I\u2019m using Cellprofiler and Cellprofiler-Core from source (5.0.0b1). I am analyzing 16 images which correspond to 4 cycles of 4 channels (DAPI, FITC, TRITC, CY5) and my workflow is as follows:</p>\n<ol>\n<li>Load all 16 images into Cellprofiler</li>\n<li>Segment individual nuclei using IdentifyPrimaryObjects on one channel (stained with Hoechst)</li>\n<li>Create Cytoplasm mask by:<br>\na) Expanding nuclei by several pixels using ExpandOrShrinkObjects<br>\nb) Masking the expanded nuclei using the original nuclei, which creates a \u201chole\u201d in the center of the expanded nuclei exactly the size of the nucleus. This is functionally equivalent to IdentifySecondaryObjects \u2192 IdentifyTertiaryObjects to generate nuclear and cytoplasmic masks; however, these modules request enormous (3+TB) memory, whereas my method does not.</li>\n<li>Measure the intensity each channel in the cytoplasm and nuclear masks.</li>\n<li>Export cell position and nuclear/cytoplasmic intensities to a database (.db) file.</li>\n</ol>\n<p>My challenges are as follows:</p>\n<ol>\n<li>\n<p>I can only measure and export one channel at a time, rather than all 16 at a time. Cellprofiler reaches the ExportToDatabase step and remains stuck on this step for 96+ hours. However, when only measuring and exporting one channel at a time, the database exports in ~5 minutes. I\u2019ve tried exporting even two channels at a time and it seems to still get stuck on this step. My current workaround is to have an individual pipeline for each channel and manually export measurements for each channel, one at a time.</p>\n</li>\n<li>\n<p>The pipeline never finishes. Even after ExportToDatabase is successful and the database file is written, the pipeline never actually finishes and is still displays: \u201cProcessing: 0 of 1 Image Sets completed\u201d for hours even though Cellprofiler doesn\u2019t seem to be doing anything. This is preventing me from writing a simple script to call a headless version of Cellprofiler to automate this one-channel-at-a-time problem. To stop the pipeline I have to manually click \u201cStop Analysis\u201d. Otherwise it seems it will continue indefinitely.</p>\n</li>\n</ol>\n<p>I am wondering if anyone has any insight as to why these two issues could be occurring.</p>\n<p>Thank you,<br>\nMatt<br>\n<a class=\"attachment\" href=\"/uploads/short-url/hkMYft8OsIwjY79ETiDh2ZxQr85.cppipe\">Pipeline.cppipe</a> (18.9 KB)</p>"], "74465": ["<p>Hi All,<br>\nI am trying to quantify intensity of a marker in the cytoplasm of cells. However, I am interested only in the region directly surround the nucleus (not all the cytoplasm), say only 2um circle around the nucleus.</p>\n<p>I am aware of cellprofiler and can use a module to segment nuclei based on the nuclear staining. However I do not know, how I specify specific region in the cytoplasm to be quantified.</p>\n<p>I can also use macros in imageJ. I need some hints to specify specific regions in the cytoplasm to be quantified instead of quantifying the whole cytoplasm.</p>\n<p>Your help and support is very much appreciated.</p>\n<p>Thanks.</p>\n<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<ul>\n<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox) \u2013 (make sure however that you are allowed to share the image data publicly under the conditions of this forum).</li>\n<li>Share a <a href=\"https://en.wikipedia.org/wiki/Minimal_working_example\" rel=\"noopener nofollow ugc\">minimal working example</a> of your macro code.</li>\n</ul>\n\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<ul>\n<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific \u201cjargon\u201d.</li>\n</ul>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<ul>\n<li>What information are you interested in getting from this image?</li>\n</ul>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<ul>\n<li>What stops you from proceeding?</li>\n<li>What have you tried already?</li>\n<li>Have you found any related forum topics? If so, cross-link them.</li>\n<li>What software packages and/or plugins have you tried?</li>\n</ul>", "<p>Hello <a class=\"mention\" href=\"/u/kamal80\">@Kamal80</a> ,</p>\n<p>In CellProfiler, the easiest way to do this is to use IdentifySecondaryObjects to generate a ring of pixels around your nuclei using the Distance - N option for \u201cSelect the method to identify secondary objects\u201d. Using this object group and your original object group, you can then use IdentifyTertiaryObjects to extract the 2um ring around your nuclei.</p>\n<p>Hope this helps.</p>"], "72419": ["<p>I\u2019m trying to install CellProfiler plugins to run on my Mac (Big Sur, 11.4). I installed CellProfiler from the Downloads page (didn\u2019t build from source) and was using the \u201cInstructions for Mac M1\u201d markdown file (<a href=\"https://github.com/CellProfiler/CellProfiler-plugins/blob/19d60b03a9ca114c8d5d46586cf79a853605f9fc/Instructions/Installation_instructions_for_CellProfiler_M1.md\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">CellProfiler-plugins/Installation_instructions_for_CellProfiler_M1.md at 19d60b03a9ca114c8d5d46586cf79a853605f9fc \u00b7 CellProfiler/CellProfiler-plugins \u00b7 GitHub</a>). I\u2019m getting an error in building the conda env for the yml file</p>\n<pre><code class=\"lang-auto\">conda env create -n cp_plugins --file cellprofiler_plugins_macM1.yml\n</code></pre>\n<p>The error I get is below:</p>\n<pre><code class=\"lang-auto\">Collecting scipy\n  Using cached scipy-1.9.1.tar.gz (42.0 MB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'error'\n\nPip subprocess error:\n  error: subprocess-exited-with-error\n  \n  \u00d7 Getting requirements to build wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500&gt; [61 lines of output]\n      The Meson build system\n      Version: 0.62.2\n      Source dir: /private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e\n      Build dir: /private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e/.mesonpy-0sm45ojg/build\n      Build type: native build\n      Project name: SciPy\n      Project version: 1.9.1\n      C compiler for the host machine: cc (clang 12.0.5 \"Apple clang version 12.0.5 (clang-1205.0.22.11)\")\n      C linker for the host machine: cc ld64 650.9\n      C++ compiler for the host machine: c++ (clang 12.0.5 \"Apple clang version 12.0.5 (clang-1205.0.22.11)\")\n      C++ linker for the host machine: c++ ld64 650.9\n      Host machine cpu family: aarch64\n      Host machine cpu: arm64\n      Compiler for C supports arguments -Wno-unused-but-set-variable: NO\n      Library m found: YES\n      Fortran compiler for the host machine: gfortran (gcc 12.2.0 \"GNU Fortran (Homebrew GCC 12.2.0) 12.2.0\")\n      Fortran linker for the host machine: gfortran ld64 650.9\n      Program cython found: YES (/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/bin/cython)\n      Program pythran found: YES (/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/bin/pythran)\n      Program cp found: YES (/bin/cp)\n      Program python found: YES (/Users/mak6968/miniconda3/envs/cp_plugins/bin/python)\n      Found pkg-config: /opt/homebrew/bin/pkg-config (0.29.2)\n      Library npymath found: YES\n      Library npyrandom found: YES\n      Did not find CMake 'cmake'\n      Found CMake: NO\n      Run-time dependency openblas found: NO (tried pkgconfig, framework and cmake)\n      Run-time dependency openblas found: NO (tried pkgconfig and framework)\n      \n      ../../scipy/meson.build:130:0: ERROR: Dependency \"OpenBLAS\" not found, tried pkgconfig and framework\n      \n      A full log can be found at /private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e/.mesonpy-0sm45ojg/build/meson-logs/meson-log.txt\n      + meson setup --native-file=/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e/.mesonpy-native-file.ini -Ddebug=false -Doptimization=2 --prefix=/Users/mak6968/miniconda3/envs/cp_plugins /private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e /private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e/.mesonpy-0sm45ojg/build\n      Traceback (most recent call last):\n        File \"/Users/mak6968/miniconda3/envs/cp_plugins/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 363, in &lt;module&gt;\n          main()\n        File \"/Users/mak6968/miniconda3/envs/cp_plugins/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 345, in main\n          json_out['return_val'] = hook(**hook_input['kwargs'])\n        File \"/Users/mak6968/miniconda3/envs/cp_plugins/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py\", line 130, in get_requires_for_build_wheel\n          return hook(config_settings)\n        File \"/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/lib/python3.8/site-packages/mesonpy/__init__.py\", line 964, in get_requires_for_build_wheel\n          with _project(config_settings) as project:\n        File \"/Users/mak6968/miniconda3/envs/cp_plugins/lib/python3.8/contextlib.py\", line 113, in __enter__\n          return next(self.gen)\n        File \"/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/lib/python3.8/site-packages/mesonpy/__init__.py\", line 943, in _project\n          with Project.with_temp_working_dir(\n        File \"/Users/mak6968/miniconda3/envs/cp_plugins/lib/python3.8/contextlib.py\", line 113, in __enter__\n          return next(self.gen)\n        File \"/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/lib/python3.8/site-packages/mesonpy/__init__.py\", line 772, in with_temp_working_dir\n          yield cls(source_dir, tmpdir, build_dir)\n        File \"/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/lib/python3.8/site-packages/mesonpy/__init__.py\", line 677, in __init__\n          self._configure(reconfigure=bool(build_dir) and not native_file_mismatch)\n        File \"/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/lib/python3.8/site-packages/mesonpy/__init__.py\", line 708, in _configure\n          self._meson(\n        File \"/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/lib/python3.8/site-packages/mesonpy/__init__.py\", line 691, in _meson\n          return self._proc('meson', *args)\n        File \"/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-build-env-ltf28bj1/overlay/lib/python3.8/site-packages/mesonpy/__init__.py\", line 686, in _proc\n          subprocess.check_call(list(args))\n        File \"/Users/mak6968/miniconda3/envs/cp_plugins/lib/python3.8/subprocess.py\", line 364, in check_call\n          raise CalledProcessError(retcode, cmd)\n      subprocess.CalledProcessError: Command '['meson', 'setup', '--native-file=/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e/.mesonpy-native-file.ini', '-Ddebug=false', '-Doptimization=2', '--prefix=/Users/mak6968/miniconda3/envs/cp_plugins', '/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e', '/private/var/folders/_r/r76720bs5_b7r054l3kn5kf40000gn/T/pip-install-iguuszcv/scipy_ccb74f99c454421481629ba6b8af329e/.mesonpy-0sm45ojg/build']' returned non-zero exit status 1.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500&gt; See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n\nfailed\n\nCondaEnvException: Pip failed\n\n</code></pre>\n<p>My machine has scipy installed:</p>\n<pre><code class=\"lang-auto\">python -c \"import scipy; print(scipy.__version__)\"\n</code></pre>\n<p>gives the output</p>\n<pre><code class=\"lang-auto\">1.10.0.dev0+1964.ac815d4\n</code></pre>\n<p>Thank you!</p>", "<p>I\u2019m not sure what exactly has happened, but something has gone horribly wrong. <code>scipy</code> has wheels for M1 both on pypi and conda-forge so you should never be building it from source.<br>\nCan you try <code>conda info</code> maybe we can get a hint as to what happened?</p>", "<p>The output to <code>conda info</code> is :</p>\n<pre><code class=\"lang-auto\">     active environment : base\n    active env location : /Users/mak6968/miniconda3\n            shell level : 1\n       user config file : /Users/mak6968/.condarc\n populated config files : \n          conda version : 22.9.0\n    conda-build version : not installed\n         python version : 3.9.12.final.0\n       virtual packages : __osx=11.4=0\n                          __unix=0=0\n                          __archspec=1=arm64\n       base environment : /Users/mak6968/miniconda3  (writable)\n      conda av data dir : /Users/mak6968/miniconda3/etc/conda\n  conda av metadata url : None\n           channel URLs : https://repo.anaconda.com/pkgs/main/osx-arm64\n                          https://repo.anaconda.com/pkgs/main/noarch\n                          https://repo.anaconda.com/pkgs/r/osx-arm64\n                          https://repo.anaconda.com/pkgs/r/noarch\n          package cache : /Users/mak6968/miniconda3/pkgs\n                          /Users/mak6968/.conda/pkgs\n       envs directories : /Users/mak6968/miniconda3/envs\n                          /Users/mak6968/.conda/envs\n               platform : osx-arm64\n             user-agent : conda/22.9.0 requests/2.28.1 CPython/3.9.12 Darwin/20.5.0 OSX/11.4\n                UID:GID : 501:20\n             netrc file : None\n           offline mode : False\n</code></pre>", "<p>Ok, looks like you\u2019re using anaconda, so that could be the problem.<br>\nYou could try:</p>\n<pre><code class=\"lang-auto\">conda config --add channels conda-forge \nconda config --set channel_priority strict \n</code></pre>\n<p>This way you should use conda-forge which has robust support for arm64.</p>", "<p>The error in trying to build the conda env does not change</p>", "<p>OK I think I see the issue.<br>\nYou\u2019re still on macOS 11. scipy doesn\u2019t support that version macOS:</p><aside class=\"onebox githubissue\" data-onebox-src=\"https://github.com/scipy/scipy/issues/15861\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/scipy/scipy/issues/15861\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com/scipy/scipy</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"></path></svg>\n  </div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https://github.com/scipy/scipy/issues/15861\" target=\"_blank\" rel=\"noopener nofollow ugc\">pip install failing on M1 mac</a>\n    </h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-03-24\" data-time=\"16:25:35\" data-timezone=\"UTC\">04:25PM - 24 Mar 22 UTC</span>\n      </div>\n\n        <div class=\"date\">\n          closed <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-03-28\" data-time=\"06:09:56\" data-timezone=\"UTC\">06:09AM - 28 Mar 22 UTC</span>\n        </div>\n\n      <div class=\"user\">\n        <a href=\"https://github.com/AlexTholen314\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"AlexTholen314\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/7/87b4aa9eba4281bf4882d77a402d310ce89ce4b9.png\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          AlexTholen314\n        </a>\n      </div>\n    </div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          Build issues\n        </span>\n    </div>\n  </div>\n</div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\">I'm trying to use pip to install scipy (python 3.10.2 and two different commands<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026</a></span><span class=\"excerpt hidden\"> as visible in the terminal output) and I get a litany of errors instead of it actually installing. I don't quite know what more information would be desired - I'm on Big Sur on a 2020 M1 if that helps.\n\n[Terminal Saved Output.txt](https://github.com/scipy/scipy/files/8343493/Terminal.Saved.Output.txt)</span></p>\n  </div>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Having updated to Monterey, I\u2019m still facing the same issue, not sure how to proceed?</p>", "<p>It seems that you have SciPy installed in your system python, which is why the command <code>python -c \"import scipy; print(scipy.__version__)\"</code> worked (assuming you were not already inside a conda environment at this stage).</p>\n<p>The crux of the error you encountered seems to be due to <strong>cmake</strong> being missing. You could try and install this with <code>conda install cmake</code> before installing other dependencies.</p>\n<p>Since you\u2019re trying to install CellProfiler from source, I\u2019d recommend following this install guide for M1 first: <a href=\"https://github.com/CellProfiler/CellProfiler/wiki/Installation-of-CellProfiler-4-from-source-on-MacOS-M1#conda-installation\" class=\"inline-onebox\">Installation of CellProfiler 4 from source on MacOS M1 \u00b7 CellProfiler/CellProfiler Wiki \u00b7 GitHub</a> (it\u2019s simpler than the plugin specific one).</p>\n<p>Once you have installed CellProfiler from source, you can then go forth and install dependencies for the plugin you\u2019re interested in.</p>", "<p>Thank you for your comment! I\u2019m trying to build it from source, but am getting an error with the javabridge install.</p>\n<pre><code class=\"lang-auto\">      _javabridge.c:612:10: fatal error: 'jni.h' file not found\n      #include \"jni.h\"\n               ^~~~~~~\n      1 warning and 1 error generated.\n      error: command '/usr/bin/clang' failed with exit code 1\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: legacy-install-failure\n</code></pre>\n<p>Please let me know how I can fix this?</p>", "<p>Typically this can happen when javabridge cannot find <code>JAVA_HOME</code>. If you take a look at step one of the conda installation linked above and make sure you\u2019ve executed all of the commands in step 1.</p>\n<p>You can check if <code>JAVA_HOME</code> has been set correctly with the following command <code>echo $JAVA_HOME</code>.</p>\n<p>Though, it may be worth first uninstalling java with <code>brew uninstall java</code> and then reinstall it with <code>brew install java</code>. When you install it for the second time, closely look at the terminal output from brew after installation. This output provides a few commands that you can copy and paste into your terminal that are not included in the above install guide since they may be system dependent.</p>", "<p>Thank you for the comment! I tried that, and ran the additional commands as well</p>\n<pre><code class=\"lang-auto\">For the system Java wrappers to find this JDK, symlink it with\n  sudo ln -sfn /opt/homebrew/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk\n\nopenjdk is keg-only, which means it was not symlinked into /opt/homebrew,\nbecause macOS provides similar software and installing this software in\nparallel can cause all kinds of trouble.\n\nIf you need to have openjdk first in your PATH, run:\n  echo 'export PATH=\"/opt/homebrew/opt/openjdk/bin:$PATH\"' &gt;&gt; ~/.zshrc\n\nFor compilers to find openjdk you may need to set:\n  export CPPFLAGS=\"-I/opt/homebrew/opt/openjdk/include\"\n</code></pre>\n<p>Which are doing pretty much the same thing as the start of the conda install. I had the same error. The JAVA_HOME path has been set to the right folder (/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home) which (I think) is symliked to  the OpenJDK folder? Not sure</p>\n<p>Thank you again!</p>", "<p>I managed to get past the JAVA_HOME problem and carried out the plugins install. However, when I try to run cellprofiler from the conda environment, it can\u2019t find the following modules:</p>\n<ul>\n<li>Pandas</li>\n<li>Stardist, numba</li>\n<li>runimagejscript, jpype</li>\n<li>cellpose</li>\n</ul>\n<p>Please let me know how I can install these into the conda env? In the plugins install guide, it specifies that I should have a yml file called cpcellpose2.yml, but I can\u2019t find this in the repository. I\u2019m not sure how to proceed.</p>\n<p>Thank you!</p>", "<p>So when running CellProfiler with plugins it\u2019s expected to see things like can\u2019t find Stardist if you don\u2019t plan on running it. This is a result of CellProfiler opening the StarDist plugin module when loading but subsequently finding that stardist is not installed. However, not finding CellPose is definitely an issue and suggests that you do not have it installed in your conda environment. You could try to rectify this with something like <code>pip install cellpose==1.0.2</code>.</p>\n<p>Good spot about the missing <code>cpcellpose2.yml</code> file - this file was renamed to <code>cellprofiler_plugins_macM1.yml</code> in the plugins repo but the instructions had not been updated. Thank you for noticing this - I\u2019ve updated the instructions accordingly. You could try following the updated instructions in the repo again and see how you get on. However with lots of packages being updated all of the time, these instructions may need adjusting here and there.</p>", "<p>Thank you! I installed each of the things that were causing me issues in the conda env individually, and it seems to have fixed that issue. However, I\u2019m still having the following error:</p>\n<p><code>could not load these modules: cellprofiler_core.readers.imageio_reader,cellprofiler_core.readers.bioformats_reader,cellprofiler_core.readers.gcs_reader</code></p>\n<p>I tried <code>pip install ./core</code> but that didn\u2019t seem to fix it either.</p>\n<p>Thanks so much for all your help!</p>", "<p>Would it be possible for you to try installation with the updated instructions found here: <a href=\"https://github.com/CellProfiler/CellProfiler-plugins/blob/ec2ce8fa96783218e1711630baed822da4350ab0/Instructions/Installation_instructions_M1_CellProfiler_CellPose_Stardist.md\" class=\"inline-onebox\">CellProfiler-plugins/Installation_instructions_M1_CellProfiler_CellPose_Stardist.md at ec2ce8fa96783218e1711630baed822da4350ab0 \u00b7 CellProfiler/CellProfiler-plugins \u00b7 GitHub</a></p>\n<p>Keep in mind this will require starting from a completely fresh conda environment. We hope that the <code>.yml</code> file here will install everything that you need and you should only need to start from step 12 for your case since you have already installed JAVA etc.</p>", "<p>I tried that, it still gives me the same error.</p>\n<pre><code class=\"lang-auto\">Could not load cellprofiler_core.readers.imageio_reader: No module named 'cellprofiler_core.readers'\n\nCould not load cellprofiler_core.readers.bioformats_reader: No module named 'cellprofiler_core.readers'\n\nCould not load cellprofiler_core.readers.gcs_reader: No module named 'cellprofiler_core.readers'\n\ncould not load these modules: cellprofiler_core.readers.imageio_reader,cellprofiler_core.readers.bioformats_reader,cellprofiler_core.readers.gcs_reader\n\nNo image readers are available, CellProfiler won't be able to load data!\n</code></pre>\n<p>How do I get past this error? I can open CellProfiler but can\u2019t actually load any images</p>"], "79076": ["<p>I am running CellProfiler on a recently updated Windows 11 computer. The program no longer opens and flashes a fast error before closing the console. It was working fine before a recent batch of Windows updates.</p>\n<p>\u201c[process exited with code 3221226356 (0xc0000374)]\u201d</p>\n<p>This has happened on a couple of computers after a recent windows update. I have tried installing and reinstalling CellProfiler 4.2.1 and 4.2.5. I have also insallted the Visual C++ Redistributable linked on the downloads page. Any help will be appreciated!</p>"], "18660": ["<p>Are you a Python programmer? Have you coded up new functionality for CellProfiler, but have been wondering how to share your efforts with the world? Have you wanted to contribute to CellProfiler source code but aren\u2019t sure how?</p>\n<p>All your answers lie here: <a href=\"https://github.com/CellProfiler/CellProfiler/wiki#wiki-What_if_I_want_to_contribute_to_CellProfiler\">github.com/CellProfiler/CellPro \u2026 llProfiler</a></p>\n<p>Regards,<br>\n-The CellProfiler Team</p>"], "79591": ["<p>Hi,</p>\n<p>Can someone tell me how do i save histogram data generated from Cellprofiler analyst onto a csv file? I can only save images for now.</p>"], "78567": ["<p>Hello again, Please i need your help<br>\nHow can I choose what I want to measure on qupath as a parameter with one line of code (script)</p>", "<p>Generally you can\u2019t. You first need to select the objects you want to measure, then perform the measurement. If you select the objects through the GUI (annotations, detections, some class) and then create the measurement through the GUI, the lines of script should show up in the workflow. <a href=\"https://qupath.readthedocs.io/en/stable/docs/scripting/workflows_to_scripts.html\" class=\"inline-onebox\">Workflows to scripts \u2014 QuPath 0.4.3 documentation</a></p>"], "77037": ["<p>I am trying to run <code>CellProfiler 4.1.3</code> on macOS Ventura (13.2 (22D49)).</p>\n<p>I installed <code>cellprofiler</code> in a miniconda3 instance (see <a href=\"https://github.com/swvanderlaan/slideToolKit/wiki/macOS:-Conda-version\" rel=\"noopener nofollow ugc\">here</a>). Here is the <a href=\"https://github.com/swvanderlaan/slideToolKit/blob/48f54964af29dc7dc1d8156ffd47e6a5d4cff486/conda_yml/conda3_8_cp413.v1.yml\" rel=\"noopener nofollow ugc\">yml</a>-file I used to install <code>cellprofiler</code>.</p>\n<p>However, I got this error:</p>\n<pre><code class=\"lang-auto\">cellprofiler -c -r -p $(pwd)/pipeLines/HE.cp413.v1.0.cppipe --file-list upgrade_study/HE/IMG1.HE/files2cp.txt -o upgrade_study/HE/IMG1.HE/cp_output/\nException in thread \"Thread-1\" java.lang.IllegalArgumentException: URI is not hierarchical\n\tat java.io.File.&lt;init&gt;(File.java:418)\n\tat org.cellprofiler.imageset.ImageFile.getFileName(ImageFile.java:72)\n\tat org.cellprofiler.imageset.filter.FileNamePredicate.getValue(FileNamePredicate.java:27)\n\tat org.cellprofiler.imageset.filter.AbstractURLPredicate.eval(AbstractURLPredicate.java:15)\n\tat org.cellprofiler.imageset.filter.AbstractURLPredicate.eval(AbstractURLPredicate.java:10)\n\tat org.cellprofiler.imageset.filter.LogicPredicate.eval(LogicPredicate.java:48)\n\tat org.cellprofiler.imageset.filter.Filter.eval(Filter.java:224)\n\tat org.cellprofiler.imageset.filter.Filter.filterURLs(Filter.java:138)\nFailed to prepare run for module Images\nTraceback (most recent call last):\n  File \"/Users/username/miniconda3/envs/cp4/lib/python3.8/site-packages/cellprofiler_core/pipeline/_pipeline.py\", line 1368, in prepare_run\n    not module.prepare_run(workspace)\n  File \"/Users/username/miniconda3/envs/cp4/lib/python3.8/site-packages/cellprofiler_core/modules/images.py\", line 354, in prepare_run\n    passes_filter = javabridge.call(\n  File \"/Users/username/miniconda3/envs/cp4/lib/python3.8/site-packages/javabridge/jutil.py\", line 892, in call\n    result = fn(*nice_args)\n  File \"/Users/username/miniconda3/envs/cp4/lib/python3.8/site-packages/javabridge/jutil.py\", line 859, in fn\n    raise JavaException(x)\njavabridge.jutil.JavaException: URI is not hierarchical\n</code></pre>\n<p>Any clues as to how to fix this?</p>\n<p>By the way: the same installation-workflow worked before on macOS and still works on <code>CentOS7</code> with <code>anaconda3</code>.</p>", "<p>Hi <a class=\"mention\" href=\"/u/swvanderlaan\">@swvanderlaan</a></p>\n<p>Sorry to hear that.</p>\n<p>There is a similar discussion <a href=\"https://forum.image.sc/t/cellprofiler-gave-error-when-start-test-mode/56102/29\">here</a>,  can you open your pipeline in the CP source installation and resave it? After you can try to run headless again and see if the error disappears?</p>\n<p>Let me know if this resolves your issue, if not can you pip freeze your environment?</p>\n<p>Best,<br>\nMario</p>", "<p>One other potential source of that issue with file lists is if your file lists are incorrect or a file is missing (or represent relative paths rather than absolute paths ie <code>file.txt</code> instead of <code>~/Users/someuser/Desktop/file.txt</code>) - can you confirm your file list is set up correctly? Do you get the same issue just running in GUI and/or running headless using something like -i rather than filelist?</p>", "<p>Actually, I have been further working on this.</p>\n<ul>\n<li>I re-installed openjdk through <code>brew install openjdk</code>\n</li>\n<li>made sure I have this in my <code>.bashrc</code>\n</li>\n</ul>\n<pre><code class=\"lang-auto\">export JAVA_HOME=\"/usr/local/Cellar/openjdk/19.0.2/libexec/openjdk.jdk/Contents/Home\"\nexport PATH=\"$JAVA_HOME/bin:$PATH\"\n</code></pre>\n<ul>\n<li>did <code>conda activate cp4</code> (my environment) and did <code>pip install javabridge</code>. This leads to:</li>\n</ul>\n<pre><code class=\"lang-auto\">pip install javabridge\nCollecting javabridge\n  Using cached javabridge-1.0.19.tar.gz (1.3 MB)\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: numpy in /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages (from javabridge) (1.21.0)\nBuilding wheels for collected packages: javabridge\n  Building wheel for javabridge (setup.py) ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 python setup.py bdist_wheel did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500&gt; [181 lines of output]\n      /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n        warnings.warn(\n      running bdist_wheel\n      running build\n      running build_py\n      creating build\n      creating build/lib.macosx-10.9-x86_64-cpython-38\n      creating build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/_version.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/jutil.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/locate.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/__init__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/wrappers.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/noseplugin.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      creating build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/__init__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/test_cpython.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/test_javabridge.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/test_jutil.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/test_wrappers.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      creating build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      copying javabridge/jars/rhino-1.7R4.jar -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      copying javabridge/jars/runnablequeue.jar -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      copying javabridge/jars/cpython.jar -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      copying javabridge/jars/test.jar -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      running build_ext\n      javac /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/runnablequeue/RunnableQueue.java\n      javac /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/javabridge/test/RealRect.java\n      javac /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/javabridge/CPython.java /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/javabridge/CPythonInvocationHandler.java\n      Note: /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/javabridge/CPythonInvocationHandler.java uses unchecked or unsafe operations.\n      Note: Recompile with -Xlint:unchecked for details.\n      building 'javabridge._javabridge' extension\n      creating build/temp.macosx-10.9-x86_64-cpython-38\n      gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/slaan3/miniconda3/envs/cp4/include -arch x86_64 -I/Users/slaan3/miniconda3/envs/cp4/include -arch x86_64 -I/usr/local/opt/llvm/include -I/Users/slaan3/miniconda3/envs/cp4/include -I/Users/slaan3/miniconda3/envs/cp4/include/darwin -I/Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include -I/Users/slaan3/miniconda3/envs/cp4/include/python3.8 -c _javabridge.c -o build/temp.macosx-10.9-x86_64-cpython-38/_javabridge.o\n      In file included from _javabridge.c:598:\n      In file included from /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n      In file included from /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h:12:\n      In file included from /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969:\n      /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: \"Using deprecated NumPy API, disable it with \"          \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n      #warning \"Using deprecated NumPy API, disable it with \" \\\n       ^\n      In file included from _javabridge.c:601:\n      ./mac_javabridge_utils.h:47:25: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacStopVM();\n                              ^\n                               void\n      ./mac_javabridge_utils.h:55:30: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacRunLoopInit();\n                                   ^\n                                    void\n      ./mac_javabridge_utils.h:63:29: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacRunLoopRun();\n                                  ^\n                                   void\n      ./mac_javabridge_utils.h:71:31: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacRunLoopReset();\n                                    ^\n                                     void\n      ./mac_javabridge_utils.h:79:30: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacStopRunLoop();\n                                   ^\n                                    void\n      ./mac_javabridge_utils.h:88:30: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT int MacIsMainThread();\n                                   ^\n                                    void\n      ./mac_javabridge_utils.h:103:20: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      void MacRunLoopStop();\n                         ^\n                          void\n      _javabridge.c:5159:37: warning: cast to smaller integer type 'int' from 'jobject' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = __Pyx_PyInt_From_int(((int)__pyx_v_self-&gt;o)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 475, __pyx_L1_error)\n                                          ^~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:5444:37: warning: cast to smaller integer type 'int' from 'jobject' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = __Pyx_PyInt_From_int(((int)__pyx_v_self-&gt;o)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 493, __pyx_L1_error)\n                                          ^~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:5668:37: warning: cast to smaller integer type 'int' from 'jclass' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = __Pyx_PyInt_From_int(((int)__pyx_v_self-&gt;c)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 502, __pyx_L1_error)\n                                          ^~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:6022:41: warning: cast to smaller integer type 'int' from 'jmethodID' (aka 'struct _jmethodID *') [-Wpointer-to-int-cast]\n        __pyx_t_4 = __Pyx_PyUnicode_From_int(((int)__pyx_v_self-&gt;id), 0, ' ', 'x'); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 521, __pyx_L1_error)\n                                              ^~~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:6301:41: warning: cast to smaller integer type 'int' from 'jfieldID' (aka 'struct _jfieldID *') [-Wpointer-to-int-cast]\n        __pyx_t_4 = __Pyx_PyUnicode_From_int(((int)__pyx_v_self-&gt;id), 0, ' ', 'x'); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 535, __pyx_L1_error)\n                                              ^~~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:8122:36: error: implicit declaration of function 'CreateJavaVM' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n        __pyx_t_3 = __Pyx_PyInt_From_int(CreateJavaVM((&amp;__pyx_v_self-&gt;vm), ((void **)(&amp;__pyx_v_env)), (&amp;__pyx_v_args))); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 651, __pyx_L1_error)\n                                         ^\n      _javabridge.c:8122:36: note: did you mean 'JNI_CreateJavaVM'?\n      /Users/slaan3/miniconda3/envs/cp4/include/jni.h:1949:1: note: 'JNI_CreateJavaVM' declared here\n      JNI_CreateJavaVM(JavaVM **pvm, void **penv, void *args);\n      ^\n      _javabridge.c:9199:5: error: implicit declaration of function 'StopVM' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n          StopVM(__pyx_v_self-&gt;vm);\n          ^\n      _javabridge.c:20700:18: warning: cast to smaller integer type 'int' from 'jobject' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = ((((int)__pyx_v_s-&gt;o) == 0) != 0);\n                       ^~~~~~~~~~~~~~~~~\n      _javabridge.c:20828:18: warning: cast to smaller integer type 'int' from 'jobject' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = ((((int)__pyx_v_s-&gt;o) == 0) != 0);\n                       ^~~~~~~~~~~~~~~~~\n      _javabridge.c:27819:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:27919:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:28050:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:28181:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:28284:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:28464:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:29747:3: warning: incompatible pointer to integer conversion returning 'void *' from a function with result type 'int' [-Wint-conversion]\n        import_array();\n        ^~~~~~~~~~~~~~\n      /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/__multiarray_api.h:1532:151: note: expanded from macro 'import_array'\n      #define import_array() {if (_import_array() &lt; 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, \"numpy.core.multiarray failed to import\"); return NULL; } }\n                                                                                                                                                            ^~~~\n      /Library/Developer/CommandLineTools/usr/lib/clang/14.0.0/include/stddef.h:89:16: note: expanded from macro 'NULL'\n      #  define NULL ((void*)0)\n                     ^~~~~~~~~~\n      _javabridge.c:32040:21: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n                          CYTHON_FALLTHROUGH;\n                          ^\n      _javabridge.c:274:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n            #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n                                       ^\n      _javabridge.c:32051:21: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n                          CYTHON_FALLTHROUGH;\n                          ^\n      _javabridge.c:274:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n            #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n                                       ^\n      _javabridge.c:33284:26: warning: code will never be executed [-Wunreachable-code]\n                      module = PyImport_ImportModuleLevelObject(\n                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      25 warnings and 2 errors generated.\n      error: command '/usr/bin/gcc' failed with exit code 1\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for javabridge\n  Running setup.py clean for javabridge\nFailed to build javabridge\nInstalling collected packages: javabridge\n  Running setup.py install for javabridge ... error\n  error: subprocess-exited-with-error\n\n  \u00d7 Running setup.py install for javabridge did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500&gt; [183 lines of output]\n      /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n        warnings.warn(\n      running install\n      /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n        warnings.warn(\n      running build\n      running build_py\n      creating build\n      creating build/lib.macosx-10.9-x86_64-cpython-38\n      creating build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/_version.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/jutil.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/locate.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/__init__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/wrappers.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      copying javabridge/noseplugin.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge\n      creating build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/__init__.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/test_cpython.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/test_javabridge.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/test_jutil.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      copying javabridge/tests/test_wrappers.py -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/tests\n      creating build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      copying javabridge/jars/rhino-1.7R4.jar -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      copying javabridge/jars/runnablequeue.jar -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      copying javabridge/jars/cpython.jar -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      copying javabridge/jars/test.jar -&gt; build/lib.macosx-10.9-x86_64-cpython-38/javabridge/jars\n      running build_ext\n      javac /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/runnablequeue/RunnableQueue.java\n      javac /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/javabridge/test/RealRect.java\n      javac /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/javabridge/CPython.java /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/javabridge/CPythonInvocationHandler.java\n      Note: /private/var/folders/mm/b4fx9qss7t79pb95gn3k6grh0000gq/T/pip-install-kwx8i8k9/javabridge_ea4459b4771447e591749f334256c8db/java/org/cellprofiler/javabridge/CPythonInvocationHandler.java uses unchecked or unsafe operations.\n      Note: Recompile with -Xlint:unchecked for details.\n      building 'javabridge._javabridge' extension\n      creating build/temp.macosx-10.9-x86_64-cpython-38\n      gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/slaan3/miniconda3/envs/cp4/include -arch x86_64 -I/Users/slaan3/miniconda3/envs/cp4/include -arch x86_64 -I/usr/local/opt/llvm/include -I/Users/slaan3/miniconda3/envs/cp4/include -I/Users/slaan3/miniconda3/envs/cp4/include/darwin -I/Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include -I/Users/slaan3/miniconda3/envs/cp4/include/python3.8 -c _javabridge.c -o build/temp.macosx-10.9-x86_64-cpython-38/_javabridge.o\n      In file included from _javabridge.c:598:\n      In file included from /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n      In file included from /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/ndarrayobject.h:12:\n      In file included from /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969:\n      /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: \"Using deprecated NumPy API, disable it with \"          \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n      #warning \"Using deprecated NumPy API, disable it with \" \\\n       ^\n      In file included from _javabridge.c:601:\n      ./mac_javabridge_utils.h:47:25: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacStopVM();\n                              ^\n                               void\n      ./mac_javabridge_utils.h:55:30: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacRunLoopInit();\n                                   ^\n                                    void\n      ./mac_javabridge_utils.h:63:29: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacRunLoopRun();\n                                  ^\n                                   void\n      ./mac_javabridge_utils.h:71:31: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacRunLoopReset();\n                                    ^\n                                     void\n      ./mac_javabridge_utils.h:79:30: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT void MacStopRunLoop();\n                                   ^\n                                    void\n      ./mac_javabridge_utils.h:88:30: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      JNIEXPORT int MacIsMainThread();\n                                   ^\n                                    void\n      ./mac_javabridge_utils.h:103:20: warning: this function declaration is not a prototype [-Wstrict-prototypes]\n      void MacRunLoopStop();\n                         ^\n                          void\n      _javabridge.c:5159:37: warning: cast to smaller integer type 'int' from 'jobject' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = __Pyx_PyInt_From_int(((int)__pyx_v_self-&gt;o)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 475, __pyx_L1_error)\n                                          ^~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:5444:37: warning: cast to smaller integer type 'int' from 'jobject' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = __Pyx_PyInt_From_int(((int)__pyx_v_self-&gt;o)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 493, __pyx_L1_error)\n                                          ^~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:5668:37: warning: cast to smaller integer type 'int' from 'jclass' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = __Pyx_PyInt_From_int(((int)__pyx_v_self-&gt;c)); if (unlikely(!__pyx_t_1)) __PYX_ERR(0, 502, __pyx_L1_error)\n                                          ^~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:6022:41: warning: cast to smaller integer type 'int' from 'jmethodID' (aka 'struct _jmethodID *') [-Wpointer-to-int-cast]\n        __pyx_t_4 = __Pyx_PyUnicode_From_int(((int)__pyx_v_self-&gt;id), 0, ' ', 'x'); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 521, __pyx_L1_error)\n                                              ^~~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:6301:41: warning: cast to smaller integer type 'int' from 'jfieldID' (aka 'struct _jfieldID *') [-Wpointer-to-int-cast]\n        __pyx_t_4 = __Pyx_PyUnicode_From_int(((int)__pyx_v_self-&gt;id), 0, ' ', 'x'); if (unlikely(!__pyx_t_4)) __PYX_ERR(0, 535, __pyx_L1_error)\n                                              ^~~~~~~~~~~~~~~~~~~~~\n      _javabridge.c:8122:36: error: implicit declaration of function 'CreateJavaVM' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n        __pyx_t_3 = __Pyx_PyInt_From_int(CreateJavaVM((&amp;__pyx_v_self-&gt;vm), ((void **)(&amp;__pyx_v_env)), (&amp;__pyx_v_args))); if (unlikely(!__pyx_t_3)) __PYX_ERR(0, 651, __pyx_L1_error)\n                                         ^\n      _javabridge.c:8122:36: note: did you mean 'JNI_CreateJavaVM'?\n      /Users/slaan3/miniconda3/envs/cp4/include/jni.h:1949:1: note: 'JNI_CreateJavaVM' declared here\n      JNI_CreateJavaVM(JavaVM **pvm, void **penv, void *args);\n      ^\n      _javabridge.c:9199:5: error: implicit declaration of function 'StopVM' is invalid in C99 [-Werror,-Wimplicit-function-declaration]\n          StopVM(__pyx_v_self-&gt;vm);\n          ^\n      _javabridge.c:20700:18: warning: cast to smaller integer type 'int' from 'jobject' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = ((((int)__pyx_v_s-&gt;o) == 0) != 0);\n                       ^~~~~~~~~~~~~~~~~\n      _javabridge.c:20828:18: warning: cast to smaller integer type 'int' from 'jobject' (aka 'struct _jobject *') [-Wpointer-to-int-cast]\n        __pyx_t_1 = ((((int)__pyx_v_s-&gt;o) == 0) != 0);\n                       ^~~~~~~~~~~~~~~~~\n      _javabridge.c:27819:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:27919:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:28050:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:28181:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:28284:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:28464:3: warning: 'tp_print' is deprecated [-Wdeprecated-declarations]\n        0, /*tp_print*/\n        ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/cpython/object.h:260:5: note: 'tp_print' has been explicitly marked deprecated here\n          Py_DEPRECATED(3.8) int (*tp_print)(PyObject *, FILE *, int);\n          ^\n      /Users/slaan3/miniconda3/envs/cp4/include/python3.8/pyport.h:515:54: note: expanded from macro 'Py_DEPRECATED'\n      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))\n                                                           ^\n      _javabridge.c:29747:3: warning: incompatible pointer to integer conversion returning 'void *' from a function with result type 'int' [-Wint-conversion]\n        import_array();\n        ^~~~~~~~~~~~~~\n      /Users/slaan3/miniconda3/envs/cp4/lib/python3.8/site-packages/numpy/core/include/numpy/__multiarray_api.h:1532:151: note: expanded from macro 'import_array'\n      #define import_array() {if (_import_array() &lt; 0) {PyErr_Print(); PyErr_SetString(PyExc_ImportError, \"numpy.core.multiarray failed to import\"); return NULL; } }\n                                                                                                                                                            ^~~~\n      /Library/Developer/CommandLineTools/usr/lib/clang/14.0.0/include/stddef.h:89:16: note: expanded from macro 'NULL'\n      #  define NULL ((void*)0)\n                     ^~~~~~~~~~\n      _javabridge.c:32040:21: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n                          CYTHON_FALLTHROUGH;\n                          ^\n      _javabridge.c:274:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n            #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n                                       ^\n      _javabridge.c:32051:21: warning: fallthrough annotation in unreachable code [-Wunreachable-code-fallthrough]\n                          CYTHON_FALLTHROUGH;\n                          ^\n      _javabridge.c:274:34: note: expanded from macro 'CYTHON_FALLTHROUGH'\n            #define CYTHON_FALLTHROUGH __attribute__((fallthrough))\n                                       ^\n      _javabridge.c:33284:26: warning: code will never be executed [-Wunreachable-code]\n                      module = PyImport_ImportModuleLevelObject(\n                               ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      25 warnings and 2 errors generated.\n      error: command '/usr/bin/gcc' failed with exit code 1\n      [end of output]\n\n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: legacy-install-failure\n\n\u00d7 Encountered error while trying to install package.\n\u2570\u2500&gt; javabridge\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for output from the failure.\n</code></pre>\n<blockquote>\n<p>continue in the next reply\u2026</p>\n</blockquote>", "<blockquote>\n<p>here\u2019s the next reply</p>\n</blockquote>\n<ul>\n<li>now the error when running <code>cellprofiler</code> is this:</li>\n</ul>\n<pre><code class=\"lang-auto\">cellprofiler413 -c -r -p ~/git/swvanderlaan/slideToolKit/pipeLines/HE.cp413.v1.0.cppipe --file-list files2cp.txt -o cp_output/\n[9587] PyInstaller Bootloader 3.x\n[9587] LOADER: executable is /Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS/cp\n[9587] LOADER: homepath is /Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS\n[9587] LOADER: _MEIPASS2 is NULL\n[9587] LOADER: archivename is /Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS/cp\n[9587] LOADER: /Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS/cp contains a digital signature\n[9587] LOADER: No need to extract files to run; setting extractionpath to homepath\n[9587] LOADER: Already in the child - running user's code.\n[9587] LOADER: Python library: /Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS/libpython3.8.dylib\n[9587] LOADER: Loaded functions from Python library.\n[9587] LOADER: Manipulating environment (sys.path, sys.prefix)\n[9587] LOADER: sys.prefix is /Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS\n[9587] LOADER: Pre-init sys.path is /Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS/base_library.zip:/Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS\n[9587] LOADER: Setting runtime options\n[9587] LOADER: Initializing python\n[9587] LOADER: Overriding Python's sys.path\n[9587] LOADER: Post-init sys.path is /Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS/base_library.zip:/Applications/CellProfiler-macOS-4.1.3.app/Contents/MacOS\n[9587] LOADER: Setting sys.argv\n[9587] LOADER: setting sys._MEIPASS\n[9587] LOADER: importing modules from CArchive\n[9587] LOADER: extracted struct\n[9587] LOADER: callfunction returned...\n[9587] LOADER: extracted pyimod01_os_path\n[9587] LOADER: callfunction returned...\n[9587] LOADER: extracted pyimod02_archive\n[9587] LOADER: callfunction returned...\n[9587] LOADER: extracted pyimod03_importers\n[9587] LOADER: callfunction returned...\n[9587] LOADER: Installing PYZ archive with Python modules.\n[9587] LOADER: PYZ archive: PYZ-00.pyz\n[9587] LOADER: Running pyiboot01_bootstrap.py\n[9587] LOADER: Running pyi_rth_pkgres.py\n[9587] LOADER: Running pyi_rth_certifi.py\n[9587] LOADER: Running pyi_rth_mplconfig.py\n[9587] LOADER: Running pyi_rth_multiprocessing.py\n[9587] LOADER: Running CellProfiler.py\nStarting CellProfiler 4.1.3\nException in thread \"Thread-1\" java.lang.IllegalArgumentException: URI is not hierarchical\n\tat java.io.File.&lt;init&gt;(File.java:418)\n\tat org.cellprofiler.imageset.ImageFile.getFileName(ImageFile.java:72)\n\tat org.cellprofiler.imageset.filter.FileNamePredicate.getValue(FileNamePredicate.java:27)\n\tat org.cellprofiler.imageset.filter.AbstractURLPredicate.eval(AbstractURLPredicate.java:15)\n\tat org.cellprofiler.imageset.filter.AbstractURLPredicate.eval(AbstractURLPredicate.java:10)\n\tat org.cellprofiler.imageset.filter.LogicPredicate.eval(LogicPredicate.java:48)\n\tat org.cellprofiler.imageset.filter.Filter.eval(Filter.java:224)\n\tat org.cellprofiler.imageset.filter.Filter.filterURLs(Filter.java:138)\nFailed to prepare run for module Images\nTraceback (most recent call last):\n  File \"cellprofiler_core/pipeline/_pipeline.py\", line 1368, in prepare_run\n  File \"cellprofiler_core/modules/images.py\", line 354, in prepare_run\n  File \"javabridge/jutil.py\", line 892, in call\n  File \"javabridge/jutil.py\", line 859, in fn\njavabridge.jutil.JavaException: URI is not hierarchical\n[9587] LOADER: OK.\n[9587] LOADER: Manually flushing stdout and stderr\n[9587] LOADER: Cleaning up Python interpreter.\n</code></pre>\n<p>So I googled, I found <a href=\"https://stackoverflow.com/questions/45555761/illegalargumentexception-uri-is-not-hierarchical-when-calling-existing-folder\" rel=\"noopener nofollow ugc\">this article</a>.<br>\nNow I am thinking: does it have to do with the way the contents of <code>files2cp.txt</code> is handled? What should it be?</p>\n<p>This?</p>\n<pre><code class=\"lang-auto\">~/git/swvanderlaan/slideToolKit/upgrade_study/HE/IMG1.HE/IMG1.HE.tiles/IMG1.HE_009.normalized.tile.tissue.png\n~/git/swvanderlaan/slideToolKit/upgrade_study/HE/IMG1.HE/IMG1.HE.tiles/IMG1.HE_022.normalized.tile.tissue.png\n~/git/swvanderlaan/slideToolKit/upgrade_study/HE/IMG1.HE/IMG1.HE.tiles/IMG1.HE_023.normalized.tile.tissue.png\n</code></pre>\n<p>Or this?</p>\n<pre><code class=\"lang-auto\">IMG1.HE.tiles/IMG1.HE_009.normalized.tile.tissue.png\nIMG1.HE.tiles/IMG1.HE_022.normalized.tile.tissue.png\nIMG1.HE.tiles/IMG1.HE_023.normalized.tile.tissue.png\n\n</code></pre>\n<p>Or this even?</p>\n<pre><code class=\"lang-auto\">/Users/username/git/swvanderlaan/slideToolKit/upgrade_study/HE/IMG1.HE/IMG1.HE.tiles/IMG1.HE_009.normalized.tile.tissue.png\n/Users/username/git/swvanderlaan/slideToolKit/upgrade_study/HE/IMG1.HE/IMG1.HE.tiles/IMG1.HE_022.normalized.tile.tissue.png\n/Users/username/git/swvanderlaan/slideToolKit/upgrade_study/HE/IMG1.HE/IMG1.HE.tiles/IMG1.HE_023.normalized.tile.tissue.png\n</code></pre>", "<p>Yes - it\u2019s the latter. The issue is not with all the above, rather with referencing to the exact <code>.png</code>-file that needs to be used.</p>", "<p>Ah, yes - I only now noticed your reply. I had discovered and deduced that too. Thanks!</p>"], "49390": ["<h3>Sample image and/or code</h3>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7fd24cf720e2a24a27f2ad378f46e953484407e8.png\" data-download-href=\"/uploads/short-url/ieLcL6zRJyAoowQOZ5SIuIRBqOY.png?dl=1\" title=\"jNT6E-b8Gf\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/f/7fd24cf720e2a24a27f2ad378f46e953484407e8_2_690x253.png\" alt=\"jNT6E-b8Gf\" data-base62-sha1=\"ieLcL6zRJyAoowQOZ5SIuIRBqOY\" width=\"690\" height=\"253\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/f/7fd24cf720e2a24a27f2ad378f46e953484407e8_2_690x253.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7fd24cf720e2a24a27f2ad378f46e953484407e8.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7fd24cf720e2a24a27f2ad378f46e953484407e8.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/f/7fd24cf720e2a24a27f2ad378f46e953484407e8_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">jNT6E-b8Gf</span><span class=\"informations\">699\u00d7257 28.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>Background</h3>\n<p>Phase/DIC images of cells</p>\n<h3>Analysis goals</h3>\n<p>I used ilastik to generate probability maps which I am now trying to segment those probability outputs using CellProfiler. I want to perform a cell count, then measure the size of cells. From there, I want to compare the proportion of small rounded cells vs proportion of larger flattened cells (which I think the ClassifyObjects module is best for, haven\u2019t gotten there yet!)</p>\n<h3>Challenges</h3>\n<p>After testing my pipeline in Test mode, everything seemed to be working fine. However when I click \u2018Analyze Images\u2019 I get the following error message (screenshot attached):<br>\nError while processing (remote worker):<br>\n(Worker) FileNotFoundError: [Errno 2] No such file or directory: 'g\\my drive\\1 results\\cellprofiler\\Cpmeasurementsvwxvzmb.hdf5<br>\nDo you want to stop processing?</p>\n<p>I tried to set the temp folder to a location I was sure I had admin access (taking advice form <a href=\"https://forum.image.sc/t/windows-8-1-oserror-errno-2-no-such-file-or-directory/13089\">this thread</a>) to no avail. I also tried relaunching the program, no change.<br>\nIf I click \u201cContinue processing\u201d on the error message, the progress bar doesn\u2019t move and time just keeps increasing.</p>\n<p>Any help hugely appreciated, TIA!!</p>", "<p>Hi <a class=\"mention\" href=\"/u/cherry\">@cherry</a>,</p>\n<p>Are you trying to access the images that are in the external hard drive?</p>\n<p>Since you said, in the test mode your pipeline works fine is it the same image location (i.e.image file path) you are trying to use?</p>\n<p>May be screen shot of the terminal/command window, a sample image with your sample pipeline will help us to reproduce the error so we can help you better.</p>\n<p>Regards,</p>\n<p>Lakshmi<br>\n<a href=\"http://www.wakoautomation.com/\" rel=\"noopener nofollow ugc\">www.wakoautomation.com</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/cherry\">@cherry</a>,</p>\n<p>It looks like you\u2019re running this on Windows - if so would you be able to copy in the expanded error message that should appear in the black console window?</p>\n<p>It might also help if you could upload the .cpproj file you\u2019re trying to run.</p>\n<p>Thanks</p>", "<p>Hi David,</p>\n<p>The error message from the black console window is as follows:</p>\n<ul>\n<li>Progress Counter({\u2018Unprocessed\u2019: 5})</li>\n<li>Starting workers on address tcp://127.0.0.1:59822</li>\n<li>Progress Counter({\u2018Unprocessed\u2019: 4, \u2018InProcess\u2019: 1})</li>\n<li>Worker 1: Starting job</li>\n<li>Worker 1: Error in worker</li>\n<li>Worker 1: Traceback (most recent call last):</li>\n<li>Worker 1:   File \u201ccellprofiler_core\\worker_worker.py\u201d, line 197, in do_job</li>\n<li>Worker 1:   File \u201ccellprofiler_core\\utilities\\measurement.py\u201d, line 91, in load_measurements_from_buffer</li>\n<li>Worker 1:   File \u201ctempfile.py\u201d, line 331, in mkstemp</li>\n<li>Worker 1:   File \u201ctempfile.py\u201d, line 250, in _mkstemp_inner</li>\n<li>Worker 1: FileNotFoundError: [Errno 2] No such file or directory: \u2018g:\\my drive\\1 results\\cellprofiler\\Cpmeasurementshlutozgc.hdf5\u2019</li>\n<li>Worker 0: Starting CellProfiler 4.0.7</li>\n<li>Worker 1: Starting CellProfiler 4.0.7</li>\n<li>Worker 0: Cancelling worker</li>\n<li>Worker 1: Cancelling worker</li>\n<li>Worker 3: Starting CellProfiler 4.0.7</li>\n<li>Worker 3: Cancelling worker</li>\n<li>Worker 2: Starting CellProfiler 4.0.7</li>\n<li>Worker 2: Cancelling worker</li>\n<li>Cancelled!</li>\n</ul>\n<p>I have attached the .cpproj file and a sample image.</p>\n<p>Many thanks!<br>\nCherry</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/zr2VpQPUXaYklBT8JUr6UdNbu9I.cpproj\">Counting cells TEST v2.0.cpproj</a> (434.3 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/r9iKlRLBCOhXbtpuHIehLjHAsmI.tif\">T2006_LMN+DCN+IL8_day2_5Mar20_10x_bot_ADJ.tif</a> (14.1 MB)</p>", "<p>Hi Lakshmi,</p>\n<p>The images \u03b1re located in a local university network drive \u2013 could this be an issue?<br>\nI have replied to David below with the error messages etc.</p>\n<p>Thanks for your reply,<br>\nCherry</p>", "<p>Hi <a class=\"mention\" href=\"/u/cherry\">@cherry</a>,</p>\n<p>Sometimes reading the input images from the external drive might be a problem.<br>\nI tried (in version CP 4.0.7) your pipeline with the sample image that you shared. I clicked on the Analyze images, it works perfectly fine. Got all the output files. I tried both in the default output folder option &amp; elsewhere (chose a path in my local machine).<br>\nDid you try having images in your local machine?</p>\n<p>Regards,</p>\n<p>Lakshmi<br>\n<a href=\"http://www.wakoautomation.com/\" rel=\"noopener nofollow ugc\">www.wakoautomation.com</a></p>", "<p>Thanks for that <a class=\"mention\" href=\"/u/cherry\">@cherry</a>, it definitely looks like the software is unable to write temporary files to drive G. Would you be able to send a screenshot of your system environment variables?</p>", "<p>Thanks everyone! A simple move of files to a local location helped (can\u2019t write into Google Drive).</p>", "<aside class=\"quote no-group\" data-username=\"cherry\" data-post=\"1\" data-topic=\"49390\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/c/da6949/40.png\" class=\"avatar\"> Cherry Sun:</div>\n<blockquote>\n<p>(Worker) FileNotFoundError: [Errno 2] No such file or directory: 'g\\my drive\\1</p>\n</blockquote>\n</aside>\n<p>Try using the exact, or absolute, path. The terms absolute and relative also have their usual English meaning. A relative path shows where something is relative to some start point; an absolute path is a location starting from the top.</p>\n<p>When you open a file with the name \u201cfilename.ext\u201d; you are telling the open() function that your file is in the current working directory . This is called a relative path.</p>\n<p><code>file = open('filename.ext') //relative path</code></p>\n<p>In the above code, you are not giving the full path to a file to the open() function, just its name - a relative path. The error \u201c<a href=\"http://net-informations.com/python/err/path.htm\" rel=\"noopener nofollow ugc\">FileNotFoundError</a>: [Errno 2] No such file or directory\u201d is telling you that there is no file of that name in the working directory. So, try using the exact, or absolute path.</p>\n<p><code>file = open(r'C:\\path\\to\\your\\filename.ext') //absolute path</code></p>\n<p>In the above code, all of the information needed to locate the file is contained in the path string - absolute path.</p>\n<p>It\u2019s a common misconception that relative paths are relative to the location of the python script, but this is untrue. Relative file paths are always relative to the current working directory, and the current working directory doesn\u2019t have to be the location of your python script. . In order to make this work, the directory containing the python executable must be in the PATH, a so-called environment variable that contains directories that are automatically used for searching executables when you enter a command. In any case, if your Python script file and your data input file are not in the same directory, you always have to specify either a relative path between them or you have to use an absolute path for one of them.</p>"], "71923": ["<p>Hi everyone,</p>\n<p>I am working on identifying cells with a secondary stain using my DAPI stain as the nuclear marker. The secondary stain incorporates cell processes, so it should be a diffuse stain within the cytoplasm. When I use IdentifySecondaryObject (with DAPI as my primary object), some of the secondary objects overlap entirely and exactly with the DAPI primary objects. I\u2019ve determined this is auto-fluorescence and not true staining, since the cytoplasm is not included. Is there a way to eliminate any secondary object that overlaps exactly with the primary object? I\u2019d rather not mess with the brightness/contrast and thresholding to try and eliminate the remaining auto-fluorescence at this point. Perhaps a filter option that I cannot find?</p>\n<p>Here is an example: the green is the DAPI nuclei and the magenta is the secondary staining in the cytoplasm.</p>\n<p>Thanks everyone!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95.png\" data-download-href=\"/uploads/short-url/5fIIELrj26PrFlANWWq7dB9uAGp.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95_2_690x417.png\" alt=\"image\" data-base62-sha1=\"5fIIELrj26PrFlANWWq7dB9uAGp\" width=\"690\" height=\"417\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95_2_690x417.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24d1d1361c28cfe410355f24eeed35c9930bdd95_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">835\u00d7505 86.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/svkremer\">@svkremer</a></p>\n<p>Welcome to the forum!</p>\n<p>Great question, and yes, after you Identify the primary and secondary object you can measure the overlap between them using the <strong>MeasureObjectOverlap</strong> module and than you can filter out the exact overlap objects (they should have value of 1) using the <strong>FilterObjects</strong> module.</p>\n<p>Best,<br>\nMario</p>", "<p>Hi <a class=\"mention\" href=\"/u/mcruz\">@Mcruz</a> ,</p>\n<p>Thank you for your help, I think I am on the right track now!</p>\n<p>When I use the MeasureObjectOverlap module, the false positives result window is the staining that is true. I would like to just be able to take the false positive output from the MeasureObjectOverlap module and use that for my measurements. Is this possible? I do not see a way to name the false positive window and use it in future steps of the pipeline.</p>\n<p>I believe that is what the FilterObjects module is supposed to do, but I am having trouble using that module. There does not seem to be a direct link between the two modules, i.e. a way to take the false positive window and use it in the FilterObjects module. I believe I am missing a crucial piece of the FilterObjects module, so any advice would be appreciated!</p>", "<p>Hi <a class=\"mention\" href=\"/u/svkremer\">@svkremer</a>,</p>\n<p>I\u2019m looking here and I\u2019m not able to use the <strong>MeasureObjectOverlap</strong> modules output as a filter, sorry for that.</p>\n<p>We can use another strategy to remove your objects (without <strong>MeasureObjectOverlap</strong>). After the Identify Primary and Secondary objects we can add <strong>MeasureObjectSizeAndShape</strong> module, within this module we can measure the area of your nuclei and cell. After that we can add the <strong>CalculateMath</strong> module, here you can divide the Area of your cell per nuclei area (RatioCellPerNuclei).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18d62dc686b45407285436e316972922ce7f45dc.png\" data-download-href=\"/uploads/short-url/3xIkMNEQFYUnMcKK0p2CokcEz1i.png?dl=1\" title=\"Screen Shot 2022-09-26 at 8.59.20 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/8/18d62dc686b45407285436e316972922ce7f45dc.png\" alt=\"Screen Shot 2022-09-26 at 8.59.20 AM\" data-base62-sha1=\"3xIkMNEQFYUnMcKK0p2CokcEz1i\" width=\"628\" height=\"500\" data-dominant-color=\"E6E7E8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-09-26 at 8.59.20 AM</span><span class=\"informations\">659\u00d7524 109 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Finally you can use the <strong>FilterObject</strong> module to filter out the cells without cytoplasm.<br>\nFor that select the filtering method as Limits<br>\nCategory: Math<br>\nMeasurement: RatioCellPerNuclei<br>\nFilter using a minimum measurement value: Here you can choose the value you want to filter the objects. (values above 1 means the cytoplasm area are bigger than the nuclei area, but you can be more strict (eg.1.1) to filter objects with really small cytoplasm areas).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6.png\" data-download-href=\"/uploads/short-url/yE4yo4cNMaobxcKerL68gCGmLsy.png?dl=1\" title=\"Screen Shot 2022-09-26 at 9.02.16 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6_2_589x500.png\" alt=\"Screen Shot 2022-09-26 at 9.02.16 AM\" data-base62-sha1=\"yE4yo4cNMaobxcKerL68gCGmLsy\" width=\"589\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6_2_589x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/2/f2d19288e8d2edeae1277eb17aeb0efa03dc73d6.png 2x\" data-dominant-color=\"E6E7E8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-09-26 at 9.02.16 AM</span><span class=\"informations\">675\u00d7573 111 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best,<br>\nMario</p>", "<p>Hi Mario,</p>\n<p>Thank you so much! I had to step away from this project for a while, but your solution works perfectly! Really appreciate your help and expertise (:</p>\n<p>Best,<br>\nSarah</p>"], "80628": ["<p>Hello everyone,</p>\n<p>I\u2019m currently trying to access image files from OMERO via Python API and to pass them to CellProfiler within a conda environment. I mainly following the <a href=\"https://omero-guides.readthedocs.io/en/latest/cellprofiler/docs/index.html#\" rel=\"noopener nofollow ugc\">OME CellProfiler guides</a> but also any other forum thread or documentation that I could find for this topic.</p>\n<p>The examples posted on the OME site and in other threads are usually quite simple in terms of example data. Based on this example code, I could set up the connection to our OMERO server followed by successful CellProfiler analysis if I use simple tiff-files with one channel, z-plane and timepoint each.</p>\n<p>However, in my project I need to execute a pipeline on multi-dimenstional czi-files (multi-channel and time series; one example file can be downloaded at the <a href=\"https://www.ebi.ac.uk/biostudies/files/S-BIAD295/20210521_1_Treatment_MbxAFresh_250nM_20min.czi\" rel=\"noopener nofollow ugc\">bioimage archive</a>). The Cellprofiler pipeline is already established and \u2013 if run via UI \u2013 relies on the \u201cExtract metadata\u201d button and the \u201cGroups\u201d module definition for the time series. I now really struggle how to set that up in Python as the example code from the OME team is removing the \u201cGroups\u201d module which is needed for the time series, but I could not find a documentation how to pass this information to the InjectImage module.</p>\n<p>Is there anyone out that, that did already something similar and is willing to share the code?<br>\nOr maybe there is some proper documentation for the InjectImage module that I did not find yet?</p>\n<p>My idea was to pass the numpy slices per channel and timepoint to the InjectImage module but that didn\u2019t work out (at least not in my several tries).</p>\n<p>That\u2019s a part of the original code from the OME team I tried to adapt for this:</p>\n<pre><code>files = list()\nimages = list(dataset.listChildren())\nfor count, image in enumerate(images):\n    print(image.getName())\n    pixels = image.getPrimaryPixels()\n    size_c = image.getSizeC()\n    # For each Image in OMERO, we copy pipeline and inject image modules\n    pipeline_copy = pipeline.copy()\n    # Inject image for each Channel (pipeline only handles 2 channels)\n    for c in range(0, size_c):\n        plane = pixels.getPlane(0, c, 0)\n        image_name = image.getName()\n        # Name of the channel expected in the pipeline\n        if c == 0:\n            image_name = 'OrigBlue'\n        if c == 1:\n            image_name = 'OrigGreen'\n        inject_image_module = InjectImage(image_name, plane)\n        inject_image_module.set_module_num(1)\n        pipeline_copy.add_module(inject_image_module)\n    pipeline_copy.run()\n\n    # Results obtained as CSV from Cell Profiler\n    path = new_output_directory + '/Nuclei.csv'\n    files.append(path)\nprint(\"analysis done\")\nreturn files\n</code></pre>\n<p>Thanks a lot in advance for any hints,<br>\nAnna</p>", "<p>Hi <a class=\"mention\" href=\"/u/ahamacher\">@ahamacher</a></p>\n<p>Are you reading the images from OMERO or reading them from Disk since you linked to an example on BioImageArchive?<br>\nif you are reading from OMERO,  you could adjust the loop to read the timepoint<br>\ni.e.</p>\n<pre><code class=\"lang-auto\">size_c = image.getSizeC()\nsize_t = image.getSizeT()\nfor t in range(0, size_t)\n    for c in range(0, size_c):\n        plane = pixels.getPlane(0, c, t)\n</code></pre>\n<p>For the example in the guide, we did not need the \u201cGroups\u201d option, did you try to add it back and run the pipeline with the module include?</p>\n<p>Jmarie</p>", "<p>Hi Jean-Marie,</p>\n<p>thanks for your response. Yes, I\u2019m accessing the data from our OMERO server, I just posted the BioImage Archive link for some example data I use within our (not public) OMERO.</p>\n<p>Adding a loop with size_t works generally, but then all images are processed separately and the tracking modules doesn\u2019t produce any useful output. I tried to pass the whole numpy array to the InjectImage module and to leave the \u201cGroups\u201d module in the CellProfiler pipeline, but this doesn\u2019t produce any output at all (even no error). I think the problem with the \u201cGroups\u201d module is that it splits the images based on Metadata and when images are added with the InjectImage module then I do not see any \u201cMetadata_\u201d columns in the output csv-files.</p>\n<p>Therefore I guess, I need to figure out how I can pass the \u201cGroups\u201d information in python or how I can set the \u201cMetadata\u201d information before the first analysis module is processed. I was checking already the <a href=\"https://github.com/CellProfiler/core/blob/59f412cbb70fc4e1c5f3a242967b7b865deeebad/cellprofiler_core/modules/injectimage.py\" rel=\"noopener nofollow ugc\">cellprofiler_core code</a> this morning but do not see the possibility to do it with the InjectImage module.</p>\n<p>Maybe I need to define the cellprofiler workspace with image_sets before injecting the images or similar? But I guess this is something the CellProfiler people know best.  Maybe <a class=\"mention\" href=\"/u/dstirling\">@DStirling</a> or <a class=\"mention\" href=\"/u/agoodman\">@agoodman</a> have some hints on that?</p>\n<p>Any help is much appreciated!<br>\nAnna</p>"], "78586": ["<p>Hi,</p>\n<p>I\u2019m currently making some tools that will likely have to parse and modify cellprofiler pipelines. It looks like there\u2019s work towards <a href=\"https://github.com/CellProfiler/core/pull/89\" rel=\"noopener nofollow ugc\">JSON support</a>, so I\u2019m hoping I don\u2019t have to write my own parser, but I can\u2019t seem to find how to export a pipeline as JSON format in 4.2.5.</p>\n<p>Is this still work in progress, or have a missed a big obvious button somewhere?</p>"], "78080": ["<p>I want to classify and determine the number of cells positive for individual signals and those positive for two or three signals. I am using ClassifyObjects module in the Cellprofiler software but can not seem to get the right data. ### Sample image and/or code</p>\n<ul>\n<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox) \u2013 (make sure however that you are allowed to share the image data publicly under the conditions of this forum).</li>\n<li>Share a <a href=\"https://en.wikipedia.org/wiki/Minimal_working_example\" rel=\"noopener nofollow ugc\">minimal working example</a> of your macro code.</li>\n</ul>\n\n<h3>\n<a name=\"background-1\" class=\"anchor\" href=\"#background-1\"></a>Background</h3>\n<ul>\n<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific \u201cjargon\u201d.</li>\n</ul>\n<h3>\n<a name=\"analysis-goals-2\" class=\"anchor\" href=\"#analysis-goals-2\"></a>Analysis goals</h3>\n<ul>\n<li>What information are you interested in getting from this image?</li>\n</ul>\n<h3>\n<a name=\"challenges-3\" class=\"anchor\" href=\"#challenges-3\"></a>Challenges</h3>\n<ul>\n<li>What stops you from proceeding?</li>\n<li>What have you tried already?</li>\n<li>Have you found any related forum topics? If so, cross-link them.</li>\n<li>What software packages and/or plugins have you tried?</li>\n</ul>", "<p>Hi Albert, could you provide your pipeline and a sample image/image set?</p>", "<p>Sure. Sorry for the late reply. First of all, the aim of the work is to classify cells into cfos positive cells, Crh positive cells and ESR1 positive cells found in the Pontine micturition center of the brain. I then want to how many cfos positive cells also expresses Crh, Esr1 or both.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/dYZ1yuwdwTcEmzZuOYG7dutdKid.cpproj\">Nocturia Brain.cpproj</a> (1.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/dpXDG6YciQE11NhI4slxGIM435i.tif\">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c1.tif</a> (6.5 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/eSJSu0l3BtNrXWGod6kQser3moh.tif\">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c2.tif</a> (6.5 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/sfxj4nDJE5lpnTnIjnY8UxZSLF5.tif\">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c3.tif</a> (6.5 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/1wwMMh3EWOBSJTzH5hXeZtreNQb.tif\">20230123_AAA_NOC1_PMC_270_66_DAPI_488fos_550crh_647esr1_1_L_c4.tif</a> (6.5 MB)</p>\n<p>Also on a related but different inquiries, any suggestion on how to compress tiff files.</p>"], "75521": ["<p>Dear community,</p>\n<p>I\u2019m trying to use the RunImageJMacro module with CellProfiler 4.2.4 on Windows 10.</p>\n<p>I\u2019m running into an error where Cellprofiler doesn\u2019t seem to run the script, and consequently doesn\u2019t create the output.tif file.</p>\n<p>I followed the dedicated Youtube video as well as another tutorial found here:<br>\n<a href=\"https://visikol.com/blog/2022/11/18/imagej-functions-cell-profiler/\" rel=\"noopener nofollow ugc\">How to Integrate ImageJ Functions into Cell Profiler (visikol.com)</a></p>\n<p>I used the exact same configuration, but it seems the script isn\u2019t used at all by Cellprofiler. Running it within Fiji works as expected, loading and saving files wherever needed.<br>\nI used some of the simplest command (blur filter, inverting the image, writing text file,\u2026) but none work.</p>\n<p>Any ideas ? (It\u2019s not urgent).</p>", "<p>Hi Yannick,</p>\n<p>Can you share the pipeline and the script? That will help us figure out what the issue is. Thanks!</p>", "<p><a class=\"attachment\" href=\"/uploads/short-url/yCsP2wpET2BZXCNnn1WSTmkRfNN.cpproj\">cellprofiler_fiji_test_pipeline.cpproj</a> (414.5 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/aj7dNPlKo5bOLKzSgJhhjkC97Pn.ijm\">script_for_cellprofiler_.ijm</a> (230 Bytes)</p>\n<p>Here you go, it\u2019s as simple as it gets I think.<br>\nI\u2019ve tested it with CellProfiler 4.2.4 and Fiji. The script runs as expected within Fiji\u2019s script editor. If I use the pipeline attached, CellProfiler just gets stuck and does nothing. I don\u2019t see any error message nor any info in the terminal window.</p>\n<p>I must must be doing something wrong but I can\u2019t pinpoint what exactly.<br>\nThanks for the help.</p>", "<p>Took a bit to track this down, but eventually found it- apparently, there is a bug in ImageJ\u2019s Gaussian Blur that <a href=\"https://forum.image.sc/t/running-gaussian-blur-in-fiji-headless/69409\">causes it to not close when run headlessly</a> that cropped up a bit ago. Because the ImageJ macro doesn\u2019t close, CellProfiler has no way to know it\u2019s done, and it sits around waiting forever. If you change the Gaussian line to something like <code>run(\"Invert\");</code>, you should see it work.</p>\n<p>Even adding <code>exit(\"some error message\");</code> or <code>exit;</code> or <code>run(\"Quit\");</code> (h/t <a href=\"https://forum.image.sc/t/imagej-macro-to-exit-imagaj-process/28078/2\">this post</a>) don\u2019t actually exit the macro - if you do <code>exit(\"some error message\");</code>, it will print that error message to the console, but not actually-actually quit.</p>\n<p>Unfortunately, other than \u201cCellProfiler has a Gaussian Blur module, use that if you need to do a Gaussian\u201d, there\u2019s not a ton more help I can give until the ImageJ team fixes this on their end. Sorry!</p>", "<p>Thank you for finding the issue, that was a tricky one !</p>\n<p>I have tested this simple script with <em>Median</em>, <em>Minimum</em> and <em>Variance</em> instead of <em>Gaussian</em>, and I see the same behavior, so it must be the way filters are applied.<br>\nAs long as I comment out this step, the rest of the script runs fine. For instance, if I use another filter like <em>FFT</em> it works.</p>\n<p>Anyways, I used <em>Gaussian</em> just as a simple test to see if I had set up the module correctly. I think I couldn\u2019t have chosen worse example\u2026 <img src=\"https://emoji.discourse-cdn.com/twitter/sweat_smile.png?v=12\" title=\":sweat_smile:\" class=\"emoji\" alt=\":sweat_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Thanks!</p>", "<aside class=\"quote no-group\" data-username=\"bcimini\" data-post=\"4\" data-topic=\"75521\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/bcimini/40/14206_2.png\" class=\"avatar\"> Beth Cimini:</div>\n<blockquote>\n<p>Took a bit to track this down, but eventually found it- apparently, there is a bug in ImageJ\u2019s Gaussian Blur that <a href=\"https://forum.image.sc/t/running-gaussian-blur-in-fiji-headless/69409\">causes it to not close when run headlessly </a> that cropped up a bit ago.</p>\n</blockquote>\n</aside>\n<p>This appears to be an ImageJ2/Fiji problem. Macros that use Gaussian Blur will exit as expected if you run them using the -batch command line option, instead of --run, which causes ImageJ to run the macro. You can also work around this problem by adding eval(\u201cjs\u201d,\u201cSystem.exit(1)\u201d); to the end of macros.</p>"], "75009": ["<p>Hello,<br>\nI am using the Classifier tool in CPA to differentiate actin cytoskeleton phenotypes following processing of my images in a CellProfiler (4.2.4) pipeline. The properties file opens fine in CPA, but when I attempt to Fetch objects, some do not appear and this error message appears:</p>\n<p>\u201cAn error occurred in the program:<br>\nException: Failed to load coordinates for object key ImageNumber:14, ObjectNumber:1. This may indicate a problem with your per-object table.<br>\nYou can check your per-object table \u201cPer_Object\u201d in TableViewer\u201d</p>\n<p>And here is what I see in my per-object table:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4c7724380c4c750905292cb4a523fc94504216c5.jpeg\" data-download-href=\"/uploads/short-url/aUrxVqFrIGK5lnjB855qSTABMSV.jpeg?dl=1\" title=\"Screenshot 2022-12-13 at 2.36.09 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c7724380c4c750905292cb4a523fc94504216c5_2_690x326.jpeg\" alt=\"Screenshot 2022-12-13 at 2.36.09 PM\" data-base62-sha1=\"aUrxVqFrIGK5lnjB855qSTABMSV\" width=\"690\" height=\"326\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c7724380c4c750905292cb4a523fc94504216c5_2_690x326.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c7724380c4c750905292cb4a523fc94504216c5_2_1035x489.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c7724380c4c750905292cb4a523fc94504216c5_2_1380x652.jpeg 2x\" data-dominant-color=\"EDEDED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2022-12-13 at 2.36.09 PM</span><span class=\"informations\">1920\u00d7909 163 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>It is as if some of the Objects were not property related to their Parent objects, but it is not the case for all objects\u2026 is this because of an issue in the initial pipeline or in CPA? Could someone help me understand and fix this mistake?</p>\n<p>Thanks!</p>\n<p>VVR</p>"], "77059": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p><a class=\"attachment\" href=\"/uploads/short-url/gP3BmAvVQ4Kl7fVEn76qExlzq4a.cpproj\">Pipeline TJ Quant.cpproj</a> (332.7 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/oTFErmHa7j3MRWh9Vn1zbmfRlS.czi\">WP2_V3_Intestinal_Ctrl.czi</a> (8.5 MB)</p>\n\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<p>I have acquired immunofluorescence images from our intestinal model in an organ-on-chip and I would like to quantify intestinal tight junctions E-cadherin (later also ZO-1 and vascular VE-cadherin) in my images. I provided an exemplary image of the data as \u201cczi\u201d format (native format of the ZEISS Zen software; c1=E-Cadherin). I usually extract the metadata from the single channels as grayscale image and do the analysis pipeline afterwards.</p>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<p>I would like to separate the tight junction networks from the background and read the following measurements/analysis:</p>\n<ul>\n<li>Mean intensity of the tight junctions in the whole image</li>\n<li>Area occupied by the tight junctions in the whole image</li>\n<li>Length of the tight junctions in the whole image</li>\n</ul>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<ul>\n<li>Currently I struggle in my pipeline to do tresholding after \u201cMorph\u201d function. I always get a completely black image. Therefore, it is not possible for me to complete my pipeline</li>\n</ul>\n<p>Thank you very much in advance for your help and ideas!</p>", "<p>Hi <a class=\"mention\" href=\"/u/tkchip\">@TKChip</a></p>\n<p>Welcome to the forum!</p>\n<p>I look to your pipeline and I notice you have a high lower bound threshold value, and this is why your result image is blank, to solve the problem you need to change the lower bounds to 0 for example.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b7a06e1a3b980b8771b1b0305f222b4b33ddcd04.png\" data-download-href=\"/uploads/short-url/qcr60KFHVWBobTAg7qixzpJc2Tq.png?dl=1\" title=\"Screen Shot 2023-02-09 at 10.39.23 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7a06e1a3b980b8771b1b0305f222b4b33ddcd04_2_690x403.png\" alt=\"Screen Shot 2023-02-09 at 10.39.23 AM\" data-base62-sha1=\"qcr60KFHVWBobTAg7qixzpJc2Tq\" width=\"690\" height=\"403\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7a06e1a3b980b8771b1b0305f222b4b33ddcd04_2_690x403.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7a06e1a3b980b8771b1b0305f222b4b33ddcd04_2_1035x604.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b7a06e1a3b980b8771b1b0305f222b4b33ddcd04.png 2x\" data-dominant-color=\"E5E6E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-09 at 10.39.23 AM</span><span class=\"informations\">1040\u00d7608 136 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c345afffa50054d154fea4b7b107b911effd4b66.jpeg\" data-download-href=\"/uploads/short-url/rRspvPqwnMRQ633EhQQISciJ0YC.jpeg?dl=1\" title=\"Screen Shot 2023-02-09 at 10.39.43 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c345afffa50054d154fea4b7b107b911effd4b66_2_690x282.jpeg\" alt=\"Screen Shot 2023-02-09 at 10.39.43 AM\" data-base62-sha1=\"rRspvPqwnMRQ633EhQQISciJ0YC\" width=\"690\" height=\"282\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c345afffa50054d154fea4b7b107b911effd4b66_2_690x282.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/3/c345afffa50054d154fea4b7b107b911effd4b66_2_1035x423.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/3/c345afffa50054d154fea4b7b107b911effd4b66.jpeg 2x\" data-dominant-color=\"818181\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-09 at 10.39.43 AM</span><span class=\"informations\">1252\u00d7512 90.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Here is the pipeline <a class=\"attachment\" href=\"/uploads/short-url/uT5XzrGKRnERPlPy3DlJhnNFg51.cppipe\">Pipeline TJ Quant.cppipe</a> (7.6 KB)</p>\n<p>Best,<br>\nMario</p>", "<p>Dear Mario,</p>\n<p>thank you for resolving my problem so fast!</p>\n<p>Maybe you could help me out with another problem I see now after treshholding. The lines of the skeleton appear sometimes too thin or too thick after threshholding. How can I separate the tight junction lines more specific? I tried different tresholding methods which you can see in the images. Maybe you have an idea how to improve the isolation of the junctional network.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/5/9588cad71a7f9d73511d93d13ce6302769775ebd.png\" data-download-href=\"/uploads/short-url/lkQdB9LVFmTv9y5Qq5gBkN4N4aF.png?dl=1\" title=\"Threshold Adaptive Otsu 0.9 Adaptive Window 30\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/5/9588cad71a7f9d73511d93d13ce6302769775ebd_2_690x204.png\" alt=\"Threshold Adaptive Otsu 0.9 Adaptive Window 30\" data-base62-sha1=\"lkQdB9LVFmTv9y5Qq5gBkN4N4aF\" width=\"690\" height=\"204\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/5/9588cad71a7f9d73511d93d13ce6302769775ebd_2_690x204.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/5/9588cad71a7f9d73511d93d13ce6302769775ebd_2_1035x306.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/5/9588cad71a7f9d73511d93d13ce6302769775ebd_2_1380x408.png 2x\" data-dominant-color=\"9B9B9B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Threshold Adaptive Otsu 0.9 Adaptive Window 30</span><span class=\"informations\">1558\u00d7461 241 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/a/3a14db16c613bde1d6a524633e95d4f8e9205be9.png\" data-download-href=\"/uploads/short-url/8hOnlU7y48MkE93K3kS99C007wd.png?dl=1\" title=\"Threshold Adaptive Otsu 0.9 Adaptive Window 50\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a14db16c613bde1d6a524633e95d4f8e9205be9_2_690x198.png\" alt=\"Threshold Adaptive Otsu 0.9 Adaptive Window 50\" data-base62-sha1=\"8hOnlU7y48MkE93K3kS99C007wd\" width=\"690\" height=\"198\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a14db16c613bde1d6a524633e95d4f8e9205be9_2_690x198.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a14db16c613bde1d6a524633e95d4f8e9205be9_2_1035x297.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/a/3a14db16c613bde1d6a524633e95d4f8e9205be9_2_1380x396.png 2x\" data-dominant-color=\"9C9C9C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Threshold Adaptive Otsu 0.9 Adaptive Window 50</span><span class=\"informations\">1597\u00d7460 238 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/7/a7ff79d4481b1ec661f12f14fb27ef2b48e943c7.png\" data-download-href=\"/uploads/short-url/nYb5Id6cKWx5bQTXxGxVxAYm8qb.png?dl=1\" title=\"Threshold Global Otsu 0.7\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7ff79d4481b1ec661f12f14fb27ef2b48e943c7_2_690x210.png\" alt=\"Threshold Global Otsu 0.7\" data-base62-sha1=\"nYb5Id6cKWx5bQTXxGxVxAYm8qb\" width=\"690\" height=\"210\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7ff79d4481b1ec661f12f14fb27ef2b48e943c7_2_690x210.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7ff79d4481b1ec661f12f14fb27ef2b48e943c7_2_1035x315.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7ff79d4481b1ec661f12f14fb27ef2b48e943c7_2_1380x420.png 2x\" data-dominant-color=\"A2A2A2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Threshold Global Otsu 0.7</span><span class=\"informations\">1669\u00d7508 269 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thank you very much in advance!</p>", "<p>Hi <a class=\"mention\" href=\"/u/tkchip\">@TKChip</a></p>\n<p>I\u2019m happy that works for you!</p>\n<p>The first ideia that came to me was to use the <strong>EnhanceOrSuppressFeatures</strong> module, here you can enhance the tubeness prior to thresholding.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/10d40be6749fbb5fa28eaba59934f7bb02d1ff77.png\" data-download-href=\"/uploads/short-url/2oRWImWdQo6jJByEWXM5UwMQLsj.png?dl=1\" title=\"Screen Shot 2023-02-10 at 8.47.07 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10d40be6749fbb5fa28eaba59934f7bb02d1ff77_2_690x315.png\" alt=\"Screen Shot 2023-02-10 at 8.47.07 AM\" data-base62-sha1=\"2oRWImWdQo6jJByEWXM5UwMQLsj\" width=\"690\" height=\"315\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10d40be6749fbb5fa28eaba59934f7bb02d1ff77_2_690x315.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10d40be6749fbb5fa28eaba59934f7bb02d1ff77_2_1035x472.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10d40be6749fbb5fa28eaba59934f7bb02d1ff77_2_1380x630.png 2x\" data-dominant-color=\"ECEDEE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-10 at 8.47.07 AM</span><span class=\"informations\">1434\u00d7656 184 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/4/d4c41c26642db99ab7da624a4fc09e7cb51dc196.jpeg\" data-download-href=\"/uploads/short-url/umdo17vTV0sfduM5LuaclATG42G.jpeg?dl=1\" title=\"Screen Shot 2023-02-10 at 8.49.25 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4c41c26642db99ab7da624a4fc09e7cb51dc196_2_690x311.jpeg\" alt=\"Screen Shot 2023-02-10 at 8.49.25 AM\" data-base62-sha1=\"umdo17vTV0sfduM5LuaclATG42G\" width=\"690\" height=\"311\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4c41c26642db99ab7da624a4fc09e7cb51dc196_2_690x311.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4c41c26642db99ab7da624a4fc09e7cb51dc196_2_1035x466.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/4/d4c41c26642db99ab7da624a4fc09e7cb51dc196_2_1380x622.jpeg 2x\" data-dominant-color=\"484848\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-10 at 8.49.25 AM</span><span class=\"informations\">1580\u00d7714 72.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best,<br>\nMario</p>"], "77060": ["<p>Hello, i finally got to work with cellprofiler; what a great tool!!. Thanks to all the developers.<br>\nI am working with whole slide images, I have an issue loading images that are pyramidal ome.tif into cellprofiller.<br>\nWhen I open my files in Fiji the metadata is as expected. Also, the bigdata viewer in fiji can open it without any problems.<br>\nIs there any particular way to enter the metadata into cellprofiller for this kind of image?<br>\ni have attached the metadata files from fiji and cellprofiller, and also here is a link to the image (too big to attach it).</p>\n<aside class=\"onebox googledrive\" data-onebox-src=\"https://drive.google.com/file/d/12dMJneL_zzWXn5HjuJXqovbUSWkvF9oV/view?usp=share_link\">\n  <header class=\"source\">\n\n      <a href=\"https://drive.google.com/file/d/12dMJneL_zzWXn5HjuJXqovbUSWkvF9oV/view?usp=share_link\" target=\"_blank\" rel=\"noopener nofollow ugc\">drive.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n      <a href=\"https://drive.google.com/file/d/12dMJneL_zzWXn5HjuJXqovbUSWkvF9oV/view?usp=share_link\" target=\"_blank\" rel=\"noopener nofollow ugc\"><span class=\"googledocs-onebox-logo g-drive-logo\"></span></a>\n\n\n\n<h3><a href=\"https://drive.google.com/file/d/12dMJneL_zzWXn5HjuJXqovbUSWkvF9oV/view?usp=share_link\" target=\"_blank\" rel=\"noopener nofollow ugc\">2022_10_24__6175_czi_Scene #09_Overlay.ome.tif</a></h3>\n\n<p>Google Drive file.</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p><a class=\"attachment\" href=\"/uploads/short-url/cKqHRZruwYmziyrRyfph7KojMFS.csv\">Original Metadata - 2022_10_24__6175.czi - Scene #09_Overlay.ome.csv</a> (357 Bytes)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/8/f8fc4fcdff20b4a88c55879adb5fb4e7a1b98a82.jpeg\" data-download-href=\"/uploads/short-url/zwCZg9TdMhdktvmIPZeEvonOFhw.jpeg?dl=1\" title=\"metadata cell profiller\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8fc4fcdff20b4a88c55879adb5fb4e7a1b98a82_2_690x346.jpeg\" alt=\"metadata cell profiller\" data-base62-sha1=\"zwCZg9TdMhdktvmIPZeEvonOFhw\" width=\"690\" height=\"346\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8fc4fcdff20b4a88c55879adb5fb4e7a1b98a82_2_690x346.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8fc4fcdff20b4a88c55879adb5fb4e7a1b98a82_2_1035x519.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/8/f8fc4fcdff20b4a88c55879adb5fb4e7a1b98a82_2_1380x692.jpeg 2x\" data-dominant-color=\"E9EAEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">metadata cell profiller</span><span class=\"informations\">1920\u00d7963 69 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi,</p>\n<p>CellProfiler 4 does not support reading from pyramidal tiffs, or, rather, at anything but the first layer of the pyramid (the largest layer). It\u2019s something our team is working hard on for CellProfiler 5! Apologies.</p>", "<p>Thanks for the quick response!. And for a great image analysis tool. I\u2019ll look forward to the option. Your method for spot detection is wonderful.<br>\nThanks again<br>\nPaola</p>", "<p>Hi <a class=\"mention\" href=\"/u/paola\">@paola</a></p>\n<p>We had this kind of problem before and we wrote a macro in ImageJ to extract and save the layer you want as a regular tiff file, that you can use in CellProfiler. As <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> mentioned we are working to implement this support in CP5.</p>\n<p>This script will ask you the input folder, the series you wanna save and the output folder.</p>\n<pre><code class=\"lang-auto\">// This macro opens a .ome.tif file and save as a TIFF file.\n// Wrote by: Mario Cruz  \n// Last update: 07/29/2022\n// @ Cimini's Lab (The Broad Institute of MIT and Harvard)\n\ndirFiles=getDir(\"Get input directory\");\n//outputdir=getDir(\"Get outputdirectory\");\nDialog.create(\"Parameters Configuration\");\nDialog.addNumber(\"Choose which Series?\", \"3\");\nDialog.show();\noutputdir=getDir(\"Get outputdirectory\");\nS = parseInt(Dialog.getNumber());\nallFiles=getFileList(dirFiles);\nprint(allFiles.length);\nsetBatchMode(true);\nfs=File.separator;\n//count the .ome.tif files, open the bioformats and count the series\nfor(f=0; f&lt;allFiles.length; f++) {\nif (endsWith(allFiles[f], \".ome.tif\"))\n   fileName=allFiles[f];\n   series = \"_series_\" + S;\n   savePath=outputdir + fileName + fs;\n   run(\"Bio-Formats Macro Extensions\");\n   Ext.setId(dirFiles+fileName);\n   Ext.getSeriesCount(seriesCount);\n   sCount=seriesCount;\n   \n//open the .ome.tif file and pick up the chose series\nfor(l=1;l&lt;=sCount;l++){\n\t\n\trun(\"Bio-Formats Importer\", \"open=[\"+dirFiles+fileName+\"] autoscale color_mode=Default view=Hyperstack stack_order=XYCZT series_\"+(S));\n\t\n   //Get the file name without the extension\n   nameStoreNoExtension= File.nameWithoutExtension;\n\n//Convert the 8Bit composite image to a RGB image\n//run(\"RGB Color\");\n\nsaveAs(\"TIFF\", outputdir+nameStoreNoExtension+series);\n\n}\n}\nprint(\"Finished!\");\n</code></pre>\n<p>Best,<br>\nMario</p>", "<p>Wonderful! thanks!!!<br>\nI\u2019ll try it<br>\nPaola</p>"], "75014": ["<p>I need to segment the TIFF images from Akoya Biosciences inForm for segmentation and and furtheron into phenotyping.<br>\nIn the pdf file of their instruction, it says,</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://view-su2.highspot.com/viewer/63862fd9ada1cc440a00985e?source=email.63862fd9ada1cc440a00985f.0&amp;iid=6298e82a86dae13ec65a6386\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/2/b258b2416a36c7654761d35f8f15d17564d35b3b.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://view-su2.highspot.com/viewer/63862fd9ada1cc440a00985e?source=email.63862fd9ada1cc440a00985f.0&amp;iid=6298e82a86dae13ec65a6386\" target=\"_blank\" rel=\"noopener nofollow ugc\">view-su2.highspot.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/388;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7bc8a130087aafe2b1131a3b9a751f76923647ce_2_690x388.png\" class=\"thumbnail\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7bc8a130087aafe2b1131a3b9a751f76923647ce_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7bc8a130087aafe2b1131a3b9a751f76923647ce.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7bc8a130087aafe2b1131a3b9a751f76923647ce.png 2x\" data-dominant-color=\"48515F\"></div>\n\n<h3><a href=\"https://view-su2.highspot.com/viewer/63862fd9ada1cc440a00985e?source=email.63862fd9ada1cc440a00985f.0&amp;iid=6298e82a86dae13ec65a6386\" target=\"_blank\" rel=\"noopener nofollow ugc\">A. PhenoCycler and PhenoCycler-Fusion  Data Analysis Tutorial</a></h3>\n\n  <p>Akoya Biosciences</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>To use this script, drag and drop the StarDist cell segmentation script.groovy file into the QuPath window.<br>\n(Cell segmentation scripts are available for pixel sizes of 0.5, 0.37, 0.33, and 0.25 microns. The image<br>\npixel size can be found in the image tab.) When the script editor appears click Run \u2192 Run.</p>\n<p>BUT I AM GETTING A LOT OF ERRORS. WHY?<br>\nCAN SOMEONE PLEASE HELP ME.</p>\n<pre><code class=\"lang-auto\">import qupath.lib.gui.dialogs.Dialogs\ndef pathModel2 = Dialogs.showMessageDialog(C:\\Users\\PerkinElmer\\Desktop\\scripts supporting open-source tools for PhenoCycler image analysis)\ndef pathModel = Dialogs.promptForFile(null)\ndef pathModel3 = pathModel.toString()\n//def pathModel = Dialogs.showInputDialog(\"My title\", \"Please paste the StarDist file path without quotes\", 'text')\nimport stardist_cell_seg_model.pb\n\n// if you are always using the same StarDist model you can set the path below instead of selecting via dialog box\n//def pathModel = \"C:\\\\Users\\\\gcarlson\\\\OneDrive - Akoya Biosciences\\\\Desktop\\\\CODEX Analysis Demo\\\\CODEX_cell_seg.pb\"\n\ndef stardist = StarDist2D.builder(pathModel3)\n        .threshold(0.5)              // Probability (detection) threshold\n        .channels(0)            // Select detection channel\n        .normalizePercentiles(1, 99) // Percentile normalization\n        .pixelSize(0.5)              // Resolution for detection\n        .cellExpansion(5.0)          // Approximate cells based upon nucleus expansion\n        .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size\n        .measureShape()              // Add shape measurements\n        .measureIntensity()          // Add cell measurements (in all compartments)\n        .includeProbability(true)    // Add probability as a measurement (enables later filtering)\n        .build()\n\n// Run detection for the selected objects\ndef imageData = getCurrentImageData()\ndef pathObjects = getSelectedObjects()\nif (pathObjects.isEmpty()) {\n    Dialogs.showErrorMessage(\"StarDist\", \"Please select a parent object!\")\n    return\n}\nstardist.detectObjects(imageData, pathObjects)\nprintln 'Done!'\n</code></pre>\n<pre><code class=\"lang-auto\">ERROR: startup failed:\nStarDist cell segmentation script_0.5_um_per_pixel.groovy: 3: Unexpected character: '\\' @ line 2, column 46.\n   = Dialogs.showMessageDialog(C:\\Users\\Per\n                                 ^\n\n1 error\n in StarDist cell segmentation script_0.5_um_per_pixel.groovy at line number 2\n\nERROR: org.codehaus.groovy.control.ErrorCollector.failIfErrors(ErrorCollector.java:292)\n    org.codehaus.groovy.control.ErrorCollector.addFatalError(ErrorCollector.java:148)\n    org.apache.groovy.parser.antlr4.AstBuilder.collectSyntaxError(AstBuilder.java:4792)\n    org.apache.groovy.parser.antlr4.AstBuilder.createParsingFailedException(AstBuilder.java:4775)\n    org.apache.groovy.parser.antlr4.AstBuilder.convertException(AstBuilder.java:254)\n    org.apache.groovy.parser.antlr4.AstBuilder.buildCST(AstBuilder.java:228)\n    org.apache.groovy.parser.antlr4.AstBuilder.buildAST(AstBuilder.java:262)\n    org.apache.groovy.parser.antlr4.Antlr4ParserPlugin.buildAST(Antlr4ParserPlugin.java:58)\n    org.codehaus.groovy.control.SourceUnit.buildAST(SourceUnit.java:255)\n    java.base/java.util.Iterator.forEachRemaining(Unknown Source)\n    java.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Unknown Source)\n    java.base/java.util.stream.ReferencePipeline$Head.forEach(Unknown Source)\n    org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:663)\n    groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:373)\n    groovy.lang.GroovyClassLoader.lambda$parseClass$2(GroovyClassLoader.java:316)\n    org.codehaus.groovy.runtime.memoize.StampedCommonCache.compute(StampedCommonCache.java:163)\n    org.codehaus.groovy.runtime.memoize.StampedCommonCache.getAndPut(StampedCommonCache.java:154)\n    groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:314)\n    groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:298)\n    groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:258)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getScriptClass(GroovyScriptEngineImpl.java:350)\n    org.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:159)\n    qupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)\n    qupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)\n    qupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)\n    java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\n    java.base/java.util.concurrent.FutureTask.run(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n    java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n    java.base/java.lang.Thread.run(Unknown Source)\n\nERROR: \nFor help interpreting this error, please search the forum at https://forum.image.sc/tag/qupath\nYou can also start a new discussion there, including both your script &amp; the messages in this log.\n</code></pre>", "<p>That doesn\u2019t look like a  working macro. A message dialog shouldn\u2019t be equal to anything, I don\u2019t think it returns anything. Not sure if that macro was intended to work?</p>\n<p>Message Dialogs also take two strings, but nothing was provided.</p>\n<p>The following works</p>\n<pre><code class=\"lang-auto\">Dialogs.showMessageDialog('hi', 'Nope')\ndef pathModel = Dialogs.promptForFile(null)\ndef pathModel3 = pathModel.toString()\n</code></pre>", "<p>Hello,<br>\nThank you for your reply.<br>\nSo I should paste the above in the stardust box when this pops up for 0.5um segmentation?</p>", "<p>Haven\u2019t used it, but I\u2019m guessing they want the .pb file.</p>", "<aside class=\"quote no-group\" data-username=\"sdj7007\" data-post=\"5\" data-topic=\"75014\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/45deac/40.png\" class=\"avatar\"> Shweta:</div>\n<blockquote>\n<p>import \u201cC:\\Users\\PerkinElmer\\Desktop\\SDJ_NEW\\scripts supporting open-source tools for PhenoCycler image analysis\\stardist_cell_seg_model.pb\u201d</p>\n</blockquote>\n</aside>\n<p>You can\u2019t import a string, that is for libraries, generally. See other qupath posts with import statements.<br>\n<a href=\"https://forum.image.sc/search?q=qupath%20import%20script\">https://forum.image.sc/search?q=qupath%20import%20script</a></p>\n<p>That line doesn\u2019t seem to be in the original script either. If you want to target the pb file, they have a comment for that, though it saves the pb model to \u2018pathModel\u2019 rather than \u2018pathModel3\u2019, which seems to be a mistake.<br>\nYou may want to look at the documentation for StarDist for examples of working StarDist scripts.<br>\n<a href=\"https://qupath.readthedocs.io/en/0.3/docs/advanced/stardist.html\" class=\"onebox\" target=\"_blank\" rel=\"noopener\">https://qupath.readthedocs.io/en/0.3/docs/advanced/stardist.html</a></p>", "<aside class=\"quote no-group\" data-username=\"sdj7007\" data-post=\"1\" data-topic=\"75014\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/s/45deac/40.png\" class=\"avatar\"> Shweta:</div>\n<blockquote>\n<p>BUT I AM GETTING A LOT OF ERRORS. WHY?<br>\nCAN SOMEONE PLEASE HELP ME.</p>\n</blockquote>\n</aside>\n<p>You need to have your model path be a string. Here, the backslashes are treated as an escape character:</p>\n<pre><code class=\"lang-auto\">def pathModel2 = Dialogs.showMessageDialog(C:\\Users\\PerkinElmer\\Desktop\\scripts supporting open-source tools for PhenoCycler image analysis)\n</code></pre>\n<p>Instead, try:</p>\n<pre><code class=\"lang-auto\">def pathModel2 = Dialogs.showMessageDialog('C:\\Users\\PerkinElmer\\Desktop\\scripts supporting open-source tools for PhenoCycler image analysis')\n</code></pre>\n<p>Also I\u2019m like 99% sure this workflow was made by Grady Carlson back when he was at Akoya (I think he\u2019s in Merck now). You can email <a href=\"mailto:support@akoyabio.com\">support@akoyabio.com</a> or message Grady on <a href=\"https://www.linkedin.com/in/grady-carlson-ph-d-22449636/\">Linkedin</a> as they would be the best ones to answer any questions pertaining to their scripts. Also looks like they replicated some of <a class=\"mention\" href=\"/u/mike_nelson\">@Mike_Nelson</a> 's <a href=\"https://forum.image.sc/t/there-and-back-again-qupath-cytomap-cluster-analysis/43352\">CytoMAP and back</a> near the end, so if you have any issues in that portion of the workflow, he might be able to provide some insight</p>", "<aside class=\"quote no-group\" data-username=\"Mark_Zaidi\" data-post=\"7\" data-topic=\"75014\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mark_zaidi/40/37001_2.png\" class=\"avatar\"> Mark Zaidi:</div>\n<blockquote>\n<p><code>def pathModel2 = Dialogs.showMessageDialog('C:\\Users\\PerkinElmer\\Desktop\\scripts supporting open-source tools for PhenoCycler image analysis')</code></p>\n</blockquote>\n</aside>\n<p>Yeah, it still doesn\u2019t work, I tried that. The function requires 2 arguments, so it would need to be like\u2026</p>\n<pre><code class=\"lang-auto\">def pathModel2 = Dialogs.showMessageDialog('C:/Users/PerkinElmer/Desktop/scripts supporting open-source tools for PhenoCycler image analysis', 'test')\n</code></pre>\n<p>And doesn\u2019t really help much since it doesn\u2019t open a file/folder, it\u2019s just a message.</p>", "<p>Looking at <a href=\"https://akoyabio-my.sharepoint.com/personal/gcarlson_akoyabio_com/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fgcarlson%5Fakoyabio%5Fcom%2FDocuments%2Fscripts%20supporting%20open%2Dsource%20tools%20for%20PhenoCycler%20image%20analysis&amp;ga=1\">Grady\u2019s scripts in his dropbox</a>, the <code>pathmodel2</code> variable is a bit misleading. It\u2019s just a message like you mentioned. Here\u2019s the original script for <code>StarDist cell segmentation script_0.25_um_per_pixel.groovy</code>:</p>\n<pre><code class=\"lang-auto\">import qupath.lib.gui.dialogs.Dialogs\ndef pathModel2 = Dialogs.showMessageDialog(\"Please select your stardist segmentation file\",\"Please select your StarDist cell segmentation file\")\ndef pathModel = Dialogs.promptForFile(null)\ndef pathModel3 = pathModel.toString()\n//def pathModel = Dialogs.showInputDialog(\"My title\", \"Please paste the StarDist file path without quotes\", 'text')\nimport qupath.ext.stardist.StarDist2D\n\n// if you are always using the same StarDist model you can set the path below instead of selecting via dialog box\n//def pathModel = \"C:\\\\Users\\\\gcarlson\\\\OneDrive - Akoya Biosciences\\\\Desktop\\\\CODEX Analysis Demo\\\\CODEX_cell_seg.pb\"\n\ndef stardist = StarDist2D.builder(pathModel3)\n        .threshold(0.5)              // Probability (detection) threshold\n        .channels(0)            // Select detection channel\n        .normalizePercentiles(1, 99) // Percentile normalization\n        .pixelSize(0.25)              // Resolution for detection\n        .cellExpansion(5.0)          // Approximate cells based upon nucleus expansion\n        .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size\n        .measureShape()              // Add shape measurements\n        .measureIntensity()          // Add cell measurements (in all compartments)\n        .includeProbability(true)    // Add probability as a measurement (enables later filtering)\n        .build()\n\n// Run detection for the selected objects\ndef imageData = getCurrentImageData()\ndef pathObjects = getSelectedObjects()\nif (pathObjects.isEmpty()) {\n    Dialogs.showErrorMessage(\"StarDist\", \"Please select a parent object!\")\n    return\n}\nstardist.detectObjects(imageData, pathObjects)\nprintln 'Done!'\n</code></pre>\n<p>I think what <code>pathmodel2</code> and <code>pathmodel3</code> do is prompt the user to select a .pb-format StarDist model through a GUI. But if you know where the model is located, you should be able to comment out the pathModel and uncomment the last <code>pathmodel</code> with your directory.</p>\n<p><a class=\"mention\" href=\"/u/shweta\">@shweta</a> can you try the following script:</p>\n<pre><code class=\"lang-auto\">import qupath.lib.gui.dialogs.Dialogs\ndef pathModel2 = Dialogs.showMessageDialog(\"Please select your stardist segmentation file\",\"Please select your StarDist cell segmentation file\")\n//def pathModel = Dialogs.promptForFile(null)\n//def pathModel3 = pathModel.toString()\n//def pathModel = Dialogs.showInputDialog(\"My title\", \"Please paste the StarDist file path without quotes\", 'text')\nimport qupath.ext.stardist.StarDist2D\n\n// if you are always using the same StarDist model you can set the path below instead of selecting via dialog box\ndef pathModel = 'C:\\Users\\PerkinElmer\\Desktop\\scripts supporting open-source tools for PhenoCycler image analysis\\stardist_cell_seg_model.pb'\n\ndef stardist = StarDist2D.builder(pathModel3)\n        .threshold(0.5)              // Probability (detection) threshold\n        .channels(0)            // Select detection channel\n        .normalizePercentiles(1, 99) // Percentile normalization\n        .pixelSize(0.25)              // Resolution for detection\n        .cellExpansion(5.0)          // Approximate cells based upon nucleus expansion\n        .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size\n        .measureShape()              // Add shape measurements\n        .measureIntensity()          // Add cell measurements (in all compartments)\n        .includeProbability(true)    // Add probability as a measurement (enables later filtering)\n        .build()\n\n// Run detection for the selected objects\ndef imageData = getCurrentImageData()\ndef pathObjects = getSelectedObjects()\nif (pathObjects.isEmpty()) {\n    Dialogs.showErrorMessage(\"StarDist\", \"Please select a parent object!\")\n    return\n}\nstardist.detectObjects(imageData, pathObjects)\nprintln 'Done!'\n</code></pre>\n<p>I\u2019m assuming your model is located in</p>\n<pre><code class=\"lang-auto\">def pathModel = 'C:\\Users\\PerkinElmer\\Desktop\\scripts supporting open-source tools for PhenoCycler image analysis\\stardist_cell_seg_model.pb'\n</code></pre>\n<p>If not, update the path accordingly.</p>", "<p>Hello,<br>\nThank you so much for your responses, Its still not working.</p>\n<pre><code class=\"lang-auto\">import qupath.lib.gui.dialogs.Dialogs\n\ndef pathModel2 = Dialogs.showMessageDialog(\"Please select your stardist segmentation file\",\"Please select your StarDist cell segmentation file\")\n\n//def pathModel = Dialogs.promptForFile(null)\n\n//def pathModel3 = pathModel.toString()\n\n//def pathModel = Dialogs.showInputDialog(\"My title\", \"Please paste the StarDist file path without quotes\", 'text')\n\nimport qupath.ext.stardist.StarDist2D\n\n// if you are always using the same StarDist model you can set the path below instead of selecting via dialog box\ndef pathModel = 'C:\\Users\\PerkinElmer\\Desktop\\scripts_supporting_open_source_tools_for_PhenoCycler_image_analysis\\stardist_cell_seg_model.pb'\n\ndef stardist = StarDist2D.builder(pathModel3)\n\n.threshold(0.5) // Probability (detection) threshold\n\n.channels(0) // Select detection channel\n\n.normalizePercentiles(1, 99) // Percentile normalization\n\n.pixelSize(0.25) // Resolution for detection\n\n.cellExpansion(5.0) // Approximate cells based upon nucleus expansion\n\n.cellConstrainScale(1.5) // Constrain cell expansion using nucleus size\n\n.measureShape() // Add shape measurements\n\n.measureIntensity() // Add cell measurements (in all compartments)\n\n.includeProbability(true) // Add probability as a measurement (enables later filtering)\n\n.build()\n\n// Run detection for the selected objects\n\ndef imageData = getCurrentImageData()\n\ndef pathObjects = getSelectedObjects()\n\nif (pathObjects.isEmpty()) {\n\nDialogs.showErrorMessage(\"StarDist\", \"Please select a parent object!\")\n\nreturn\n\n}\n\nstardist.detectObjects(imageData, pathObjects)\n\nprintln 'Done!'\n\nERROR: startup failed:\nStarDist cell segmentation script_0.5_um_per_pixel.groovy: 17: Unexpected character: '\\'' @ line 16, column 17.\ndef pathModel = 'C:\\Users\\PerkinElmer\\Desktop\\scripts_supporting_open_source_tools_for_PhenoCycler_image_analysis\\stardist_cell_seg_model.pb'\n^\n\n1 error\nin StarDist cell segmentation script_0.5_um_per_pixel.groovy at line number 16\n\nERROR: org.codehaus.groovy.control.ErrorCollector.failIfErrors(ErrorCollector.java:292)\norg.codehaus.groovy.control.ErrorCollector.addFatalError(ErrorCollector.java:148)\norg.apache.groovy.parser.antlr4.AstBuilder.collectSyntaxError(AstBuilder.java:4792)\norg.apache.groovy.parser.antlr4.AstBuilder.createParsingFailedException(AstBuilder.java:4775)\norg.apache.groovy.parser.antlr4.AstBuilder.convertException(AstBuilder.java:254)\norg.apache.groovy.parser.antlr4.AstBuilder.buildCST(AstBuilder.java:228)\norg.apache.groovy.parser.antlr4.AstBuilder.buildAST(AstBuilder.java:262)\norg.apache.groovy.parser.antlr4.Antlr4ParserPlugin.buildAST(Antlr4ParserPlugin.java:58)\norg.codehaus.groovy.control.SourceUnit.buildAST(SourceUnit.java:255)\njava.base/java.util.Iterator.forEachRemaining(Unknown Source)\njava.base/java.util.Spliterators$IteratorSpliterator.forEachRemaining(Unknown Source)\njava.base/java.util.stream.ReferencePipeline$Head.forEach(Unknown Source)\norg.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:663)\ngroovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:373)\ngroovy.lang.GroovyClassLoader.lambda$parseClass$2(GroovyClassLoader.java:316)\norg.codehaus.groovy.runtime.memoize.StampedCommonCache.compute(StampedCommonCache.java:163)\norg.codehaus.groovy.runtime.memoize.StampedCommonCache.getAndPut(StampedCommonCache.java:154)\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:314)\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:298)\ngroovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:258)\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.getScriptClass(GroovyScriptEngineImpl.java:350)\norg.codehaus.groovy.jsr223.GroovyScriptEngineImpl.eval(GroovyScriptEngineImpl.java:159)\nqupath.lib.gui.scripting.languages.DefaultScriptLanguage.execute(DefaultScriptLanguage.java:233)\nqupath.lib.gui.scripting.DefaultScriptEditor.executeScript(DefaultScriptEditor.java:1113)\nqupath.lib.gui.scripting.DefaultScriptEditor$3.run(DefaultScriptEditor.java:1478)\njava.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)\njava.base/java.util.concurrent.FutureTask.run(Unknown Source)\njava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\njava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\njava.base/java.lang.Thread.run(Unknown Source)\n\nERROR:\nFor help interpreting this error, please search the forum at [https://forum.image.sc/tag/qupath](https://forum.image.sc/tag/qupath)\nYou can also start a new discussion there, including both your script &amp; the messages in this log.\n</code></pre>", "<p>Hello,</p>\n<p>Thank you, Thank you both for your replies and prompt help. I am truly grateful.<br>\nI am not too sure how it worked but it finally worked.<br>\nI did do some changes,</p>\n<ol>\n<li>In the path where QuPath was located, I just removed all scripts and stored it together instead of another folder.</li>\n</ol>\n<p>Then, when I ran it according to the pdf, this was the script:</p>\n<pre><code class=\"lang-auto\">import qupath.lib.gui.dialogs.Dialogs\ndef pathModel2 = Dialogs.showMessageDialog(\"Please select your stardist segmentation file\",\"Please select your StarDist cell segmentation file\")\ndef pathModel = Dialogs.promptForFile(null)\ndef pathModel3 = pathModel.toString()\n//def pathModel = Dialogs.showInputDialog(\"My title\", \"Please paste the StarDist file path without quotes\", 'text')\nimport qupath.ext.stardist.StarDist2D\n\n// if you are always using the same StarDist model you can set the path below instead of selecting via dialog box\n//def pathModel = \"C:\\\\Users\\\\gcarlson\\\\OneDrive - Akoya Biosciences\\\\Desktop\\\\CODEX Analysis Demo\\\\CODEX_cell_seg.pb\"\n\ndef stardist = StarDist2D.builder(pathModel3)\n        .threshold(0.5)              // Probability (detection) threshold\n        .channels(0)            // Select detection channel\n        .normalizePercentiles(1, 99) // Percentile normalization\n        .pixelSize(0.5)              // Resolution for detection\n        .cellExpansion(5.0)          // Approximate cells based upon nucleus expansion\n        .cellConstrainScale(1.5)     // Constrain cell expansion using nucleus size\n        .measureShape()              // Add shape measurements\n        .measureIntensity()          // Add cell measurements (in all compartments)\n        .includeProbability(true)    // Add probability as a measurement (enables later filtering)\n        .build()\n\n// Run detection for the selected objects\ndef imageData = getCurrentImageData()\ndef pathObjects = getSelectedObjects()\nif (pathObjects.isEmpty()) {\n    Dialogs.showErrorMessage(\"StarDist\", \"Please select a parent object!\")\n    return\n}\nstardist.detectObjects(imageData, pathObjects)\nprintln 'Done!'\n</code></pre>", "<p>Ah, based on the links, the original version of the script was fine. You broke it at some point by modifying the showMessageDialog, resulting in the errors.</p>"], "75529": ["<p>Our Imaging Applications team is hiring again! We are searching for a Bioimage Analyst that will work with researchers on their image analysis needs and provide support on training and institutional-level projects. If you are proficient on any image analysis tool and can do a bit of coding, this job is for you! <a href=\"https://thejacksonlaboratory.wd1.myworkdayjobs.com/External_JAX/job/Farmington-Connecticut/Systems-Analyst-I_JR002913\" rel=\"noopener nofollow ugc\">Applications are open here. </a></p>\n<p>Job description follows below, but they don\u2019t really tell you WHY you\u2019d want to apply for this position. I\u2019ve also made a list of things that make this a really unique opportunity in my opinion:</p>\n<ol start=\"0\">\n<li>\n<p>Before anything: we are always particularly interested in applications from women, people of color, LGBTQIA+ people, and under-represented minorities in research and IT. It\u2019s important to us.</p>\n</li>\n<li>\n<p>The work environment: our team is in the Research IT department, which means that we get to interact with researchers one-to-one very often, stay very close to the research, and work in things that are wider than just individual research projects.</p>\n</li>\n<li>\n<p>Our team: We are a team of four (or will be, when you join us <img src=\"https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\">) doing everything imaging data at JAX: data management, help with image analysis, training. This position will probably be more focused on the data analysis side of things, but everyone does a bit of everything.<br>\nThings we love: open-source, the bioimaging community, flexibility, work-life balance, honesty, going outside. Things we don\u2019t love: data without metadata, overly rigid processes, closed software ecosystems.<br>\nI manage the group but <a class=\"mention\" href=\"/u/mellertd\">@mellertd</a> oversees us from above, and I cannot say enough good things about him as a manager. He shields our team from bullshit in general, gives us space to do the things we enjoy, and takes work-life balance extremely seriously.</p>\n</li>\n<li>\n<p>Funding and resources: this is a \u201chard money\u201d position. No clock ticking, no \u201cneed to get another grant\u201d, nothing of that. Still not a lot of those in the imaging data world. The resources we get, both in terms of budgets and hardware to operate what we need and experiment, are incredible. And, in general, we are also given the time and the patience to try new things and fail.</p>\n</li>\n<li>\n<p>Salary and benefits: can\u2019t give specific numbers, but JAX benefits are great (for American standards, at least) and, if you\u2019re a postdoc, this might be a SIGNIFICANT salary bump. We\u2019re paid like IT, not research. (if you want numbers, I can do that in private. DM me.)</p>\n</li>\n<li>\n<p>Locations: this time we\u2019re recruiting for someone who will be based at our Farmington, CT campus. You are welcome to go into the office as often or as seldom as you\u2019d like (I rarely ever go in), and remote from anywhere in the US is a possibility depending on circumstances. You\u2019ll also travel to our Bar Harbor, ME campus on occasion to meet with researchers and collaborators, and this is a massive perk of the job in my opinion. Bar Harbor is gorgeous and there\u2019s always some time to enjoy Acadia National Park when you\u2019re there.</p>\n</li>\n</ol>", "<p>Are you a scientist with skills in bioimage analysis who is looking for a nontraditional research career? Are you excited by the opportunity to provide computational resources and support to world-class faculty, who perform leading-edge research in the fields of human genomics and mouse genetics? Come join us in Research IT at The Jackson Laboratory (<a href=\"http://www.jax.org\" rel=\"noopener nofollow ugc\">http://www.jax.org</a>), an independent, nonprofit biomedical research institution internationally recognized for its research excellence. At JAX, our Information Technology is as exciting and progressive as our cutting-edge science. In Research IT, you will help to plan, design, and launch efficient systems for bioimage analysis in support of scientific research.</p>\n<p>The ideal candidate will have a deep understanding of scientific image data analysis and processing. Preference will be given to candidates with hands-on experience in automated image analysis pipelines, High-Performance Computing (HPC) environments, laboratory information systems, and working collaboratively with researchers and Information Technology.</p>\n<p>The option to work remotely may be considered for this role on a case-by-case basis.</p>\n<p>Responsibilities</p>\n<ul>\n<li>Research and evaluate new tools for applicability to in-house needs. Stay current with new trends and emerging information technologies and informatics tools; advocate for and lead the adoption of these technologies at JAX.</li>\n<li>Develop appropriate computational strategies to acquire, store, process, and analyze imaging data.</li>\n<li>Develop and deliver training programs, tutorials, and workshops for scientists on the optimal use of IT resources in their image analysis projects.</li>\n<li>Assist in preparation for submitting grant proposals.</li>\n<li>Investigate application issues and document findings. Communicate with scientists to answer questions and troubleshoot problems; provide guidance, support, and advice on Research IT systems.</li>\n<li>Based on the scientific needs of faculty and research staff, identify and recommend innovative solutions and integrated proposals for server, storage, and network systems; including hardware and software acquisition, installation and administration.</li>\n<li>Install and update scientific applications in the Research IT environment, including but not limited to relevant HPC clusters.</li>\n<li>Provide technical support internally and to external vendors, including problem resolution for complex systems and storage solutions.</li>\n<li>Benchmark application and workflow performance and determine areas for improved efficiencies.</li>\n</ul>\n<p>Qualifications</p>\n<p>Level I</p>\n<ul>\n<li>Bachelor\u2019s degree with a minimum of five (5) years of experience, Master\u2019s degree with a minimum of three (3) years of experience, or a Ph.D. with zero (0) to two (2) years of experience, in a relevant area of data analysis or management is required</li>\n</ul>\n<p>All Levels</p>\n<ul>\n<li>A background in (or familiarity with) biology and/or microscopy is preferred.</li>\n<li>Proficient in tool-agnostic image analysis and general-purpose programming</li>\n<li>Experience with Scikit-image, ImageJ/Fiji, CellProfiler, QuPath, OpenCV or other relevant image analysis tools.</li>\n<li>Familiar with Cloud computing, Machine Learning, and Artificial Intelligence.</li>\n<li>Experience with engaging with scientists and open-source communities and operating in a highly interdisciplinary environment</li>\n<li>Experience with OMERO and/or other storage of large multidimensional arrays (e.g., zarr, N5, HDF5) is preferred.</li>\n<li>Experience with software development best practices (e.g., git, CI/CD, documentation) is preferred.</li>\n<li>Experience with medical imaging (e.g., DICOM, MRI, CT), remote sensing, or satellite imaging is not necessary but will be considered a strength</li>\n</ul>\n<p>About JAX:</p>\n<p>The Jackson Laboratory (JAX) is an independent, nonprofit biomedical research institution with more than 2,400 employees. Headquartered in Bar Harbor, Maine, it has a National Cancer Institute-designated Cancer Center in Augusta, Maine, a genomic medicine institute in Farmington, Connecticut, and facilities in Ellsworth, Maine, Sacramento, California, and Shanghai, China. Its mission is to discover precise genomic solutions for disease and empower the global biomedical community in the shared quest to improve human health.</p>\n<p>JAX employees work in a collaborative, value-driven, and team-based environment where the focus is on advancing science and improving patients\u2019 lives. Researchers apply genetics to increase the understanding of human disease and advance treatments and cures for cancer, neurological and immune disorders, diabetes, aging, and heart disease. JAX was voted among the top 15 \u201cBest Places to Work in Academia\u201d in the United States in a poll conducted by The Scientist magazine!</p>", "<p>We are still recruiting for this position - happy to answer any questions about it! This is a position that comes with a lot of freedom to establish your own priorities and define best practices and policies for bioimage analysis at JAX. I personally think it\u2019s a very exciting position, but of course I am biased <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>Hi, I was wondering if the application window is still open.<br>\nThanks!</p>", "<p>Thank you for your interest! Unfortunately applications are closed - we have just made a job offer to a candidate last week.</p>"], "78089": ["<p>Hallo,<br>\nI\u2019m a fresher in this field. Recently, we use Fluorescence Microscope IX73 to post, using its black and white chip. But its illumination Correction is hard to perform. The reason I think is its Intensity of Background are similar in a large block size. The parameter is Background-Each-block size50-median-smooth300-substract.<br>\nThese Images are part of my experiment, with Dapi and Oligo-dT FISH. We need to quanlitate the intensity of Cells(Oligo-dT FISH).<br>\nPlease help me to correct the illumination TaT thx!<br>\n<a class=\"attachment\" href=\"/uploads/short-url/9zb5HSOTR8vzJuXD8s9qcn0hJQk.rar\">2023.2.22.rar</a> (5.6 MB)</p>"], "79628": ["<p>I am trying to extract actin data from cell images. So far, I am able to find area covered by cytoplasm of cell, but finding the area covered by actin filaments (or the number of actin filaments) is proving difficult.</p>\n<p>I used IdentifyPrimaryObjects to get the individual nuclei and IdentifySecondaryObjects to find area occupied by the cell cytoplasm:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df5cf49f89c5c04f9ec18d40d1bfd43f225e13bc.png\" data-download-href=\"/uploads/short-url/vRXDctrLWlyxUciy5VLfKPygfpO.png?dl=1\" title=\"Screenshot 2023-04-07 012249\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df5cf49f89c5c04f9ec18d40d1bfd43f225e13bc_2_571x500.png\" alt=\"Screenshot 2023-04-07 012249\" data-base62-sha1=\"vRXDctrLWlyxUciy5VLfKPygfpO\" width=\"571\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df5cf49f89c5c04f9ec18d40d1bfd43f225e13bc_2_571x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df5cf49f89c5c04f9ec18d40d1bfd43f225e13bc_2_856x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df5cf49f89c5c04f9ec18d40d1bfd43f225e13bc.png 2x\" data-dominant-color=\"B8B9B8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-04-07 012249</span><span class=\"informations\">946\u00d7827 191 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>However, I cannot find a way to quantify actin filaments. The best I can do is use EnhanceOrSuppressFeatures to get a clearer view of the actin filaments:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06b86fa126576c5b0351b4dddf1b598cfe11c2fa.png\" data-download-href=\"/uploads/short-url/Xs14hx2ESijyKtEEa3gGU0CBNU.png?dl=1\" title=\"Screenshot 2023-04-07 012650\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/06b86fa126576c5b0351b4dddf1b598cfe11c2fa_2_690x309.png\" alt=\"Screenshot 2023-04-07 012650\" data-base62-sha1=\"Xs14hx2ESijyKtEEa3gGU0CBNU\" width=\"690\" height=\"309\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/6/06b86fa126576c5b0351b4dddf1b598cfe11c2fa_2_690x309.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06b86fa126576c5b0351b4dddf1b598cfe11c2fa.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06b86fa126576c5b0351b4dddf1b598cfe11c2fa.png 2x\" data-dominant-color=\"ECECEC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-04-07 012650</span><span class=\"informations\">783\u00d7351 52 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a0c5cf7e5d6e7600216667e9c835b0b0ca0e412a.png\" data-download-href=\"/uploads/short-url/mWgcxSaItFNE8Sry8tNmt4VfTPs.png?dl=1\" title=\"Screenshot 2023-04-07 012608\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/0/a0c5cf7e5d6e7600216667e9c835b0b0ca0e412a_2_690x324.png\" alt=\"Screenshot 2023-04-07 012608\" data-base62-sha1=\"mWgcxSaItFNE8Sry8tNmt4VfTPs\" width=\"690\" height=\"324\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/0/a0c5cf7e5d6e7600216667e9c835b0b0ca0e412a_2_690x324.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a0c5cf7e5d6e7600216667e9c835b0b0ca0e412a.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a0c5cf7e5d6e7600216667e9c835b0b0ca0e412a.png 2x\" data-dominant-color=\"686868\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-04-07 012608</span><span class=\"informations\">871\u00d7410 154 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I was hoping there could be something I could do to quantify the actin filaments. Any help would be appreciated. Thank you.</p>"], "74509": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p><a class=\"attachment\" href=\"/uploads/short-url/OMAKsBwS1tzOpOY6GQ7ci10uSo.tif\">220527.tif</a> (5.0 MB)</p>\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<p>This is cell staining stained with 2 antibodies for specific proteins, DAPI and phalloidin.<br>\nWe took several images of the slides to have up 100 cells for analysis and also have z stacks for each image.</p>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<p>I want to know first, if I should set the contrast similar to all images. then how to analyse co localization of the 2 proteins and changes in fluorescence intensity.</p>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<p>-New to fiji and cell profiler<br>\n-what are the initial steps<br>\n-how to generate a macro to analyse multiple images faster.</p>", "<aside class=\"quote no-group\" data-username=\"fc2022\" data-post=\"1\" data-topic=\"74509\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/fc2022/40/63590_2.png\" class=\"avatar\"> Fiorella:</div>\n<blockquote>\n<p>if I should set the contrast similar to all images. then how to analyse co localization of the 2 proteins and changes in fluorescence intensity.</p>\n</blockquote>\n</aside>\n<p>Assuming your acquisition settings are the same across your images, there generally should not be a need to \u2018set\u2019 contrast.</p>\n<p>For your second question, it depends a bit on what your ultimate goal is and what the underlying hypothesis is. If you expect areas that have more DAPI to also have more phalloidin, you might want to look at simple pearson correlation. However, I think you\u2019ll likely want to look at <strong>segmenting</strong> your nuclei first, then looking at overlap between the two proteins and intensity <em>within your nuclei</em>. One of our <a href=\"https://cellprofiler.org/examples\">examples</a> is colocalization. I\u2019d suggest you check it out!</p>", "<aside class=\"quote no-group\" data-username=\"Rebecca_Senft\" data-post=\"2\" data-topic=\"74509\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/rebecca_senft/40/48617_2.png\" class=\"avatar\"> Rebecca Senft:</div>\n<blockquote>\n<p>Assuming your acquisition settings are the same across your images</p>\n</blockquote>\n</aside>\n<p>+1</p>\n<p>Also, in case it helps frame the problem, \u201chow to analyze colocalization\u201d is a whole thing in and of itself. There isn\u2019t really a single type of \u201cI want to measure the colocalization\u201d experiment. As <a class=\"mention\" href=\"/u/rebecca_senft\">@Rebecca_Senft</a> mentions, there isn\u2019t one value to measure either. After segmentation you might try intersect over union when comparing two collections of cells, but even that can be biased.</p><div class=\"youtube-onebox lazy-video-container\" data-video-id=\"P2JvFe0hB_M\" data-video-title=\"Deconstructing co-localisation workflows: A journey into the black boxes [NEUBIAS Academy@Home]\" data-provider-name=\"youtube\">\n  <a href=\"https://www.youtube.com/watch?v=P2JvFe0hB_M\" target=\"_blank\" rel=\"noopener\">\n    <img class=\"youtube-thumbnail\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/e/aef6144bd03654bd8a68e0c25aede7479bd15834.jpeg\" title=\"Deconstructing co-localisation workflows: A journey into the black boxes [NEUBIAS Academy@Home]\" width=\"690\" height=\"388\">\n  </a>\n</div>\n\n<p>Great post here as well <a href=\"https://forum.image.sc/t/imagej-coloc-2-with-tissue-sections/57343/6\" class=\"inline-onebox\">ImageJ Coloc 2 with tissue sections - #6 by biovoxxel</a></p>", "<p>Hi<br>\nAll my images are  acquired using the same settings, but they look dark in imageJ as compared when I acquired them in the microscope.<br>\nI want to check for changes in fluorescence intensity in control vs treated cells with a drug. My images have 4 channels (DAPI, Protein 1 , protein 2 (this in the one of intereste for changes), and phalloidin) and are in lsm format. I have single photos and z stacks too.<br>\nthen I would like to look at co localization of protein 1 and 2<br>\n<a class=\"attachment\" href=\"/uploads/short-url/AqliLhHGP5JVtSU7Dm4Dhlw9rf4.tif\">021222_FKH1 crRNA-A 2.1exp 291122 2.tif</a> (1.0 MB)</p>", "<aside class=\"quote no-group\" data-username=\"fc2022\" data-post=\"4\" data-topic=\"74509\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/fc2022/40/63590_2.png\" class=\"avatar\"> Fiorella:</div>\n<blockquote>\n<p>but they look dark in imageJ as compared when I acquired them in the microscope.</p>\n</blockquote>\n</aside>\n<p>This doesn\u2019t matter. Brightness and contrast settings change how you see the image, not the underlying data.</p>\n<p>If you are performing a complete analysis, you are better off sticking to one type of image. Alternatively, collect enough data that you can split into two analyses. 3D with most microscopes, like confocals, results in isotropic images. Lots of stretching on the Z axis. And depending on the objective type and what is being imaged, a lot of people don\u2019t adapt their Z spacing correctly. If you increment the Z spacing at a regular interval and there is a refractive index change, like air into something else, the Z spacing between slices you image changes the deeper you get into the samples.</p><aside class=\"onebox pdf\" data-onebox-src=\"https://dmg5c1valy4me.cloudfront.net/wp-content/uploads/2020/08/03114610/s41596-020-0360-2-1.pdf\">\n  <header class=\"source\">\n\n      <a href=\"https://dmg5c1valy4me.cloudfront.net/wp-content/uploads/2020/08/03114610/s41596-020-0360-2-1.pdf\" target=\"_blank\" rel=\"noopener\">dmg5c1valy4me.cloudfront.net</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <a href=\"https://dmg5c1valy4me.cloudfront.net/wp-content/uploads/2020/08/03114610/s41596-020-0360-2-1.pdf\" target=\"_blank\" rel=\"noopener\"><span class=\"pdf-onebox-logo\"></span></a>\n\n<h3><a href=\"https://dmg5c1valy4me.cloudfront.net/wp-content/uploads/2020/08/03114610/s41596-020-0360-2-1.pdf\" target=\"_blank\" rel=\"noopener\">s41596-020-0360-2-1.pdf</a></h3>\n\n  <p class=\"filesize\">2.45 MB</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nAlso a good comment on colocalization in the box in</p><aside class=\"onebox pdf\" data-onebox-src=\"https://scian.cl/scientific-image-analysis/wp-content/uploads/2021/10/Jonkman-et-al-2020_Tutorial-guidance-for-quantitative-confocal-microscopy.pdf\">\n  <header class=\"source\">\n\n      <a href=\"https://scian.cl/scientific-image-analysis/wp-content/uploads/2021/10/Jonkman-et-al-2020_Tutorial-guidance-for-quantitative-confocal-microscopy.pdf\" target=\"_blank\" rel=\"noopener\">scian.cl</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <a href=\"https://scian.cl/scientific-image-analysis/wp-content/uploads/2021/10/Jonkman-et-al-2020_Tutorial-guidance-for-quantitative-confocal-microscopy.pdf\" target=\"_blank\" rel=\"noopener\"><span class=\"pdf-onebox-logo\"></span></a>\n\n<h3><a href=\"https://scian.cl/scientific-image-analysis/wp-content/uploads/2021/10/Jonkman-et-al-2020_Tutorial-guidance-for-quantitative-confocal-microscopy.pdf\" target=\"_blank\" rel=\"noopener\">Jonkman-et-al-2020_Tutorial-guidance-for-quantitative-confocal-microscopy.pdf</a></h3>\n\n  <p class=\"filesize\">5.47 MB</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>As <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> mentions, the images <em>appearing</em> dark doesn\u2019t matter. In general you likely want to approach your analysis somewhat like this:</p>\n<ol>\n<li>Identify your cells (ideally NOT with the stain you think is changing)</li>\n<li>Measure intensity in your identified objects</li>\n<li>Examine how these intensities relate to each other; in CellProfiler you can use MeasureColocalization to perform pixel-wise colocalization measurements)</li>\n</ol>\n<p>An important caveat: intensity measurements are really easy to perform, but tricky to get right. Be sure you\u2019re on the lookout for saturation (means you can\u2019t use intensity-based measurements because your detector is maxed out) and be aware of chromatic shift in x,y and z. This matters more if your objects you\u2019re looking at are very small.</p>"], "77582": ["<p>Hi,</p>\n<p>I have StarDist module installed and somehow it gives error while running. It runs ok on the same images in test mode. Any help/suggestion would be greatly appreciated. Below are errors.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/c/3cbad52ab7311d488c48185cee45274d0673a2ad.png\" data-download-href=\"/uploads/short-url/8FeVZH1sIbsxxXHZWvC3joWuxWJ.png?dl=1\" title=\"badObjectHearder\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/c/3cbad52ab7311d488c48185cee45274d0673a2ad_2_690x276.png\" alt=\"badObjectHearder\" data-base62-sha1=\"8FeVZH1sIbsxxXHZWvC3joWuxWJ\" width=\"690\" height=\"276\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/c/3cbad52ab7311d488c48185cee45274d0673a2ad_2_690x276.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/c/3cbad52ab7311d488c48185cee45274d0673a2ad.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/c/3cbad52ab7311d488c48185cee45274d0673a2ad.png 2x\" data-dominant-color=\"F3F3F3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">badObjectHearder</span><span class=\"informations\">857\u00d7344 52.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/5/156ca3d7ad4077d307cfc6a2b13f8a661c36f00c.png\" data-download-href=\"/uploads/short-url/33wMNkvYFI2fVjhCKNnsu1XkwWg.png?dl=1\" title=\"2023-02-21 17_56_15-Message\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/156ca3d7ad4077d307cfc6a2b13f8a661c36f00c_2_690x263.png\" alt=\"2023-02-21 17_56_15-Message\" data-base62-sha1=\"33wMNkvYFI2fVjhCKNnsu1XkwWg\" width=\"690\" height=\"263\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/156ca3d7ad4077d307cfc6a2b13f8a661c36f00c_2_690x263.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/5/156ca3d7ad4077d307cfc6a2b13f8a661c36f00c.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/5/156ca3d7ad4077d307cfc6a2b13f8a661c36f00c.png 2x\" data-dominant-color=\"F4F4F3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">2023-02-21 17_56_15-Message</span><span class=\"informations\">980\u00d7374 54.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ee35b570774c47d31f775e11c1294837e6c6681.png\" data-download-href=\"/uploads/short-url/4pfqTxC6QaZ2p2fhcsyPZLwbET7.png?dl=1\" title=\"2023-02-21 17_56_50-Message\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ee35b570774c47d31f775e11c1294837e6c6681_2_690x262.png\" alt=\"2023-02-21 17_56_50-Message\" data-base62-sha1=\"4pfqTxC6QaZ2p2fhcsyPZLwbET7\" width=\"690\" height=\"262\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1ee35b570774c47d31f775e11c1294837e6c6681_2_690x262.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ee35b570774c47d31f775e11c1294837e6c6681.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1ee35b570774c47d31f775e11c1294837e6c6681.png 2x\" data-dominant-color=\"F3F3F3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">2023-02-21 17_56_50-Message</span><span class=\"informations\">984\u00d7374 52.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>When I clicked continue, the it showed the following ones. For a small dataset, I kept rerunning the analysis a few times, the same data set, eventually I won\u2019t get any error. I can\u2019t do the same for large data set.</p>\n<p>Below is my pip freeze</p>\n<pre><code class=\"lang-auto\">absl-py==1.4.0\nasciitree==0.3.3\nastunparse==1.6.3\nboto3==1.26.59\nbotocore==1.29.59\ncachetools==5.3.0\ncellpose==1.0.2\ncellpose-omni==0.7.3\nCellProfiler==4.2.5\ncellprofiler-core==4.2.5\ncentrosome==1.2.1\ncertifi==2022.12.7\ncharset-normalizer==3.0.1\nclang==5.0\ncolorama==0.4.6\ncolour==0.1.5\ncsbdeep==0.7.2\ncycler==0.11.0\ndarkdetect==0.8.0\ndeprecation==2.1.0\ndocutils==0.15.2\nedt==2.3.0\nentrypoints==0.4\nfasteners==0.18\nfastremap==1.13.3\nflatbuffers==1.12\nfsspec==2023.1.0\nfuture==0.18.3\ngast==0.4.0\ngoogle-api-core==2.11.0\ngoogle-auth==2.16.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-cloud-core==2.3.2\ngoogle-cloud-storage==2.7.0\ngoogle-crc32c==1.5.0\ngoogle-pasta==0.2.0\ngoogle-resumable-media==2.4.1\ngoogleapis-common-protos==1.58.0\ngrpcio==1.51.1\nh5py==3.6.0\nidna==3.4\nimagecodecs==2023.1.23\nimageio==2.25.0\nimagej==0.3.1\nimportlib-metadata==6.0.0\ninflect==6.0.2\njgo==1.0.5\nJinja2==3.1.2\njmespath==1.0.1\njoblib==1.2.0\nJPype1==1.4.1\nkeras==2.9.0\nKeras-Preprocessing==1.1.2\nkiwisolver==1.4.4\nlibclang==15.0.6.1\nllvmlite==0.39.1\nlxml==4.9.2\nmahotas==1.4.13\nMarkdown==3.4.1\nMarkupSafe==2.1.2\nmatplotlib==3.1.3\nmgen==1.2.0\nmysqlclient==1.4.6\nnatsort==8.2.0\nncolor==1.2.1\nnetworkx==3.0\nnumba==0.56.4\nnumcodecs==0.11.0\nnumpy==1.23.0\noauthlib==3.2.2\nomnipose==0.3.6\nomnipose-theme==1.1.2\nopencv-python-headless==4.7.0.68\nopt-einsum==3.3.0\npackaging==23.0\npandas==1.5.3\npbr==5.11.1\nPillow==9.4.0\npip==23.0\nprokaryote==2.4.4\nprotobuf==3.19.6\npsutil==5.9.4\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydantic==1.10.4\nPygments==2.14.0\npyjnius==1.4.1\npyparsing==3.0.9\nPyQt6==6.4.2\nPyQt6-Qt6==6.4.2\nPyQt6-sip==13.4.1\npyqtgraph==0.13.1\npython-bioformats==4.0.7\npython-dateutil==2.8.2\npython-javabridge==4.0.3\npytorch-ranger==0.1.1\npytz==2022.7.1\nPyWavelets==1.4.1\nPyYAML==6.0\npyzmq==22.3.0\nQtPy==2.3.0\nrequests==2.28.2\nrequests-oauthlib==1.3.1\nrsa==4.9\ns3transfer==0.6.0\nscikit-image==0.18.3\nscikit-learn==1.2.1\nscipy==1.9.0\nscyjava==1.8.1\nsentry-sdk==0.18.0\nsetuptools==67.0.0\nsix==1.15.0\nstardist==0.8.3\nsuperqt==0.4.1\ntensorboard==2.9.1\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.9.1\ntensorflow-estimator==2.9.0\ntensorflow-io-gcs-filesystem==0.30.0\ntermcolor==1.1.0\nthreadpoolctl==3.1.0\ntifffile==2022.4.8\ntorch==1.13.1\ntorch-optimizer==0.3.0\ntqdm==4.64.1\ntyping_extensions==4.4.0\nurllib3==1.26.14\nWerkzeug==2.2.2\nwheel==0.38.4\nwrapt==1.12.1\nwxPython==4.1.0\nzarr==2.13.6\nzipp==3.12.0\n</code></pre>\n<p>Thank you!<br>\nPearl</p>"], "76047": ["<p>Hello,<br>\nI am using the Classifier tool in CPA to differentiate actin cytoskeleton phenotypes following processing of my images in a CellProfiler (4.2.4) pipeline. The properties file opens fine in CPA, but when I attempt to Fetch objects, some appear as black images and of those, some indicate the following error message:</p>\n<p>\u201cAn error occurred in the program:<br>\nException: Failed to load coordinates for object key ImageNumber:14, ObjectNumber:1. This may indicate a problem with your per-object table.<br>\nYou can check your per-object table \u201cPer_Object\u201d in TableViewer\u201d</p>\n<p>And here is what I see in my per-object table:<br>\nScreenshot 2022-12-13 at 2.36.09 PM<br>\nScreenshot 2022-12-13 at 2.36.09 PM<br>\n1920\u00d7909 163 KB<br>\nIt is as if some of the Objects were not property related to their Parent objects, but it is not the case for all objects and all images were acquired and process exactly the same way\u2026 is this because of an issue in the initial pipeline or in CPA? Could someone help me understand and fix this mistake?</p>\n<p>Thanks!</p>\n<p>VVR</p>", "<p>Here is a new screenshot of the per-object table as I believe it is not appearing in the previous post.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/3/e3757fb600b50fbca35438f709f58be576e58a10.png\" data-download-href=\"/uploads/short-url/wsc7OcdZl99aqaCH7TRWCQbJaj6.png?dl=1\" title=\"Screenshot 2023-01-17 at 12.20.07 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3757fb600b50fbca35438f709f58be576e58a10_2_690x108.png\" alt=\"Screenshot 2023-01-17 at 12.20.07 PM\" data-base62-sha1=\"wsc7OcdZl99aqaCH7TRWCQbJaj6\" width=\"690\" height=\"108\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3757fb600b50fbca35438f709f58be576e58a10_2_690x108.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3757fb600b50fbca35438f709f58be576e58a10_2_1035x162.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/3/e3757fb600b50fbca35438f709f58be576e58a10_2_1380x216.png 2x\" data-dominant-color=\"ECECED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-01-17 at 12.20.07 PM</span><span class=\"informations\">1766\u00d7278 66.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi VVR,</p>\n<p>Yes your per-object table looks like it only has one object measured (at least for the view shown). This type of error can sometimes happen when using the ExportToDatabase module if the exporting isn\u2019t formatted appropriately for your data. Have you taken a look at our CellProfiler Analyst tutorial? You can download materials at <a href=\"http://broad.io/CZItransloc\" class=\"inline-onebox\">Translocation_Portable.zip - Google Drive</a> and we have a video here: <a href=\"https://www.youtube.com/watch?v=DfluRVNqA08&amp;t=1065s\" class=\"inline-onebox\">Intro to CellProfiler Analyst (2022) - YouTube</a></p>\n<p>The written component of the tutorial should walk you through how to set up export so the objects are findable in CPA.</p>", "<p>Hi Rebecca!</p>\n<p>Thank you for your prompt reply. I have watched the tutorial and followed the same steps as the example seen in the tutorial. I also tried to change the \u201cWhich objects should be used for locations?\u201d setting in the ExportToDatabase module, but it did not change the outcome.<br>\nWhat confuses me is that for those objects that location is not properly exported, the intensity measurements are also absent (0), but other measurements such as texture and area are present. And within one image, I can have one object for which all measurements were exported while the location and intensity measurements are missing for another object present in the same image.</p>", "<p>Hmm weird!</p>\n<p>Can you share your pipeline along with an image set where you notice this issue?</p>\n<p>Rebecca</p>", "<p>Of course, thanks for taking the time to help!</p>\n<p>Here is the pipeline and the first 3 images (out of 9, need to separate since I am not able to upload more than 5 files at a time).</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/1T4l2mTAtUFLYSoKNv6am4wlcEF.cpproj\">ActinSegmentationPipeline_V2.cpproj</a> (113.7 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/nwLl8IOJeySccUMAOntgvIkQziI.tif\">C1-VVR027-Cos7.lif - pcDNA3_2Ab-Cy3_Phalloidin-488_DAPI_6.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/yfV2koHCXl31XT2PbblzkhaGh0x.tif\">C2-VVR027-Cos7.lif - pcDNA3_2Ab-Cy3_Phalloidin-488_DAPI_6.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/7fvhzBWhBY7Rw5WYklTuUFngiML.tif\">C3-VVR027-Cos7.lif - pcDNA3_2Ab-Cy3_Phalloidin-488_DAPI_6.tif</a> (8.0 MB)</p>\n<p>Briefly, what I am looking at is the phenotype of actin cytoskeleton organization: cells transfected with empty vector or my gene of interest (with Myc tag). I am really only interested in measurements made on the actin cytoskeleton (channel 488). You will notice in the pipeline that I have an EditObjectManually module, this is because I attempted different thresholding methods to select only relevant cells (not touching others, included whole cytoskeleton, not split in multiple parts\u2026) but none worked for my whole dataset therefore I decided to select the cells I wanted to take measurements from manually.</p>\n<p>I hope this is enough for now, but let me know if you need more information!</p>\n<p>PS: note this is only a sample image set which includes examples of the issue I am having and one example of an appropriate object location extraction.</p>\n<p>Victoria</p>", "<p><a class=\"attachment\" href=\"/uploads/short-url/6eyo7fdVZWryHZV1gV96QkMmgnq.tif\">C1-VVR027-Cos7.lif - WT-Myc_Myc-Cy3_Phalloidin-488_DAPI_2.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/tOVsMGk1PjN9OMkNJhhFVq86CLR.tif\">C2-VVR027-Cos7.lif - WT-Myc_Myc-Cy3_Phalloidin-488_DAPI_2.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/hYZ1Ws3KiYjOTWXjDf9IvfDGzts.tif\">C3-VVR027-Cos7.lif - WT-Myc_Myc-Cy3_Phalloidin-488_DAPI_2.tif</a> (8.0 MB)</p>", "<p><a class=\"attachment\" href=\"/uploads/short-url/t3Z0JMGWm1yyakGvTdsrqlZRI8v.tif\">C1-VVR027-Cos7.lif - WT-Myc_Myc-Cy3_Phalloidin-488_DAPI_6.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/jnsGQn4cLsbJ8SiYEwbQrThLAdh.tif\">C2-VVR027-Cos7.lif - WT-Myc_Myc-Cy3_Phalloidin-488_DAPI_6.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/dhqwpS5D9cX3C19MCsbXXMjWERj.tif\">C3-VVR027-Cos7.lif - WT-Myc_Myc-Cy3_Phalloidin-488_DAPI_6.tif</a> (8.0 MB)</p>", "<p>Hi Victoria,</p>\n<p>So I did get a similar error to you. For me, this was resolved if I selected to export \u201cAll objects to database\u201d under ExportToDatabase. This is what you had previously selected:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb975e505a6a1168216fa813da97edee2df73fcb.png\" data-download-href=\"/uploads/short-url/zTG9kbFii8e95saJValJKX4AP11.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb975e505a6a1168216fa813da97edee2df73fcb_2_690x89.png\" alt=\"image\" data-base62-sha1=\"zTG9kbFii8e95saJValJKX4AP11\" width=\"690\" height=\"89\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb975e505a6a1168216fa813da97edee2df73fcb_2_690x89.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb975e505a6a1168216fa813da97edee2df73fcb_2_1035x133.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb975e505a6a1168216fa813da97edee2df73fcb_2_1380x178.png 2x\" data-dominant-color=\"EBEBEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1762\u00d7228 45.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I\u2019m not 100% sure there isn\u2019t another issue related to editing objects manually, but I would try that solution first and see if it works for you. If possible, I\u2019d recommend not editing the objects manually if you can avoid it. If you still have issues, I would also try using \u201cRenumber\u201d instead of \u201cRetain\u201d for renumbering the objects when you edit them.</p>", "<p>It seems to have worked! Thank you so much for taking the time to look at my pipeline.</p>\n<p>Best,</p>\n<p>Victoria</p>"], "78097": ["<p>Hello!</p>\n<p>I am analysing fluorescence images from C. elegans treated with different compounds that alter the worm fluorescence intensity and/or pattern. Using CellProfiler and the WormToolbox I have managed to put together a pipeline that does the following:</p>\n<ol>\n<li>Identify worms in brightfield images</li>\n<li>Measure fluorescence intensity, texture, and granularity of worm objects in fluorescence images</li>\n<li>Count the number of worm progeny objects</li>\n</ol>\n<p>The data I get out of this pipeline is already very useful but I am struggling to find a way to do the following:</p>\n<p>A) Distinguish dead worms from alive worms:</p>\n<p>some compounds kill the worms (labeled as \u201cdeadWorms\u201d in example images). While these images look quite different than the negative controls (\u201cnegCtrl\u201d) to the eye, I would like to be able to distinguish them using CellProfiler. It seems that the area, compactness, majoraxislength, and perimeter are slighlty higher in dead worms compared to the negative controls (see distribution plots attached), but the difference is not big enough to be able to establish a clear threshold. The fluorescence (<em>MeanIntensity</em>) is slightly lower in dead worms, but other compounds also cause a reduction in fluorescence without killing the worms (e.g. \u201cH07_90_DAPI_fluorescencePattern.tif\u201d image), so I can\u2019t really use this parameter.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/ofEtcJM3pSzr9SaCpWkVgn9DKr.tiff\">measurements_distribution.tiff</a> (5.2 MB)</p>\n<p>I have inspected the measurements produced by <em>MeasureTexture</em> and <em>MeasureGranularity</em> modules and I haven\u2019t been able to find a parameter that is clearly different in the dead worm images.</p>\n<p>I have also tried the module <em>IdentifyDeadWorms</em> using the WormBinary image obtained from UntangleWorms <span class=\"hashtag\">#15</span> and it doesn\u2019t quite work with these images.</p>\n<p>B) Distinguish a particular fluorescence pattern:</p>\n<p>In the image \u201cH07_90_DAPI_fluorescencePattern.tif\u201d, the fluorescence pattern is different from the negative control \u2013 there is an accumulation of fluorescence in the anterior part of the worm. Again, this is quite obvious to the eye but I can\u2019t find a way to identify it with CellProfiler. I have tried the <em>MeasureObjectIntensityDistribution</em> module for this but the bins start at the center of the worm:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/572454906b0d2c4f39a4399556c715ca817bdfdd.png\" data-download-href=\"/uploads/short-url/cqTnokXU7KaEPCJekiOmD23S6Ad.png?dl=1\" title=\"MeasureObjectIntensityDistribution-output\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/572454906b0d2c4f39a4399556c715ca817bdfdd_2_690x265.png\" alt=\"MeasureObjectIntensityDistribution-output\" data-base62-sha1=\"cqTnokXU7KaEPCJekiOmD23S6Ad\" width=\"690\" height=\"265\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/572454906b0d2c4f39a4399556c715ca817bdfdd_2_690x265.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/572454906b0d2c4f39a4399556c715ca817bdfdd_2_1035x397.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/572454906b0d2c4f39a4399556c715ca817bdfdd.png 2x\" data-dominant-color=\"C4C1C1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">MeasureObjectIntensityDistribution-output</span><span class=\"informations\">1145\u00d7440 26.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nand I would need them to start at the head to see if the distribution of fluorescence throughout the worm is even or not.</p>\n<p>Any suggestion on how to tackle these two questions and improve the pipeline would be greatly appreciated! Thank you!</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/qpffU7560vPg8DrWOjA9kaSCa26.zip\">qty_worm_fluorescence_test.zip</a> (14.5 MB)</p>"], "23827": ["<p>To help identify yeast from transmitted-light images, I\u2019d like to try the <a href=\"http://cellstar-algorithm.org/\" rel=\"nofollow noopener\">CellStar</a> algorithm.</p>\n<p>However, after downloading the plugins from <a href=\"https://docs.google.com/uc?id=0B3to8FwFxuTHdXlTRWhEZUVSNk0&amp;export=download\" rel=\"nofollow noopener\">here</a> to my CellProfiler plugins directory, I still don\u2019t see the new modules. I think the plugin directory is specified correctly because I do see the module from a different plugin that I have in the same directory.</p>\n<p>Is this because the original plugin was written for CP 2, and if so, are there guidelines for updating plugins to be compatible with CP 3?</p>\n<p><a class=\"mention\" href=\"/u/fafafft\">@fafafft</a>, do you have any advice for using CellStar in CellProfiler 3?</p>\n<p>Thanks!</p>", "<p>Hi <a class=\"mention\" href=\"/u/tswayne\">@tswayne</a>!</p>\n<p>Great to hear that you are interesting in giving CellStar a try!</p>\n<p>The plugin has been written for CellProfiler 2 so I\u2019m not sure that it can work for CP 3 - however maybe it is the same as to be CP 2.3 - time passes so fast : )</p>\n<p>Anyway I will take a look at it this weekend and let you know how much work is necessary to make it work.</p>", "<p>Thanks so much, <a class=\"mention\" href=\"/u/fafafft\">@fafafft</a>!</p>", "<p>Ok, so it works with CellProfiler 3.0.0 and after a small change it also works with CP 3.1.8.</p>\n<p>Please use this version: <a href=\"https://github.com/Fafa87/CellProfiler-plugins/tree/sstoma-yit-seg\" rel=\"nofollow noopener\">https://github.com/Fafa87/CellProfiler-plugins/tree/sstoma-yit-seg</a>. Use cellstar folder and identifyyeastcells.py from repo instead of the ones from the zip that you downloaded:</p>\n<p>Let me know if have any problems, I will be glad to help.</p>", "<p>Thank you! I will try it out soon!</p>", "<p>Did it work? Did it fail?<br>\nI hope it did not harm your computer <img src=\"https://emoji.discourse-cdn.com/twitter/stuck_out_tongue.png?v=9\" title=\":stuck_out_tongue:\" class=\"emoji\" alt=\":stuck_out_tongue:\"></p>", "<p>Hi <a class=\"mention\" href=\"/u/fafafft\">@fafafft</a>, I\u2019m sorry, some urgent projects intervened and I didn\u2019t test it yet. Many thanks for your help \u2013 I will let you know how it goes!</p>", "<p>Hi <a class=\"mention\" href=\"/u/fafafft\">@fafafft</a>,<br>\nI just tried CellStar with CellProfiler 3. I do see some errors/warnings when loading pipelines, but they seem to work well with your example data! First pipeline I see that produces good segmentations on bright-field images. <img src=\"https://emoji.discourse-cdn.com/twitter/smiley.png?v=9\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\"><br>\nWhen working with my own images though I still need to figure out the right parameter settings. Was wondering if you have any advice for me on that end. Below is the output using pipeline \u201cStrongEdgesAndWhite_1_Default.cpproj\u201d:</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/y3YxQ0NQxEn7WvgdDhZNMLskBgW.tiff\">Stack0003_outlinesBoth.tiff</a> (4.1 MB)</p>\n<p>Any advice you may have on whether I\u2019m missing any pre-processing steps, which pipeline to use and how to adjust parameters would be much appreciated.</p>", "<p>Hi noemi!</p>\n<p>Could you upload the original image and point which object you are interested in?</p>", "<p>Sure \u2013 below outlines of the cells I would like to identify:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/d/cd88f56b2ca48b1f70ae544d2e7ce688b6c684c5.png\" data-download-href=\"/uploads/short-url/tkfjt8BE6DPTtcFI79X53QSd5ch.png?dl=1\" title=\"FijiIn0001\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd88f56b2ca48b1f70ae544d2e7ce688b6c684c5_2_669x500.png\" alt=\"FijiIn0001\" data-base62-sha1=\"tkfjt8BE6DPTtcFI79X53QSd5ch\" width=\"669\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd88f56b2ca48b1f70ae544d2e7ce688b6c684c5_2_669x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd88f56b2ca48b1f70ae544d2e7ce688b6c684c5_2_1003x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd88f56b2ca48b1f70ae544d2e7ce688b6c684c5_2_1338x1000.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/d/cd88f56b2ca48b1f70ae544d2e7ce688b6c684c5_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">FijiIn0001</span><span class=\"informations\">1392\u00d71040 1.39 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Note that some of the outlines contain multiple cells, that we will want to track individually.</p>", "<p>The original image looks completely black:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/3DfNlfgcaxtmYBNll2oC2qi5Il8.TIF\">TimePoint_01_final.TIF</a> (2.8 MB)</p>\n<p>I pre-processed it with Fiji to obtain the image from above as input to CellStar.</p>", "<p>Hi!<br>\nI have checked your data and unfortunately it will not be possible to use CellStar it in your case. CellStar has two main limitation: cells are expected to be quite regular and roundish and either interior or borders has to be distinctive.</p>\n<p>Here are the samples for which CellStar is working well:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/f/5f10dba1f633698ea2138635a84eb2e51bbff210.jpeg\" data-download-href=\"/uploads/short-url/dyZtC8oXLwvxvwDN9wYaA3hsgVO.jpeg?dl=1\" title=\"image\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/f/5f10dba1f633698ea2138635a84eb2e51bbff210_2_333x250.jpeg\" alt=\"image\" data-base62-sha1=\"dyZtC8oXLwvxvwDN9wYaA3hsgVO\" width=\"333\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/f/5f10dba1f633698ea2138635a84eb2e51bbff210_2_333x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/f/5f10dba1f633698ea2138635a84eb2e51bbff210_2_499x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/f/5f10dba1f633698ea2138635a84eb2e51bbff210_2_666x500.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/f/5f10dba1f633698ea2138635a84eb2e51bbff210_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">720\u00d7540 171 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>As to your data you could try:</p>\n<ul>\n<li>reimage the data with different focus plane setting so that cells are more visible (<a href=\"https://blog.cellprofiler.org/2017/06/15/quantifying-microscopy-images-top-10-tips-for-image-acquisition/\" rel=\"nofollow noopener\">https://blog.cellprofiler.org/2017/06/15/quantifying-microscopy-images-top-10-tips-for-image-acquisition/</a>)</li>\n<li>use something like Ilastik to find pixels belonging to cells and then use CellProfiler for the rest of processing (<a href=\"https://blog.cellprofiler.org/2017/01/19/cellprofiler-ilastik-superpowered-segmentation\" rel=\"nofollow noopener\">https://blog.cellprofiler.org/2017/01/19/cellprofiler-ilastik-superpowered-segmentation</a>)</li>\n</ul>", "<p>OK, I will try that. Thank you!</p>", "<p>Hello,<br>\nI am very interested in applying identifyyeastcells to my brightfield images<br>\nwith Cell Profiler v4.2.4.<br>\nIdentify Yeast Cells does not appear anywhere in the Add Modules dialog.<br>\nAny ideas on the following start-up error?<br>\nCould not load identifyyeastcells<br>\nTraceback (most recent call last):<br>\nFile \u201ccellprofiler_core\\utilities\\core\\modules_<em>init</em>_.py\u201d, line 71, in add_module<br>\nFile \u201cD:\\source\\keesh_programming\\GMI\\cell_profiler_plugins\\CellProfiler4_AutoConvert\\identifyyeastcells.py\u201d, line 132, in <br>\nimport cellprofiler.measurement as cpmeas<br>\nModuleNotFoundError: No module named \u2018cellprofiler.measurement\u2019<br>\nthx<br>\n<a href=\"mailto:keesh@ieee.org\">keesh@ieee.org</a></p>", "<p>I will be glad to help.<br>\nFirst I will try to setup the plugin in the CellProfiler 4 on my side and I will let you know how it went.</p>", "<p>Fafafft,<br>\nAny luck as IdentifyPrimaryObjects is not producing great results?<br>\nthx e.-</p>", "<p>I am on it.<br>\nOne of the problems is that cellstar does not work inside Python3 : ] so I am updating it to using Python3 and potentially newer packages - and as usual some obscure errors follow as I want to be sure that it is not broken. Then I will make it work with CellProfiler4.</p>\n<p>I am quite sure I will get it all done this week.</p>", "<p>Your efforts are greatly appreciated.</p>", "<p>Still on it, fixed a number of p2 vs p3 division issues.</p>", "<p>Almost, cellstar package is updated, however the plugin itself is still struggling a bit:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/0/9029bd42dff079a459743d27099baaf0fd05d9ec.png\" data-download-href=\"/uploads/short-url/kzkbgMIckolanXD54DtT40w21qI.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/0/9029bd42dff079a459743d27099baaf0fd05d9ec_2_690x254.png\" alt=\"image\" data-base62-sha1=\"kzkbgMIckolanXD54DtT40w21qI\" width=\"690\" height=\"254\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/0/9029bd42dff079a459743d27099baaf0fd05d9ec_2_690x254.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/0/9029bd42dff079a459743d27099baaf0fd05d9ec_2_1035x381.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/0/9029bd42dff079a459743d27099baaf0fd05d9ec.png 2x\" data-dominant-color=\"B4B4B4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1263\u00d7466 85.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nHopefully there are only some minor python3 errors left.</p>\n<p>First I will make a zip package that you can use by just setting plugins path.<br>\nIntegration with cellprofiler plugins repo I will do later as I do not yet grasp how to it is supposed to work.</p>"], "78612": ["<p>Hi everyone,</p>\n<p>Thank you in advance for any help. I have a quick question about relate objects and colocalization. I noticed today that, unlike other times I\u2019ve used relate objects for fos and another stain, I was getting a colocalization count higher than possible if it was 1-to-1. Does the order of the relate objects for parent and child matter such that the parent should always be the stain with the smaller number of objects? Or is there another step I can take to ensure there\u2019s only 1 fos object per stained cell? Thank you!!</p>\n<p>Best,<br>\nBrandon</p>"], "79646": ["<p>Hi, hope all are well. Would like to say in advance any help is appreciated.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1e9614fb6a68afd92c354e61478ef3f342c785c.jpeg\" data-download-href=\"/uploads/short-url/yw35gGg5YyLtmtW2iOcq0wx12X2.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1e9614fb6a68afd92c354e61478ef3f342c785c.jpeg\" alt=\"image\" data-base62-sha1=\"yw35gGg5YyLtmtW2iOcq0wx12X2\" width=\"504\" height=\"500\" data-dominant-color=\"1A0A0D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">526\u00d7521 72.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8fab2d1cb543c54bc3de4f7a23b5c08d26a9327f.png\" alt=\"image\" data-base62-sha1=\"kuX1l9Fv3FnHJazxn8ZwMsxIncb\" width=\"614\" height=\"199\"><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/17612c6994a738a340b1bf4b8e5b863ac6aa2bd6.png\" data-download-href=\"/uploads/short-url/3kPaQUgogpTVBhkNLCqxEza5lVs.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/17612c6994a738a340b1bf4b8e5b863ac6aa2bd6.png\" alt=\"image\" data-base62-sha1=\"3kPaQUgogpTVBhkNLCqxEza5lVs\" width=\"607\" height=\"500\" data-dominant-color=\"5D90BA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1081\u00d7889 44.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0de07f7492b0883b0ad17afa9993c422b782b0a.jpeg\" data-download-href=\"/uploads/short-url/w5gEtzBKMzzcLTajBA5fpqm2oZk.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0de07f7492b0883b0ad17afa9993c422b782b0a_2_582x500.jpeg\" alt=\"image\" data-base62-sha1=\"w5gEtzBKMzzcLTajBA5fpqm2oZk\" width=\"582\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0de07f7492b0883b0ad17afa9993c422b782b0a_2_582x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/0/e0de07f7492b0883b0ad17afa9993c422b782b0a_2_873x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/0/e0de07f7492b0883b0ad17afa9993c422b782b0a.jpeg 2x\" data-dominant-color=\"747C81\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1055\u00d7905 168 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>image1055\u00d7905 168 KB<br>\nBackground<br>\n\u2022 The 1st image is all 3 channels, Nuclear(DAPI), MMP1, and GDF15, channels 1,2, and 3 the other is an example of the channels separated. The images are originally z stacked nd2 files converted to 2 on image j \u2026 the last two are what I tried to do when uploading the channels separately, I also tried loading the merged channels<br>\nAnalysis goals<br>\nI am hoping to 1. quantify all the cells, 2. Cell : GDF15+ and MMP1+ 3. Cell: GDF15+ and MMP1- 4. Cell: GDF15- and MMP1+ ---- and quantify the level of expression of both.<br>\nChallenges<br>\nWe tried to get the integrated density using image j. However, there have been issues with getting the density per cell that expresses the protein. Sometimes two cells are counted as one. or the distribution of protein leads to the density being added up separately and a few other issues. I apologize ahead of time for asking questions that may be similar to others. However, I have not been able to find any that quite ask what I am looking to do. I have watched a few tutorials and worked through a few. However, I am new to image processing and I am pretty lost.<br>\nIf there are any tutorials that could help me walk through this that would be great. I am hoping to learn how to use Illastik, CellProfiler etc. Lastly, if there is other ways to do this I am open to all help.</p>", "<p>Do you have a single channel that defines the outline of your cells in order to segment them? Sometimes the information just isn\u2019t in the image due to experimental design.</p>\n<p>If you can use something like CellPose to target a channel or two to create cell borders, then it should be fairly straightforward to check for the presence of other stains within those borders. But having that 100% present channel for all cells is necessary.</p>\n<p>Once you have cells, I suspect a variety of software has workflows like the following for QuPath <a href=\"https://qupath.readthedocs.io/en/0.4/docs/tutorials/multiplex_analysis.html\" class=\"inline-onebox\">Multiplexed analysis \u2014 QuPath 0.4.3 documentation</a> which can also be used with the CellPose extension.</p>\n<p>In CellProfiler I\u2019m guessing this would be done using secondary objects for the other channels within the primary object for the 100% present channel.</p>", "<p>Hi, sorry for the late reply. Thank you so much for trying to provide some assistance.No, I do not,  I have a DAPI channel for nuclear staining and then one for GDF15 and MMP1.</p>", "<p>If you have an original image you can share or your attempted pipeline, other users might be able to help. Defining the outline of cells usually requires some sort of marker that is in 100% of the cells though, or else what determines the edge/size of cells?</p>\n<p>Alternatively, approaches like the standard QuPath cell seg don\u2019t even try to get the outline of the cells, but just pop a slight expansion around the nucleus. Which can work as long as at least some of each stain is tightly packed around the nucleus\u2026 and the nuclei are not packed too densely.</p>", "<p>Dear <a class=\"mention\" href=\"/u/user1234\">@user1234</a>,</p>\n<p>if you want to stay within ilastik I think you are more or less on the right track, already.</p>\n<p>First question would be, how you converted your data from 3D to 2D. In principle ilastik works in 3D the same as in 2D, so no real need to do that. Also I\u2019m not sure how to interpret/quantify intensities after this conversion.</p>\n<p>Given that you don\u2019t have a channel to identify the cell outline/volume, you\u2019ll have to go by nuclei. This would require that the characteristics that you\u2019re highlight with the other stainings (sorry, I\u2019m not a biologist <img src=\"https://emoji.discourse-cdn.com/twitter/grimacing.png?v=12\" title=\":grimacing:\" class=\"emoji\" alt=\":grimacing:\" loading=\"lazy\" width=\"20\" height=\"20\">) are localized close or better overlapping with the nuclei (it looks like MMP1 is close, and GDF15 overlapping?). So, you might have to fill in some blanks <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Then I\u2019d say that the general approach is to do Pixel Classification, to differentiate between foreground and background (this would for you probably be nuclei? and anything else). Then exporting the probability image.</p>\n<p>With this you\u2019d go into a separate Object Classification workflow (we don\u2019t recommend using the combined workflow, as in the screenshot, for serious work as it can be very memory hungry and less stable). In Object classification you\u2019d have the different classes that you\u2019re interested in and choose features that help you in this task (see here for an overview of the <a href=\"https://www.ilastik.org/documentation/objects/objectfeatures\">available features</a>). Since the information is predominantly in the intensity information, I\u2019d focus on those features and go maybe for the <code>mean</code> and <code>mean in the neighborhood</code>. Make sure to adjust the neighborhood to something that makes sense (distance will be to the outline for the nuclei, in pixels). With this it should be possible to learn the different classes.<br>\nIf you should run into problems separating nuclei, you could also add an additional class in object classification to filter those out (then maybe also add \u201csize\u201d as a feature) - if you can afford to ignore these kind of \u201cimperfect\u201d segmentations.</p>\n<p>Our i2k tutorial goes into some detail for the pixel classification and object classification workflows: <a href=\"https://youtu.be/F6KbJ487iiU\" class=\"inline-onebox\">interactive image segmentation with ilastik - YouTube</a> (albeit on a more simple example).</p>\n<p>Hope this helps\u2026</p>\n<p>Cheers<br>\nDominik</p>"], "77087": ["<p>We\u2019re hiring for our Postdoctoral Training Program in Bioimage Analysis! Are you a wet lab biologist who wants to move into computational tools and workflows? Do you love collaborating with others &amp; solving puzzles? Do you want an excuse to hang out on image.sc as part of your job? Do you want a career-specific training program? Do you want to do all this in Cambridge, MA, USA, with awesome coworkers and collaborators? If so, check us out!</p>\n<aside class=\"onebox googledocs\" data-onebox-src=\"https://docs.google.com/document/d/1mcjHtTfIe3_LH4aHoly1sFIrRhS3GsOx2Yi6lnlQyZ0/preview\">\n  <header class=\"source\">\n\n      <a href=\"https://docs.google.com/document/d/1mcjHtTfIe3_LH4aHoly1sFIrRhS3GsOx2Yi6lnlQyZ0/preview\" target=\"_blank\" rel=\"noopener\">docs.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <a href=\"https://docs.google.com/document/d/1mcjHtTfIe3_LH4aHoly1sFIrRhS3GsOx2Yi6lnlQyZ0/preview\" target=\"_blank\" rel=\"noopener\"><span class=\"googledocs-onebox-logo g-docs-logo\"></span></a>\n\n<h3><a href=\"https://docs.google.com/document/d/1mcjHtTfIe3_LH4aHoly1sFIrRhS3GsOx2Yi6lnlQyZ0/preview\" target=\"_blank\" rel=\"noopener\">Image analyst postdoc job description</a></h3>\n\n<p>Postdoctoral Training Program in Bioimage Analysis Broad Institute of MIT and Harvard, Cambridge, MA  Impact the discovery of new disease therapies in the Cimini laboratory at the Imaging Platform  Join our vibrant, world-class, nonprofit biomedical...</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n"], "78624": ["<p>I was wondering if cellprofiler offers Java APIs so I can use the particular module (i.e feature extraction). I found on the GitHub it is possible on Python but no info on Java.</p>\n<p>Thanks everyone in advance.</p>"], "75044": ["<p>Greetings. To fulfill the objectives of our study, we have outlined the margin of a tumoral area (yellow line in the attached image) and then we want QuPath be taught to analyze in each single tile (1 mm^2) the cancer cells found. Therefore we would like the <strong>tiles</strong> to be <strong>automatically centered</strong> with respect to the aforementioned line. Successive tiles can be placed staggered if necessary, but <strong>never overlap</strong>. See the picture (hand drown) as final result we would achieve. <em>Is this feature already implemented or it could be released in the future update? thanks to who will reply and help me.</em></p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/5/2511b78f10172054b3500ae52d340a5b5be2cc87.jpeg\" alt=\"exemple\" data-base62-sha1=\"5hVCKlM1dsNnEZQMxZNbMwbHeir\" width=\"525\" height=\"373\"></p>", "<p>Seems like the sort of thing that could probably be coded, but since you used a polyline tool with click points here rather than drag/draw you have made the job a lot harder. Ideally you\u2019d traverse along the line and find out when you are \u201cfar enough\u201d in X and or Y to draw a new box. But QuPath stores vertices along the line, and if you don\u2019t have enough vertices, you will need to interpolate somehow. Maybe create a line equation between any two points and then step along that line a certain distance until you are past the next vertex.</p>\n<p>Regardless, pretty sure the code to do that doesn\u2019t exist. I think that\u2019s the first time I\u2019ve seen something like that on the forum. The closest was tiling overlapping fields of view within a given annotation for export.</p>", "<p>For this specific scenario, I would personally just create an annotation of fixed width around the line instead of generating tiles. Then just analyse the cancer cells in this annotation.</p>\n<p>This would be significantly easier to do, just by dilating the line by x microns or pixels.</p>", "<p>I agree with <a class=\"mention\" href=\"/u/ym.lim\">@ym.lim</a>, something like <a href=\"https://petebankhead.github.io/qupath/scripts/2018/08/08/three-regions.html\" rel=\"noopener nofollow ugc\">this</a> script by <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> would probably give you the sort of information you\u2019re looking for</p>", "<p><strong>Thanks</strong> to all who answered, much appreciated! <img src=\"https://emoji.discourse-cdn.com/twitter/pray.png?v=12\" title=\":pray:\" class=\"emoji\" alt=\":pray:\" loading=\"lazy\" width=\"20\" height=\"20\">  I decided to draw a poliline and after had expanded it, finally calculating the objects inside. Just a question for you all: is it possible to make <em>QuPath</em> drawing it, at a certain distance from the tumor? maybe using the counting grid (as visible in the picture)?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/2/02c93e1e5604174af69948eba7e826e816d650fc.jpeg\" data-download-href=\"/uploads/short-url/oE79kzGdAxI9oSRXaAooDZITeI.jpeg?dl=1\" title=\"Immagine 2023-05-05 184716\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/02c93e1e5604174af69948eba7e826e816d650fc_2_690x490.jpeg\" alt=\"Immagine 2023-05-05 184716\" data-base62-sha1=\"oE79kzGdAxI9oSRXaAooDZITeI\" width=\"690\" height=\"490\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/02c93e1e5604174af69948eba7e826e816d650fc_2_690x490.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/2/02c93e1e5604174af69948eba7e826e816d650fc_2_1035x735.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/2/02c93e1e5604174af69948eba7e826e816d650fc.jpeg 2x\" data-dominant-color=\"CCC5C8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Immagine 2023-05-05 184716</span><span class=\"informations\">1047\u00d7745 263 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>I\u2019m assuming your tumour is on the left side of the blue line?</p>\n<p>If so, if you want to dilate your annotations outwards by a certain distance, you will need to annotate your tumour as a closed annotation, rather than just a polyline.</p>", "<p>yes, the core of the tumour is on the left, as you can see in the picture. We would like to identify and drawing the line between the tumour and the stroma (normal tissue)\u2026</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/2/22d3c46868a951cb0d611d034b1435b0e1a7740e.jpeg\" data-download-href=\"/uploads/short-url/4Y5Wtxgt9aNQDhXmYer24zuhbga.jpeg?dl=1\" title=\"Immagine 2023-05-05 190950\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/2/22d3c46868a951cb0d611d034b1435b0e1a7740e_2_690x316.jpeg\" alt=\"Immagine 2023-05-05 190950\" data-base62-sha1=\"4Y5Wtxgt9aNQDhXmYer24zuhbga\" width=\"690\" height=\"316\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/2/22d3c46868a951cb0d611d034b1435b0e1a7740e_2_690x316.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/2/22d3c46868a951cb0d611d034b1435b0e1a7740e_2_1035x474.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/2/22d3c46868a951cb0d611d034b1435b0e1a7740e.jpeg 2x\" data-dominant-color=\"BCB6B5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Immagine 2023-05-05 190950</span><span class=\"informations\">1353\u00d7621 331 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"NicPiz\" data-post=\"5\" data-topic=\"75044\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/n/9de053/40.png\" class=\"avatar\"> NicPiz:</div>\n<blockquote>\n<p>Just a question for you all: is it possible to make <em>QuPath</em> drawing it, at a certain distance from the tumor?</p>\n</blockquote>\n</aside>\n<p>Dilation from the tumor, which you could pixel classifier or threshold would could give you some options. But not a polyline. For that you would need some mathematical way to define the starting and ending point, which you haven\u2019t suggested how to do.</p>"], "66860": ["<p>Hello everyone, I wanted to install the latest RunCellPose plugin in CellProfiler for better image analysis, hence require a compilation of CellProfiler from the GitHub source code. Following the installation instruction (<a href=\"https://github.com/CellProfiler/CellProfiler/wiki/Installation-of-CellProfiler-4-from-source-on-MacOS-M1\" rel=\"noopener nofollow ugc\">for CellProfiler</a>, <a href=\"https://github.com/CellProfiler/CellProfiler-plugins/blob/361c858f3d2a93901e329fc1c2c8d8a2935d1748/Instructions/Instructions_runCellpose.md\" rel=\"noopener nofollow ugc\">for RunCellPose</a>) of Github, I downloaded CellProfiler, cellulose, and some dependency packages in a conda environment, here are my installed packages according to <strong>pip list</strong></p>\n<pre><code class=\"lang-auto\">Package                Version   Editable project location\n---------------------- --------- -----------------------------------------------------------\nboto3                  1.22.11\nbotocore               1.25.11\ncellpose               2.0.5\nCellProfiler           4.2.1     /Users/gilbertlab/miniconda3/envs/new_3.8/core/CellProfiler\ncellprofiler-core      4.2.1\ncentrosome             1.2.0\ncertifi                2021.10.8\ncharset-normalizer     2.0.12\ncycler                 0.11.0\ndeprecation            2.1.0\ndocutils               0.15.2\nedt                    2.2.0\nfastremap              1.12.2\nfuture                 0.18.2\nh5py                   3.6.0\nidna                   3.3\nimagecodecs            2022.2.22\nimageio                2.19.1\ninflect                5.6.0\nJinja2                 3.1.2\njmespath               1.0.0\njoblib                 1.1.0\nkiwisolver             1.4.2\nllvmlite               0.38.0\nmahotas                1.4.12\nMarkupSafe             2.1.1\nmatplotlib             3.1.3\nmysqlclient            1.4.6\nnatsort                8.1.0\nncolor                 1.1.2\nnetworkx               2.8\nnumba                  0.55.1\nnumpy                  1.22.3\nomnipose               0.2.1\nopencv-python-headless 4.5.5.64\npackaging              21.3\nPillow                 9.1.0\npip                    22.0.4\nprokaryote             2.4.4\npsutil                 5.9.0\npyparsing              3.0.8\npython-bioformats      4.0.5\npython-dateutil        2.8.2\npython-javabridge      4.0.3\nPyWavelets             1.3.0\npyzmq                  22.3.0\nrequests               2.27.1\ns3transfer             0.5.2\nscikit-image           0.19.2\nscikit-learn           1.0.2\nscipy                  1.8.0\nsentry-sdk             0.18.0\nsetuptools             62.2.0\nsix                    1.16.0\nthreadpoolctl          3.1.0\ntifffile               2022.5.4\ntorch                  1.11.0\ntqdm                   4.64.0\ntyping_extensions      4.2.0\nurllib3                1.26.9\nwheel                  0.37.1\nwxPython               4.1.1\n</code></pre>\n<p>The problem arise when I try to run cell profiler from my conda env, using the command <code>python3 -m cellprofiler</code> returns the error message: \" This program needs access to the screen. Please run with a Framework build of python, and only when you are logged in on the main display of your Mac.\" I instead tried the command <code>pythons -m cellprofiler</code>, but got a JAVA related error:</p>\n<pre><code class=\"lang-auto\">Failed to open libjli.dylib.\n\nTraceback (most recent call last):\n  File \"/Users/gilbertlab/miniconda3/envs/new_3.8/lib/python3.8/site-packages/javabridge/jutil.py\", line 282, in start_thread\n    vm.create_mac(args, RQCLS, library_path, libjli_path)\n  File \"_javabridge.pyx\", line 709, in _javabridge.JB_VM.create_mac\nRuntimeError: Failed to create Java VM. Return code = -1\nFailed to create Java VM\nTraceback (most recent call last):\n  File \"/Users/gilbertlab/miniconda3/envs/new_3.8/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/Users/gilbertlab/miniconda3/envs/new_3.8/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/gilbertlab/miniconda3/envs/new_3.8/core/CellProfiler/cellprofiler/__main__.py\", line 948, in &lt;module&gt;\n    sys.exit(main())\n  File \"/Users/gilbertlab/miniconda3/envs/new_3.8/core/CellProfiler/cellprofiler/__main__.py\", line 276, in main\n    app = cellprofiler.gui.app.App(\n  File \"/Users/gilbertlab/miniconda3/envs/new_3.8/core/CellProfiler/cellprofiler/gui/app.py\", line 54, in __init__\n    start_java()\n  File \"/Users/gilbertlab/miniconda3/envs/new_3.8/lib/python3.8/site-packages/cellprofiler_core/utilities/java.py\", line 114, in start_java\n    javabridge.start_vm(args=args, class_path=class_path)\n  File \"/Users/gilbertlab/miniconda3/envs/new_3.8/lib/python3.8/site-packages/javabridge/jutil.py\", line 319, in start_vm\n    raise RuntimeError(\"Failed to start Java VM\")\nRuntimeError: Failed to start Java VM\n</code></pre>\n<p>My current JAVA version is:</p>\n<pre><code class=\"lang-auto\">openjdk 18.0.1 2022-04-19\nOpenJDK Runtime Environment Homebrew (build 18.0.1+0)\nOpenJDK 64-Bit Server VM Homebrew (build 18.0.1+0, mixed mode, sharing)\n</code></pre>\n<p>I have tried different versions of JAVA from multiple vendors, but I am unsure if java version doesn\u2019t is the main issue. Should I try uninstalling and re-installing java from brew? Or perhaps the app couldn\u2019t find the correct path to Java VM? This error message doesn\u2019t give much clue as to what\u2019s wrong.</p>\n<p>For context, I am running on Apple M1 chip with arm64e architectures. If anyone is wondering, I did try installing the app and its plugin without conda but I ran into more trouble downloaded required packages, I was not able to do download dependencies for RunCellpose without conda.</p>\n<p>Any insights on running CP in a virtual environment would be greatly appreciated!<br>\nThank you in advance!</p>", "<p>Hello <a class=\"mention\" href=\"/u/karen-k\">@Karen-K</a></p>\n<p>Can you check if you set java as an environment variable, please run  echo $JAVA_HOME in your terminal.</p>\n<p>echo $JAVA_HOME<br>\n/Library/Java/JavaVirtualMachines/jdk-16.jdk/Contents/Home</p>\n<p>If java is not configured can you export <code>$JAVA_HOME</code> at <code>~/.zshenv</code> or <code>~/.zshrc</code> and try again?</p>\n<p>echo \u201cexport JAVA_HOME=$(your_path)\u201d &gt;&gt; ~/.zshrc<br>\nsource ~/.zshrc</p>\n<p>more information: <a href=\"https://mkyong.com/java/how-to-set-java_home-environment-variable-on-mac-os-x/\" rel=\"noopener nofollow ugc\">https://mkyong.com/java/how-to-set-java_home-environment-variable-on-mac-os-x/</a></p>\n<p>Best<br>\nMario</p>", "<p>I\u2019m also running on macOS on M1, and also see this error.</p>\n<p><code>JAVA_HOME</code> is <code>/opt/homebrew/Cellar/openjdk/19.0.2/libexec/openjdk.jdk/Contents/Home</code>.</p>\n<p>I\u2019ve also tried manually trusting <code>libjli.dylib</code> using <code>spctl</code>, but that does not change the symptoms.</p>\n<p>Do you have any other ideas/possible fixes, please?</p>\n<p>Thanks.</p>"], "77612": ["<p>I am not able to detect small spots when plates are analyzed in batch.Changing the parameters is making me picking non spotted regions<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/002c11d9e77b745893ba0484f36e1b1c9289674f.jpeg\" data-download-href=\"/uploads/short-url/1wpYuLFvFp06prfjTRHXzc4Vsz.jpeg?dl=1\" title=\"Run-1-Plate-003 - Original_NaturalOutline\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/002c11d9e77b745893ba0484f36e1b1c9289674f.jpeg\" alt=\"Run-1-Plate-003 - Original_NaturalOutline\" data-base62-sha1=\"1wpYuLFvFp06prfjTRHXzc4Vsz\" width=\"690\" height=\"449\" data-dominant-color=\"353333\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Run-1-Plate-003 - Original_NaturalOutline</span><span class=\"informations\">729\u00d7475 48.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/129312d3c57df9123f5170451df27626a0eba06c.jpeg\" data-download-href=\"/uploads/short-url/2EjHhqCYBTXqegrxMXwZQHRb45e.jpeg?dl=1\" title=\"Run-1-Plate-003 - Original_NaturalOutline\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/2/129312d3c57df9123f5170451df27626a0eba06c.jpeg\" alt=\"Run-1-Plate-003 - Original_NaturalOutline\" data-base62-sha1=\"2EjHhqCYBTXqegrxMXwZQHRb45e\" width=\"690\" height=\"452\" data-dominant-color=\"373535\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Run-1-Plate-003 - Original_NaturalOutline</span><span class=\"informations\">712\u00d7467 47.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<ul>\n<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox) \u2013 (make sure however that you are allowed to share the image data publicly under the conditions of this forum).</li>\n<li>Share a <a href=\"https://en.wikipedia.org/wiki/Minimal_working_example\" rel=\"noopener nofollow ugc\">minimal working example</a> of your macro code.</li>\n</ul>\n\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<ul>\n<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific \u201cjargon\u201d.</li>\n</ul>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<ul>\n<li>What information are you interested in getting from this image?</li>\n</ul>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<ul>\n<li>What stops you from proceeding?</li>\n<li>What have you tried already?</li>\n<li>Have you found any related forum topics? If so, cross-link them.</li>\n<li>What software packages and/or plugins have you tried?</li>\n</ul>", "<p>The pipeline I use<br>\n<a class=\"attachment\" href=\"/uploads/short-url/ska6yxSc7uWAv27y9GndW24EhjC.cpproj\">ExampleYeastPatchesJE (1).cpproj</a> (467.3 KB)</p>", "<p>Hi Rima,</p>\n<p>Could you provide an example image? The screenshots you have here are very useful for seeing your problem, but I can\u2019t troubleshoot with them. The project file itself does not contain images.</p>\n<p>Rebecca</p>", "<p>Thanks rebecca for your reply. Here are some images.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/76828277529f41ae2915f82a945ac29838d03415.jpeg\" data-download-href=\"/uploads/short-url/gUnXl8JdBfb964KFGmneCe9K7pb.jpeg?dl=1\" title=\"Run-1-Plate-004 - Original\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/76828277529f41ae2915f82a945ac29838d03415_2_666x500.jpeg\" alt=\"Run-1-Plate-004 - Original\" data-base62-sha1=\"gUnXl8JdBfb964KFGmneCe9K7pb\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/76828277529f41ae2915f82a945ac29838d03415_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/6/76828277529f41ae2915f82a945ac29838d03415_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/76828277529f41ae2915f82a945ac29838d03415.jpeg 2x\" data-dominant-color=\"393833\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Run-1-Plate-004 - Original</span><span class=\"informations\">1280\u00d7960 83 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/008ea714df3df6f66e38cd6bb5c4b6e3d2252525.jpeg\" data-download-href=\"/uploads/short-url/4VDb63v36uZMBR1TbwD7kPtcot.jpeg?dl=1\" title=\"Run-1-Plate-004 - Original\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/008ea714df3df6f66e38cd6bb5c4b6e3d2252525_2_666x500.jpeg\" alt=\"Run-1-Plate-004 - Original\" data-base62-sha1=\"4VDb63v36uZMBR1TbwD7kPtcot\" width=\"666\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/008ea714df3df6f66e38cd6bb5c4b6e3d2252525_2_666x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/0/008ea714df3df6f66e38cd6bb5c4b6e3d2252525_2_999x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/0/008ea714df3df6f66e38cd6bb5c4b6e3d2252525.jpeg 2x\" data-dominant-color=\"353939\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Run-1-Plate-004 - Original</span><span class=\"informations\">1280\u00d7960 204 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hello,<br>\nCan anyone help me out with this?</p>", "<p>Hey,</p>\n<p>Thanks for providing the images. I had a look at the pipeline you posted with the first image you posted. I adjusted a few things to give better segmentation.</p>\n<p>I wasn\u2019t sure what you meant by objects not being detected in a \u201cbatch\u201d, but in case you meant batch mode I\u2019ve added fixed coordinates to the crop module in the pipeline. This means that the same region will be cropped for each image, so it\u2019s essential that all plates to be compared are imaged without the camera or stage moving (it seems that they are already).</p>\n<p>The major change was to use adaptive thresholding in <strong>IdentifyPrimaryObjects</strong>. Though, it\u2019s not perfect - in order to get some of the \u201cdimmer\u201d spots, you pick up some non-relevant spots.</p>\n<p>Looking at your images it also seems that there\u2019s some sort of illuminating light that causes the plastic plate walls to be reflected in the agar. If you can minimize this, it would make the image processing a lot more easier.</p>\n<p>Here\u2019s the pipeline: <a class=\"attachment\" href=\"/uploads/short-url/btFUgZay3kTEK77f0fcGVD6KEqr.cppipe\">ExampleYeastPatchesJE.cppipe</a> (23.8 KB)</p>\n<p>Here\u2019s an example image (<em>OutlinedForced</em>):<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/0/a0bbaf87959cf1a49c0491f0f19190993c866f49.png\" alt=\"SCR-20230403-mnl\" data-base62-sha1=\"mVUvCfel6IzK2t1LSnvR5hijWlz\" width=\"501\" height=\"324\"></p>"], "78636": ["<p><a class=\"attachment\" href=\"/uploads/short-url/lhe9ef8xSjl6wdR35ie39GurX9E.cpproj\">David_roh.cpproj</a> (702.3 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/amZgRzf971WPs5qtgdIMgVGP2uS.cpproj\">gro\u00df2_AlexaFluor150.cpproj</a> (702.3 KB)</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f.jpeg\" data-download-href=\"/uploads/short-url/znCbf4xVCsE36LLE3pnFx0L70UT.jpeg?dl=1\" title=\"David_roh_automatisch_belichtet\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_667x500.jpeg\" alt=\"David_roh_automatisch_belichtet\" data-base62-sha1=\"znCbf4xVCsE36LLE3pnFx0L70UT\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f7f77d972fd7154892fc25a3785dbdb6a889423f_2_1334x1000.jpeg 2x\" data-dominant-color=\"1A1525\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">David_roh_automatisch_belichtet</span><span class=\"informations\">1388\u00d71040 56.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4.jpeg\" data-download-href=\"/uploads/short-url/akcuJVgazKVFhpnNT6dCwAkmUF6.jpeg?dl=1\" title=\"David_roh\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_667x500.jpeg\" alt=\"David_roh\" data-base62-sha1=\"akcuJVgazKVFhpnNT6dCwAkmUF6\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/8/485e567e608d4a9349618978f6773b39e3c8fae4_2_1334x1000.jpeg 2x\" data-dominant-color=\"1A0D2E\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">David_roh</span><span class=\"informations\">1388\u00d71040 56.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd.jpeg\" data-download-href=\"/uploads/short-url/1yHp9zI2ePDOmT4OJUF33FIuA9L.jpeg?dl=1\" title=\"gro\u00df_automatischeBelichtung\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_667x500.jpeg\" alt=\"gro\u00df_automatischeBelichtung\" data-base62-sha1=\"1yHp9zI2ePDOmT4OJUF33FIuA9L\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0aee55d5d470232338f6dfe167b7507d488985dd_2_1334x1000.jpeg 2x\" data-dominant-color=\"170B1F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">gro\u00df_automatischeBelichtung</span><span class=\"informations\">1388\u00d71040 53.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8.jpeg\" data-download-href=\"/uploads/short-url/tObNxTSMUFtZnW9rdiVeq3mU9VS.jpeg?dl=1\" title=\"gro\u00df2_AlexaFluor150\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_667x500.jpeg\" alt=\"gro\u00df2_AlexaFluor150\" data-base62-sha1=\"tObNxTSMUFtZnW9rdiVeq3mU9VS\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/0/d0eb7780161cf790d1889cd8f2b3c0b46b4b88e8_2_1334x1000.jpeg 2x\" data-dominant-color=\"32181F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">gro\u00df2_AlexaFluor150</span><span class=\"informations\">1388\u00d71040 84.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017.jpeg\" data-download-href=\"/uploads/short-url/sQC0JhGxwgcvfVu4zJpwyeno7PN.jpeg?dl=1\" title=\"Test5_AlexaFluor370\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_667x500.jpeg\" alt=\"Test5_AlexaFluor370\" data-base62-sha1=\"sQC0JhGxwgcvfVu4zJpwyeno7PN\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca2f677115cb75491c63ac75fc9a834d223aa017_2_1334x1000.jpeg 2x\" data-dominant-color=\"3B1D48\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Test5_AlexaFluor370</span><span class=\"informations\">1388\u00d71040 118 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Hello everybody,<br>\nI am trying to determine the cell boundaries on the images in the attachment. Unfortunately, my results are not accurate enough so far. Does anyone of you maybe know a way how I can best determine the cell borders?<br>\nI have attached two pipelines of mine. Maybe one of you has an idea how I should improve them for more accurate results. Furthermore, I have attached some more sample images so that you know how the images look like with which a pipeline should work.</p>\n<p>I would really appreciate your help.<br>\nWith kind regards,<br>\nJustine</p>", "<p>Here I have four more pictures so you can get an impression.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad.jpeg\" data-download-href=\"/uploads/short-url/f6pqeiHv10NkXTbnJf3GcFd7rEp.jpeg?dl=1\" title=\"David_roh_2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_667x500.jpeg\" alt=\"David_roh_2\" data-base62-sha1=\"f6pqeiHv10NkXTbnJf3GcFd7rEp\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/9/69da1afbeab551a55c8ebe11074986c6a49e9aad_2_1334x1000.jpeg 2x\" data-dominant-color=\"3E1F29\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">David_roh_2</span><span class=\"informations\">1388\u00d71040 101 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd.jpeg\" data-download-href=\"/uploads/short-url/2PF6lBAfqnzvNgheiq68i8SUuhn.jpeg?dl=1\" title=\"Test4_automatischeBelichtung\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_667x500.jpeg\" alt=\"Test4_automatischeBelichtung\" data-base62-sha1=\"2PF6lBAfqnzvNgheiq68i8SUuhn\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13db62975d493f19c1e19fd75529c558027079cd_2_1334x1000.jpeg 2x\" data-dominant-color=\"1E0F26\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Test4_automatischeBelichtung</span><span class=\"informations\">1388\u00d71040 72.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e.jpeg\" data-download-href=\"/uploads/short-url/cwc2CemlbbZzwOa5ku77OlLnHNc.jpeg?dl=1\" title=\"Test3_AlexaFluor250\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_667x500.jpeg\" alt=\"Test3_AlexaFluor250\" data-base62-sha1=\"cwc2CemlbbZzwOa5ku77OlLnHNc\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/7/57bdbbca8114628b842b40e6fd79d03f8f38ee2e_2_1334x1000.jpeg 2x\" data-dominant-color=\"28141F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Test3_AlexaFluor250</span><span class=\"informations\">1388\u00d71040 87.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428.jpeg\" data-download-href=\"/uploads/short-url/l3ffxwNBcvPkRyQa592OUQ0wpKw.jpeg?dl=1\" title=\"gro\u00df2_automatischeBelichtung\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_667x500.jpeg\" alt=\"gro\u00df2_automatischeBelichtung\" data-base62-sha1=\"l3ffxwNBcvPkRyQa592OUQ0wpKw\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_667x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_1000x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/3/938b961ee58e9020f0a9e8fe9e4961479e337428_2_1334x1000.jpeg 2x\" data-dominant-color=\"160B20\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">gro\u00df2_automatischeBelichtung</span><span class=\"informations\">1388\u00d71040 48.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Not familiar with cellprofiler, but if it has edge detection, running that in the membrane channel might generate a good binary image. Possibly through ImageJ integration if nothing else.</p>", "<p>An other option would be to install Cellpose plugin for Cellprofiler.<br>\nHowever that plugin doesn\u2019t work with the standalone Cellprofiler. You would need to install Cellprofiler from github repository.<br>\n(Check here: <a href=\"https://forum.image.sc/t/new-cellprofiler-4-plugin-runcellpose/56858\" class=\"inline-onebox\">New CellProfiler 4 Plugin: RunCellpose</a>)</p>\n<p>If all your images are like those you shared here <img src=\"https://emoji.discourse-cdn.com/twitter/star_struck.png?v=12\" title=\":star_struck:\" class=\"emoji\" alt=\":star_struck:\" loading=\"lazy\" width=\"20\" height=\"20\"> It should give very very accurate segmentation.</p>"], "60721": ["<p>Hi,</p>\n<p>I\u2019m trying to analyze some 40,628 images where the pipeline identifies the object, makes measurements, and then exports said measurements to a .csv. The program keeps failing around the 1300 image mark where the UI stops updating. I suspect it has something to do with the warning at the beginning regarding large datasets being exported to .csv, but I had the same problem when using the ExportToDatabase module. Any advice or workarounds would be greatly appreciated.</p>\n<p>Best,<br>\nAlex</p>", "<p>Here\u2019s a snippet of the error log I managed to copy down. From what I can tell, the GUI fails to update and subsequently on the last step, the data can\u2019t enter post-processing that would allow the saving of the csv file. The data continues to run once the GUI freezes, but once all the images have been analyzed, the final csv never gets exported even if all the data is processed.</p>\n<p>&lt;traceback object at 0x0000017E982DB540&gt;<br>\nTraceback (most recent call last):<br>\nFile \u201ccellprofiler\\gui\\pipelinecontroller.py\u201d, line 2965, in module_display_request<br>\nFile \u201ccellprofiler\\modules\\identifyprimaryobjects.py\u201d, line 1606, in display<br>\nFile \u201ccellprofiler\\gui\\figure_figure.py\u201d, line 2220, in subplot_table<br>\nwx._core.wxAssertionError: C++ assertion \u201cm_hdc\u201d failed at \u2026\\src\\msw\\textmeasure.cpp(64) in wxTextMeasure::BeginMeasuring(): Must not be used with non-native wxDCs<br>\nWorker 5: Sun Dec  5 23:00:24 2021: Image # 149, module IdentifyPrimaryObjects # 7: CPU_time = 0.03 secs, Wall_time = 10.09 secs<br>\n&lt;traceback object at 0x0000017E965B0840&gt;<br>\nTraceback (most recent call last):<br>\nFile \u201ccellprofiler\\gui\\pipelinecontroller.py\u201d, line 2965, in module_display_request<br>\nFile \u201ccellprofiler\\modules\\exporttospreadsheet.py\u201d, line 792, in display<br>\nFile \u201ccellprofiler\\gui\\figure_figure.py\u201d, line 2220, in subplot_table<br>\nwx._core.wxAssertionError: C++ assertion \u201cm_hdc\u201d failed at \u2026\\src\\msw\\textmeasure.cpp(64) in wxTextMeasure::BeginMeasuring(): Must not be used with non-native wxDCs<br>\nWorker 1: Sun Dec  5 23:00:29 2021: Image # 144, module ExportToSpreadsheet # 12: CPU_time = 0.00 secs, Wall_time = 6.24 secs</p>", "<p>Hi Alex,</p>\n<p>When analyzing this many images, we often find that cellprofiler crashes on CSV export.  However, it looks like you probably have an offending image set that you need to identify and then figure out why it is crashing the pipeline.</p>\n<p>We have two strategies that we use that may help you:</p>\n<ul>\n<li>\n<strong>Export in chunks (usually by PlateID)</strong>.  If you are extracting metadata, you have access to that for naming the folder the csv file is saved in the export to spreadsheet module.</li>\n</ul>\n<p>Output file location \u2192 select \u201cDefault Output Folder sub-folder\u201d, and then right-click and select your plateID metadata (see attached image).  This will split the CSV output into separate folders by plate and usually greatly improves the success of the output.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/ca32de3d27da6a976190a7aed81e82faff3ed924.jpeg\" data-download-href=\"/uploads/short-url/sQJqShBOMsU98w9owYpivtrV2xm.jpeg?dl=1\" title=\"CP_exptcsv\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca32de3d27da6a976190a7aed81e82faff3ed924_2_690x241.jpeg\" alt=\"CP_exptcsv\" data-base62-sha1=\"sQJqShBOMsU98w9owYpivtrV2xm\" width=\"690\" height=\"241\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca32de3d27da6a976190a7aed81e82faff3ed924_2_690x241.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca32de3d27da6a976190a7aed81e82faff3ed924_2_1035x361.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/ca32de3d27da6a976190a7aed81e82faff3ed924.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/a/ca32de3d27da6a976190a7aed81e82faff3ed924_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">CP_exptcsv</span><span class=\"informations\">1141\u00d7400 44.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<ul>\n<li>\n<strong>Export to database</strong> (a better approach) - and export to a SQLite file, then read the SQLlite file in Knime and export it to CSV.</li>\n</ul>\n<p>The export to database module writes continuously to the SQLite file so even if it crashes, you can still pick up where you left off or this will help you to find the offending image set and fix the problem or skip that image set.</p>\n<p>In CellProfiler \u2192 ExportToDatabase \u2192 Database type \u2192 SQLite \u2192 and give it a file name.</p>\n<p>When the run finishes, use <strong>Knime</strong> to read the SQLite file, select the <strong>Image</strong> table (if you want image level averages) or <strong>Object</strong> table (if you want cell level data), read the database and then write to CSV file.  This is super easy and a screen shot of the 4 nodes are shown below.  Just configure the SQL1ite Connector to read the db file, select the table in DB Table Selector and then read the table in DB Reader.  That may take a few minutes for a large run with object level data where the database file is in the GB range.  The CSV writer will then write out the table to CSV and you can proceed with analysis.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/928f9bd9bedf9e2edd2d70b81518373f3b0d9513.jpeg\" data-download-href=\"/uploads/short-url/kUxofowO21u32qaSG6TYErfCqST.jpeg?dl=1\" title=\"CP_knime\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/2/928f9bd9bedf9e2edd2d70b81518373f3b0d9513_2_690x170.jpeg\" alt=\"CP_knime\" data-base62-sha1=\"kUxofowO21u32qaSG6TYErfCqST\" width=\"690\" height=\"170\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/2/928f9bd9bedf9e2edd2d70b81518373f3b0d9513_2_690x170.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/928f9bd9bedf9e2edd2d70b81518373f3b0d9513.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/2/928f9bd9bedf9e2edd2d70b81518373f3b0d9513.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/2/928f9bd9bedf9e2edd2d70b81518373f3b0d9513_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">CP_knime</span><span class=\"informations\">732\u00d7181 26.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>You can also read this SQLite file in Python, R, Orange, JMP, etc.</p>\n<p>We prefer this method as it works best with large datasets as the export to CSV is failure-prone.</p>\n<p>Best,</p>\n<p>Jonny Sexton</p>", "<p>Hi,</p>\n<p>Thank you for the quick response. I received a similar failure message with the ExportToDatabase when I last tried it - just to confirm: if the GUI stops updating (progress bar), is the SQL file still updating?</p>\n<p>Additionally, I\u2019m unsure if it\u2019s an offending image set that\u2019s causing the problem. Since hitting this error, I\u2019ve been chunking the data set 1000 images at a time and have already passed the 1300 mark where the previous complete image set failed.</p>\n<p>Best,<br>\nAlex</p>", "<p>You should be able to tell because the .db file will be updating and increasing in size.</p>\n<p>Also, you should still see progress for workers across modules in the command window.</p>\n<p>Here are a few other things you can try:</p>\n<ul>\n<li>\n<p>Reduce the number of workers - File \u2192 Preferences and set the maximum number of workers in half to see if that helps.  If you have too many workers and lots of objects, this can make things unstable.</p>\n</li>\n<li>\n<p>You can also try to run \u201cheadless\u201d from the command line but that will only run one worker per command, so you can start several jobs and give each one a start and stop image set.</p>\n</li>\n</ul>\n<p>Here is Beth\u2019s recent post on headless: <a href=\"https://carpenter-singh-lab.broadinstitute.org/blog/getting-started-using-cellprofiler-command-line\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Getting started using CellProfiler from the command line | Carpenter-Singh Lab</a></p>\n<p>Hope this helps,</p>\n<p>Jonny</p>", "<p>Thanks so much, I\u2019ll give it a test run with 2000 images and let you know how it goes.</p>\n<p>Best,<br>\nAlex</p>", "<p>Hi,</p>\n<p>So I managed to get a functional database. Thank you very much. However is there a way to also save the image file names the way you would in the ExportToSpreadSheet module? I use the filenames as a point of reference for downstream analysis and I can\u2019t seem to find such an option in ExportToDatabase.</p>\n<p>Best,<br>\nAlex</p>", "<p>Hello,</p>\n<p>I have a pipeline that applies <strong>illumination correction</strong> functions to a subset of the <em>CPJUMP1</em> dataset, which consists of around 60,000 images across 5 channels. The pipeline groups the images by their <em>plateID</em> and saves them in their respective plate folder within the output directory. I set the <code>worker-counts</code> to <code>16</code>, which is the maximum number for my setup. However, despite the pipeline still running and exporting the images, I encountered the same exception:</p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"wx\\core.py\", line 3383, in &lt;lambda&gt;\n  File \"cellprofiler\\gui\\preferences_view\\_preferences_view.py\", line 368, in on_pipeline_progress\n  File \"cellprofiler\\gui\\preferences_view\\_progress_watcher.py\", line 65, in on_pipeline_progress\n  File \"cellprofiler\\gui\\preferences_view\\_progress_watcher.py\", line 98, in on_receive_work\n  File \"cellprofiler\\gui\\preferences_view\\_progress_watcher.py\", line 54, in update_multiprocessing\n  File \"cellprofiler\\gui\\preferences_view\\_preferences_view.py\", line 391, in update_progress\nwx._core.wxAssertionError: C++ assertion \"m_hdc\" failed at ..\\..\\src\\msw\\textmeasure.cpp(64) in wxTextMeasure::BeginMeasuring(): Must not be used with non-native wxDCs\n</code></pre>\n<p>I have two questions regarding this issue:</p>\n<ol>\n<li>Is there a solution to this error?</li>\n<li>Is this a significant problem, and do I need to run the pipeline from the beginning? In other words, are the images already exported, corrupted, or affected in any way?</li>\n</ol>"], "80691": ["<p>Hello fellow CellProfiler users,</p>\n<p>We have been utilizing CellProfiler for a number of years and have processed upwards of a billion cell images through our pipeline. I am eager to engage in a high-level discussion regarding the characteristics of an optimally designed analysis pipeline.</p>\n<p>At present, we perform over 500 feature extractions on our multichannel segmented cell images. However, we have encountered significant variability across our features over time. When comparing all features to the global average, we observe that some features deviate by several thousand percent.</p>\n<p>It is worth noting that we did not manually craft each feature extraction, leading me to question the quality of the data generated by the algorithms. Given the sheer number of features, it\u2019s challenging to monitor the performance of each individual feature. I recognize that variations and errors are inevitable in any imaging system, but when I previously created my own analysis tools using OpenCV, I often needed to apply new preprocessing/filters to avoid major corruption of data outputs.</p>\n<p>With this context, I would like to learn more about the processes others use for running and maintaining a CellProfiler analysis pipeline. Can we trust the built-in feature extractions to intelligently handle large variations in image data over time?</p>\n<p>In short, how do you run CellProfiler as an analysis tool?</p>\n<p>Looking forward to your insights and experiences!</p>"], "79671": ["<p>Hello Everyone,</p>\n<p>I implemented the cell segment function of cellpose 2.2 in my web project.<br>\nBut I am not sure of the meaning of channel input in cellpose.<br>\nAs all of you know, there are 2 channels in cellpose input parameters----chan and --chan2.<br>\nI used the cellpose command line for cell segmentation and it worked properly except for the --chan parameter.<br>\nWhenever I change this parameter, I am not sure what is changed from the result image.<br>\nWishing you\u2019re doing well, I am waiting for your help.</p>\n<p>Thanks.<br>\nJames<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7.jpeg\" data-download-href=\"/uploads/short-url/kujbWkGoDTC7ugyvwr7JHJF8ytF.jpeg?dl=1\" title=\"result\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7_2_690x346.jpeg\" alt=\"result\" data-base62-sha1=\"kujbWkGoDTC7ugyvwr7JHJF8ytF\" width=\"690\" height=\"346\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7_2_690x346.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7_2_1035x519.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/f/8f98960dfa95f7f0d09a6c5cc887dcfdbe5ea3a7.jpeg 2x\" data-dominant-color=\"332D33\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">result</span><span class=\"informations\">1221\u00d7614 115 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Have you tried the documentation? <a href=\"https://cellpose.readthedocs.io/en/latest/command.html\" class=\"inline-onebox\">Command line \u2014 cellpose 0.7.2 documentation</a><br>\nYou haven\u2019t stated what you used the parameter for so it isn\u2019t really possible to answer your question.</p>", "<p>Hi <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a></p>\n<p>Thank you for your kind reply.<br>\nCertainly I tried the documentation but I can\u2019t understand enough because  the documentation is quite simple and didn\u2019t contain the detailed information(In my personal opinion I am sorry).<br>\nI want to know the general meaning of --chan and --chan2.<br>\nCould you please explain this?</p>\n<p>Thanks again.<br>\nJames</p>", "<p>Not really any better than what is already there. Chan is the channel to segment, chan2 is optional and usually expecting nucleus data.</p>\n<blockquote>\n<h2>Channels<a href=\"https://cellpose.readthedocs.io/en/latest/settings.html#channels\">\uf0c1</a>\n</h2>\n<p>There are two channels inputs. The first channel is the channel you want to segment. The second channel is an optional channel that is helpful in models trained with images with a nucleus channel. See more details in the models page.</p>\n<ol>\n<li>0=grayscale, 1=red, 2=green, 3=blue</li>\n<li>0=None (will set to zero), 1=red, 2=green, 3=blue</li>\n</ol>\n<p>Set channels to a list with each of these elements, e.g. <code>channels = [0,0]</code> if you want to segment cells in grayscale or for single channel images, or <code>channels = [2,3]</code> if you green cells with blue nuclei.</p>\n<p>On the command line the above would be <code>--chan 0 --chan2 0</code> or <code>--chan 2 --chan2 3</code>.</p>\n</blockquote>", "<p>Hi <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a></p>\n<p>Thank you for your reply.<br>\nIf so, why can\u2019t we find some differences from above two results images?<br>\nThey have different channel and I confirmed that the input channel parameters applied properly.</p>", "<p>You haven\u2019t said what you did to create the two images, so it\u2019s not really possible to say.</p>\n<p>You might have a two channel image and put either the first or second, (red or green?) channel into the chan input. Or you might have put red into chan and green into chan2. Or maybe put red into chan1 and then green into chan2, but something always goes into chan, so it isn\u2019t clear what you are doing for the second image in that case. Maybe there is noting in --chan2 in the first image.</p>\n<p>You might have an RGB image that has red green and blue channels? Or just red and green? Your background looks grayscale, maybe there isn\u2019t an RGB image at all?</p>\n<p>Nothing appears to be different in the images, but it isn\u2019t clear what you did, so I don\u2019t think anyone on the forum can respond usefully.</p>", "<aside class=\"quote no-group\" data-username=\"JamesWang80428\" data-post=\"5\" data-topic=\"79671\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jameswang80428/40/68768_2.png\" class=\"avatar\"> James Wang:</div>\n<blockquote>\n<p>If so, why can\u2019t we find some differences from above two results images?</p>\n</blockquote>\n</aside>\n<p>Maybe provide the two images? If your image is grayscale, then you are feeding it two channels with identical values, so it would make sense that you do not get a difference.</p>\n<aside class=\"quote no-group\" data-username=\"JamesWang80428\" data-post=\"3\" data-topic=\"79671\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jameswang80428/40/68768_2.png\" class=\"avatar\"> James Wang:</div>\n<blockquote>\n<p>the documentation is quite simple and didn\u2019t contain the detailed information</p>\n</blockquote>\n</aside>\n<p>What is detailed information? What question are you asking, specifically?</p>\n<p>In the paper, the following is stated:</p>\n<blockquote>\n<p><strong>Deep neural network</strong>. The input to the neural network was a two-channel image<br>\nwith the primary channel corresponding to the cytoplasmic label, and the optional<br>\nsecondary channel corresponding to nuclei, which in all cases was a DAPI stain.<br>\nWhen a second channel was not available, it was replaced with an image of zeros.<br>\nChannel 2 is an optional channel</p>\n</blockquote>\n<p>Meaning: If you have single channel data, channel 2 should not be provided and is empty (all zeroes).<br>\nSeeing as cellpose was made in order to obtain semi-arbitrary shaped objects, like cells (not nuclei), if you have two channels where one is the nuclei (DAPI) and the other represents the outline or entirety of your cells (HCS-CellMask, Phalloidin, etc\u2026) then:</p>\n<ul>\n<li>\n<code>chan</code> should be the channel that represents your cell</li>\n<li>\n<code>chan2</code> should point to your nuclear channel</li>\n</ul>\n<p>The reason the descripion seems vauge is that their documentation is about the specific case of RGB images, which does not happens when you use fluorescence.</p>\n<p>Case 1:  RGB image where the cell nuclei are strong in the red channel (1) and the cell outline is quite visible in the green channe (2)l:</p>\n<ul>\n<li>Settings would be <code>--chan 1 --chan2 2</code> meaning \u201cuse the red channel and use the optional blue channel for help in segmenting the cells\u201d</li>\n</ul>\n<p>Case 2: RGB image where the cells are roughly visible in all channels</p>\n<ul>\n<li>Settings would be <code>--chan 0 --chan2 0</code> meaning \u201cuse the average of all 3 channels to segment cells and do not use the second channel\u201d</li>\n</ul>\n<p>Case 3: 2 channel image where channel 1 contains the cells, and channel 2 contains the nuclei</p>\n<ul>\n<li>Settings would be : <code>--chan 1 --chan2 2</code> meaning \u201cuse the first channel for finding cells, aided by the second channel which contains nuclei\u201d</li>\n</ul>\n<p>Case 4: A single channel image</p>\n<ul>\n<li>Settings would be nothing, which would default to  <code>--chan 0 --chan2 0</code>\n</li>\n</ul>\n<p>If there are other cases not covered here, feel free to provide an example situation so that we can talk about it.<br>\nBest</p>\n<p>Oli</p>", "<p>Hi <a class=\"mention\" href=\"/u/oburri\">@oburri</a></p>\n<p>Thank you very much for your kindness.<br>\nI understood what you mean and that was really what I wanted to know.</p>\n<blockquote>\n<p>Meaning: If you have single channel data, channel 2 should not be provided and is empty (all zeroes).<br>\nSeeing as cellpose was made in order to obtain semi-arbitrary shaped objects, like cells (not nuclei), if you have two channels where one is the nuclei (DAPI) and the other represents the outline or entirety of your cells (HCS-CellMask, Phalloidin, etc\u2026) then:</p>\n<ul>\n<li>\n<code>chan</code> should be the channel that represents your cell</li>\n<li>\n<code>chan2</code> should point to your nuclear channel</li>\n</ul>\n<p>The reason the descripion seems vauge is that their documentation is about the specific case of RGB images, which does not happens when you use fluorescence.</p>\n</blockquote>\n<p>Thank you very much.<br>\nAnd thanks everyone for teaching me.</p>"], "74551": ["<p><a class=\"hashtag\" href=\"/c/image-analysis/6\">#<span>image-analysis</span></a>  Hello everone,</p>\n<p>We were working on a course with students with cell profiler and some of the students have this error in their PC when the pipeline tries to save the segmented images. We tried to address it by changing the path file to save somewhere else but it does not seem to work. Does any one know how to solve this?<br>\nI attached the error and the pipeline here.</p>\n<p>Cheers,<br>\nPhani,</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/tXztnjHyofua22LWbbkXIZJTRgv.cppipe\">10. Pipeline_2ConditionsFeaturesExtraction.cppipe</a> (19.3 KB)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2e53c238827999bf27631063334668b4ce5de470.png\" data-download-href=\"/uploads/short-url/6BPpMewg2Q35AJUXQdoXhk2DX1u.png?dl=1\" title=\"MicrosoftTeams-image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e53c238827999bf27631063334668b4ce5de470_2_690x388.png\" alt=\"MicrosoftTeams-image\" data-base62-sha1=\"6BPpMewg2Q35AJUXQdoXhk2DX1u\" width=\"690\" height=\"388\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e53c238827999bf27631063334668b4ce5de470_2_690x388.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e53c238827999bf27631063334668b4ce5de470_2_1035x582.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2e53c238827999bf27631063334668b4ce5de470_2_1380x776.png 2x\" data-dominant-color=\"E7E9E9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">MicrosoftTeams-image</span><span class=\"informations\">1920\u00d71080 407 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "77114": ["<p>Hello there,</p>\n<p>I am totally new to Cellprofiler (like started this week new), and I am working on this image to covert after threshold to a B&amp;W mask, to apply skeletonize.<br>\nMy problem this moment is, skeletonize recognizes foreground as background.<br>\nAnd I honestly don\u2019t know where I can change this setting?</p>\n<p>So far my pipeline relies on<br>\n-ColorToGrey<br>\n-IdentifyPrimaryObjects<br>\n-IdentifySecondaryObjects<br>\n-Threshold<br>\n-MorphologicalSkeleton</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/b/dbadcc46f30b3e61d01c8c7af41bf99ae607055a.png\" alt=\"image\" data-base62-sha1=\"vlmVrdrKlCZCKf9PZYtVqQimVqW\" width=\"678\" height=\"296\"><br>\nThis is the application of the threshold, the definition of the objects is yet to be perfect, but not terrible. And in the threshold image, in black is what I consider the cells, i.e. foreground.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/1/5192ebe864059c98031c24190970906c7844e575.png\" alt=\"image\" data-base62-sha1=\"bDDs5kA0S9Uit0JSX0anUl6GuNL\" width=\"644\" height=\"270\"><br>\nBut then, skeletonize reads the white, which I consider background.</p>\n<p>Can someone help me? I Can\u2019t seem to find how to change these settings.</p>\n<p>Thanks in advance,<br>\nSandra</p>", "<p>Hi <a class=\"mention\" href=\"/u/simarques\">@SIMarques</a></p>\n<p>Welcome to CellProfiler!</p>\n<p>In this case you can add the module ImageMath after your Threshold module and choose the operation invert to invert your image.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/5/9505d011d422de824f5f70b5504d6889d051064f.jpeg\" data-download-href=\"/uploads/short-url/lgjB0uPNH80Y2KHILhzJm66QzIr.jpeg?dl=1\" title=\"Screen Shot 2023-02-10 at 8.19.59 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/5/9505d011d422de824f5f70b5504d6889d051064f_2_690x374.jpeg\" alt=\"Screen Shot 2023-02-10 at 8.19.59 AM\" data-base62-sha1=\"lgjB0uPNH80Y2KHILhzJm66QzIr\" width=\"690\" height=\"374\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/5/9505d011d422de824f5f70b5504d6889d051064f_2_690x374.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/5/9505d011d422de824f5f70b5504d6889d051064f_2_1035x561.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/5/9505d011d422de824f5f70b5504d6889d051064f_2_1380x748.jpeg 2x\" data-dominant-color=\"EEEEEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-10 at 8.19.59 AM</span><span class=\"informations\">1920\u00d71043 70.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>After you can use the output of ImageMath to perform the MorphologicalSkeleton.</p>\n<p>Hope this work,</p>\n<p>Best<br>\nMario</p>", "<p>Thank you very much for your help! It realy did it!</p>"], "77629": ["<p>We\u2019re investigating a long-term project to start using WIPP for image analysis. Currently we think that it has excellent potential, but it would probably be too steep a learning curve at present to be used more widely by lab-based researchers who are used to using tools such as PerkinElmer analysis pipelines (in Harmony and Columbus) or ImageJ plugins without needing to write any code. For example, we think a GUI to convert custom ImageJ macros into the format needed to import as a WIPP plugin would be helpful. Would anyone else be interested in a project to improve user-friendliness of WIPP?</p>\n<p>Any thoughts or suggestions would be welcome!</p>\n<p>Thanks,</p>\n<p>Sam Braithwaite</p>"], "76093": ["<p>Hi everyone,</p>\n<p>I am a bioinformatician new for what concern Digital Pathology and Bioimages analysis.</p>\n<p>I am wondering how could I distinguish between tumor and normal cells of a WSI in .mrxs format.</p>\n<p>I know that QuPath and Ilastik are two amazing tools for this kind of tasks, but I wanted to know exactly the steps that I should follow especially because .mrxs format is not that nice to handle.</p>\n<p>I wanted to know if someone can suggest me a tutorial or something like this in order to analyze these images.</p>\n<p>If someone knows a more appropriate tool or algorithm, I am more than happy for suggestions.</p>\n<p>Thank you for availability,</p>\n<p>Bests,</p>\n<p>Chiara</p>", "<p>I would recommend trying QuPath. They have a bunch of tutorials available here: <a href=\"https://qupath.readthedocs.io/en/0.4/docs/tutorials/index.html\" class=\"inline-onebox\">Tutorials \u2014 QuPath 0.4.1 documentation</a> In particular the Cell Classification and Pixel Classification tutorials might be useful to help you distinguish between regular tissue and tumor.</p>", "<p>Additionally, if the mrxs files are saved using jpgxr or similar, you still may want to convert them away from mrxs. There are a few options you can find on the forum.</p>", "<p>Very thank you for the fast reply!<br>\nDo you have any suggestion for a tool to convert mrxs images in the right format for Qupath?</p>\n<p>And which is the best format to run this kind of analysis?</p>", "<p>Very thank you for the answer!<br>\nI will try to explore Qupath documentation and see how to perform this task.</p>\n<p>Additionally, I would like to know how to identify biomarker presence analyzing the slide.</p>\n<p>Do you have any suggestion for some tools/scripts?</p>", "<p><a href=\"https://www.glencoesoftware.com/blog/2019/12/09/converting-whole-slide-images-to-OME-TIFF.html\">This blog post</a> details a method for converting .mrxs into OME-TIFF, which is supported by QuPath. You can also check out<a href=\"https://forum.image.sc/t/export-mrxs-file-to-tiff-jpeg/38945/4\"> this forum post</a> for a way to do it in Python using the pyvips library.</p>", "<aside class=\"quote no-group\" data-username=\"Mark_Zaidi\" data-post=\"6\" data-topic=\"76093\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/mark_zaidi/40/37001_2.png\" class=\"avatar\"> Mark Zaidi:</div>\n<blockquote>\n<p><a href=\"https://www.glencoesoftware.com/blog/2019/12/09/converting-whole-slide-images-to-OME-TIFF.html\">This blog post</a> details a method for converting .mrxs into OME-TIFF, which is supported by QuPath.</p>\n</blockquote>\n</aside>\n<p>Although note that some .mrxs images work in QuPath without any conversion (thanks to OpenSlide) \u2013 see <a href=\"https://qupath.readthedocs.io/en/0.4/docs/intro/formats.html#mrxs-3d-histech\">here</a>.</p>\n<p>It\u2019s worth a try anyway.</p>", "<p>I\u2019m not sure I\u2019d recommend using pyvips like that \u2014 conversion will be slow, and the image will take forever to load into QuPath.</p>\n<p>You can make a nice OME-TIFF with pyvips like this:</p>\n<aside class=\"quote quote-modified\" data-post=\"14\" data-topic=\"51223\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/jcupitt/40/42678_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/writing-qupath-bio-formats-compatible-pyramidal-image-with-libvips/51223/14\">Writing QuPath/Bio-Formats compatible pyramidal image with libvips</a> <a class=\"badge-wrapper  bullet\" href=\"/c/development/5\"><span class=\"badge-category-bg\" style=\"background-color: #F7941D;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for development (i.e., programming) questions about scientific image software.\">Development</span></a>\n  </div>\n  <blockquote>\n    Here\u2019s a slightly updated conversion script for libvips 8.14: \n#!/usr/bin/python3\n\nimport sys\nimport pyvips\n\n# the \"rgb=True\" will make openslide drop the alpha from the image, saving a\n# useful chunk of memory\nim = pyvips.Image.new_from_file(sys.argv[1], rgb=True)\n\nimage_height = im.height\n\n# split to separate image planes and stack vertically ready for OME \nim = pyvips.Image.arrayjoin(im.bandsplit(), across=1)\n\n# set minimal OME metadata\n# before we can modify an image (set metadata in this ca\u2026\n  </blockquote>\n</aside>\n\n<p>That will convert any slide that openslide can load into a file that will load very quickly in QuPath. Conversion should be pretty quick and only use a little memory.</p>", "<p>Yes my experience is that .mrxs files can work in QuPath without any conversion. <a class=\"mention\" href=\"/u/frasca17\">@frasca17</a> , if you\u2019d like to see file structure for a project where this worked, try checking out <a href=\"https://carpenter-singh-lab.broadinstitute.org/blog/how-export-tiles-large-histology-images-qupath\">this blogpost</a>.</p>", "<p>Yup, had no issue opening the already pyramided CMU-1 test image from <a href=\"https://openslide.cs.cmu.edu/download/openslide-testdata/Mirax/\" class=\"inline-onebox\">MIRAX (MRXS)</a> without conversion, and using OpenSlide as the server</p>", "<p>The issue is specifically with certain sets of mrxs files. JPEGXR encoding, 16bit depth, and I think Z-stacks all break the Openslide server.</p>", "<p>Ah, I see. Opening the<a href=\"https://openslide.cs.cmu.edu/download/openslide-testdata/Mirax/Mirax2-Fluorescence-2.zip\"> Mirax2-Fluorescence2</a> sample image (8-bit, regular JPEG compression) also results in some bizzare rendering of the IF images as brightfield:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/04e15b6944b06caf1d16f82b984950ad74259ef0.jpeg\" data-download-href=\"/uploads/short-url/HaJA3WF1adpa99lLyT5BVeCSw8.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/04e15b6944b06caf1d16f82b984950ad74259ef0_2_690x434.jpeg\" alt=\"image\" data-base62-sha1=\"HaJA3WF1adpa99lLyT5BVeCSw8\" width=\"690\" height=\"434\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/04e15b6944b06caf1d16f82b984950ad74259ef0_2_690x434.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/04e15b6944b06caf1d16f82b984950ad74259ef0_2_1035x651.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/04e15b6944b06caf1d16f82b984950ad74259ef0_2_1380x868.jpeg 2x\" data-dominant-color=\"85787E\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1514\u00d7954 290 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nAlso reminds me of this MRXS-JPEGXR issue: <a href=\"https://github.com/openslide/openslide/issues/184\" class=\"inline-onebox\">MRXS with JPEGXR tiles \u00b7 Issue #184 \u00b7 openslide/openslide \u00b7 GitHub</a>. Found the final comment to be quite interesting:</p>\n<blockquote>\n<p>just FYI, I gave up. I recommend people buying machines that support open standards.</p>\n</blockquote>", "<p>Yep, the 4 channel images also need to be restructured/exported to make them read as 4 channel. 3 channel are <em>usually</em> fine. Ancient history and I only vaguely remember the process using some Caseviewer associated software.</p>\n<p>Wow, them\u2019s some tiling artifacts though. I can\u2019t believe that was a used sample image.</p>", "<p><a class=\"mention\" href=\"/u/jcupitt\">@jcupitt</a> I\u2019m having a bit of trouble running a modified form of your script on one of the Openslide test images ( <a href=\"https://openslide.cs.cmu.edu/download/openslide-testdata/Mirax/Mirax2-Fluorescence-2.zip\">Mirax2-Fluorescence-2.zip</a>). My goal was to convert the above .mrxs file into an ome.tiff with correctly displayed channels, however the conversion took quite long, presumably due to all the padding generated as shown below:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/6/f678db5eb021f49eec7566eabba3b87e249e1f88.png\" data-download-href=\"/uploads/short-url/zaookFrShg8Y2cBpH74g3LjnZlC.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/6/f678db5eb021f49eec7566eabba3b87e249e1f88_2_690x365.png\" alt=\"image\" data-base62-sha1=\"zaookFrShg8Y2cBpH74g3LjnZlC\" width=\"690\" height=\"365\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/6/f678db5eb021f49eec7566eabba3b87e249e1f88_2_690x365.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/6/f678db5eb021f49eec7566eabba3b87e249e1f88_2_1035x547.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/6/f678db5eb021f49eec7566eabba3b87e249e1f88_2_1380x730.png 2x\" data-dominant-color=\"373535\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71017 39.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nHere\u2019s the modified code I used, adapted from <a href=\"https://forum.image.sc/t/writing-qupath-bio-formats-compatible-pyramidal-image-with-libvips/51223/14\">this</a> to run in a Python IDE like Spyder:</p>\n<pre><code class=\"lang-auto\"># -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Jan 26 11:13:18 2023\n\n@author: Mark Zaidi\n\ninstall pyvips via non conda method: https://pypi.org/project/pyvips/\nmrxs info from sample images: https://openslide.cs.cmu.edu/download/openslide-testdata/Mirax/\n\"\"\"\n#%% load libraries and set constants\nimport os\nvipshome = r'E:\\Img_storage\\Tutorials\\pyvips testing\\vips-dev-8.14\\bin'\nos.environ['PATH'] = vipshome + ';' + os.environ['PATH']\ninput_image=r'E:\\Img_storage\\Tutorials\\pyvips testing\\image\\Mirax2-Fluorescence-2\\Mirax2-Fluorescence-2.mrxs'\noutput_image=r'E:\\Img_storage\\Tutorials\\pyvips testing\\image\\mirax2_ometiff\\Mirax2-Fluorescence-2.ome.tiff'\n#pyvips must be loaded after the environment has vips-dev-8.14\\bin in it's path\nimport pyvips\n\n#%% load mrxs file\n# the \"rgb=True\" will make openslide drop the alpha from the image, saving a\n# useful chunk of memory\nim = pyvips.Image.new_from_file(input_image, rgb=True)\n\n#%% process image and extract metadata\nimage_height = im.height\n\n# split to separate image planes and stack vertically ready for OME \nim = pyvips.Image.arrayjoin(im.bandsplit(), across=1)\n\n# set minimal OME metadata\n# before we can modify an image (set metadata in this case), we must take a \n# private copy\nim = im.copy()\nim.set_type(pyvips.GValue.gint_type, \"page-height\", image_height)\nim.set_type(pyvips.GValue.gstr_type, \"image-description\",\nf\"\"\"&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;OME xmlns=\"http://www.openmicroscopy.org/Schemas/OME/2016-06\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://www.openmicroscopy.org/Schemas/OME/2016-06 http://www.openmicroscopy.org/Schemas/OME/2016-06/ome.xsd\"&gt;\n    &lt;Image ID=\"Image:0\"&gt;\n        &lt;!-- Minimum required fields about image dimensions --&gt;\n        &lt;Pixels DimensionOrder=\"XYCZT\"\n                ID=\"Pixels:0\"\n                SizeC=\"3\"\n                SizeT=\"1\"\n                SizeX=\"{im.width}\"\n                SizeY=\"{image_height}\"\n                SizeZ=\"1\"\n                Type=\"uint8\"&gt;\n        &lt;/Pixels&gt;\n    &lt;/Image&gt;\n&lt;/OME&gt;\"\"\")\n#%% Write image\nim.tiffsave(output_image, compression=\"jpeg\", tile=True,\n            tile_width=512, tile_height=512,\n            pyramid=True, subifd=True)\n</code></pre>\n<p><code>image_height</code> does match what\u2019s listed in QuPath (152080 pixels), but I\u2019m unclear as to where it\u2019s obtaining this value from. Seems like it\u2019s padding everything within <code>im.width</code> and <code>im.height</code> with zeroes. And the three RGB channels are still displayed as a pseudo-brightfield image. Here are the warning logs from when I loaded it into QuPath:</p>\n<pre><code class=\"lang-auto\">15:13:22.428 [project-import1] [INFO ] q.l.i.s.b.BioFormatsServerOptions - Setting max Bio-Formats readers to 32\n15:13:22.764 [project-import1] [WARN ] loci.formats.FormatHandler - SamplesPerPixel mismatch: OME=-1, TIFF=1\n15:13:22.764 [project-import1] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #0\n15:13:22.766 [project-import1] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #1\n15:13:22.768 [project-import1] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #2\n15:13:22.768 [project-import1] [WARN ] loci.formats.FormatHandler - Using TiffReader to determine the number of planes.\n15:13:23.139 [project-import2] [WARN ] loci.formats.FormatHandler - SamplesPerPixel mismatch: OME=-1, TIFF=1\n15:13:23.139 [project-import2] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #0\n15:13:23.140 [project-import2] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #1\n15:13:23.140 [project-import2] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #2\n15:13:23.141 [project-import2] [WARN ] loci.formats.FormatHandler - Using TiffReader to determine the number of planes.\n15:13:23.542 [thumbnail-loader1] [WARN ] loci.formats.FormatHandler - SamplesPerPixel mismatch: OME=-1, TIFF=1\n15:13:23.542 [thumbnail-loader1] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #0\n15:13:23.545 [thumbnail-loader1] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #1\n15:13:23.546 [thumbnail-loader1] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #2\n15:13:23.548 [thumbnail-loader1] [WARN ] loci.formats.FormatHandler - Using TiffReader to determine the number of planes.\n15:13:29.261 [JavaFX Application Thread] [WARN ] loci.formats.FormatHandler - SamplesPerPixel mismatch: OME=-1, TIFF=1\n15:13:29.263 [JavaFX Application Thread] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #0\n15:13:29.264 [JavaFX Application Thread] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #1\n15:13:29.265 [JavaFX Application Thread] [WARN ] loci.formats.FormatHandler - Image ID 'Image:0': missing plane #2\n15:13:29.267 [JavaFX Application Thread] [WARN ] loci.formats.FormatHandler - Using TiffReader to determine the number of planes.\n15:13:29.724 [JavaFX Application Thread] [INFO ] qupath.lib.gui.viewer.QuPathViewer - Image data set to ImageData: Not set, Mirax2-Fluorescence-2.ome.tiff\n15:13:29.726 [JavaFX Application Thread] [INFO ] qupath.lib.gui.QuPathGUI - Image type estimated to be Fluorescence\n</code></pre>\n<p>Any advice would be greatly appreciated!</p>", "<p>Hi <a class=\"mention\" href=\"/u/mark_zaidi\">@Mark_Zaidi</a>, yes, that\u2019s an openslide feature. By default it\u2019ll convert the whole of the slide area, not just the part with the actual pixels. Try:</p>\n<pre><code class=\"lang-python\">im = pyvips.Image.new_from_file(input_image, rgb=True, autocrop=True)\n</code></pre>\n<p>The docs for the libvips openslide loader are here:</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://www.libvips.org/API/current/VipsForeignSave.html#vips-openslideload\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/e/fe2ed5976310f1a4e0a3eb03cdbaa5bb00d05d0c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://www.libvips.org/API/current/VipsForeignSave.html#vips-openslideload\" target=\"_blank\" rel=\"noopener nofollow ugc\">libvips.org</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://www.libvips.org/API/current/VipsForeignSave.html#vips-openslideload\" target=\"_blank\" rel=\"noopener nofollow ugc\">libvips</a></h3>\n\n  <p>A fast image processing library with low memory needs.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>You can fetch associated images too, and read out metadata. <strong>edit</strong> updated the page, it should be correct now.</p>", "<p>hello<br>\nare you asking how to handle specific files format and find cells in terms of segmentation algorithms and software?<br>\nor are you asking which markers, which parameters, which data science algos you should use to discriminate cancer cells vs normal cells?</p>", "<p>Hi,<br>\nYes I am.<br>\nDo you have any further suggestion?</p>", "<p>Very thanks for the suggestion!<br>\nI tried using .mrxs images without conversion and QuPath actually worked properly!</p>"], "75587": ["<p>Hello,</p>\n<p>I used cell profiler 4.2.4 to analyze IF images in which Rel A protein was stained with Texas red and nucleus with DAPI in keratinocytes in 0, 25-, 60-, 90- and 120-minutes post two treatments. I first split images into red channel for Rel A and blue for DNA in gray format using FIJI, and then applied cell profiler pipeline I created to measure Rel A intensity in cytoplasm and nucleus on IF images with two channels. I got caught in two issues:</p>\n<ul>\n<li>\n<ol>\n<li>When I checked segmented images, I found that a few images were not segmented correctly and shown in attached images.</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>RelA color was changed to green instead of red and nucleus became red instead of blue when images were imported to cell profiler analyst.</li>\n</ol>\n</li>\n</ul>\n<p>I also attached the files of pipelines for this analysis. Could someone please help me sort out above issues?</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/7A2lb4IWQKWGkqhF79zHklMp0dx.tiff\">BL3LD_RelA_Y+_2h_4r_Overlay.tiff</a> (329.3 KB)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/5PchdATSVzvjmvpZKmhUneZDN77.tiff\">BL3LD_RelA_Y-_1.5h_1r_Overlay.tiff</a> (346.4 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/iVEpUbsaFQGkLETh2ssvGprwh9x.cpproj\">BL3_LD_RELA_NucCytoRatio.cpproj</a> (1.3 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/3GNjd9XctSyiNyPnkzySfMGYBP9.cpproj\">BL3_LD_RELA_NCR_Expt_CPA 122122.cpproj</a> (1.3 MB)</p>", "<p>Hi,</p>\n<p>Can you tell us a little more about \u201cnot segmenting correctly\u201d means? Are there any differences you can identify that are consistent between images that work vs don\u2019t work? In general, you may find our <a href=\"https://youtu.be/26Mzx3ObIYo\">video tutorial on segmentation principles</a> helpful in debugging segmentation problems.</p>\n<p>With regards to color display in CPA, you can just open the properties file in Notepad or any text editor (though I wouldn\u2019t use something like Word) and set the display colors to whatever you want (or just change them in CPA itself while it\u2019s running, though those changes won\u2019t persist); if you want these images set automatically to certain colors going forward, you can in ExportToDatabase set these during analysis by changing \u201cInclude information for all images, using default values?\u201d from \u201cYes\u201d to \u201cNo\u201d and specifying the images you want CPA to show you and the colors for each image.</p>", "<p>Hi Beth,<br>\nI am sorry to miss the email notice. Thank you for answering my questions!</p>\n<p>I will follow your suggestion to correct color change in Image Gallery/CPA.</p>\n<p>Based on what I learned from CellProfiler tutorials including video and papers, I developed the pipelines for image analysis to measure nuclear and cytoplasmic ratio of RELA protein. I think it worked well with majority of images as it could properly identify primary objects-nucleus and secondary objects-cytoplasm in majority of images by mocking with different colors. I pasted a few images as examples. In contrast,  in incorrectly segmented images pasted a week ago, most cells were not seen with a nucleus mocked with pink line and cytoplasm with green boundary. Only quite a few green empty objects with asymmetry shapes were seen in these questionable images. I am trying to sort out why this pipeline did not work with the questionable images in the same way as it worked with other images.</p>\n<p>Bin.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/3TfOv2utpIvJHEntdnjDS0oaJ3R.tiff\">BL3LD_RelA_0h_3r_Overlay.tiff</a> (352.4 KB)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/wVROcfafCLugQgnPRLI0cW5aD43.tiff\">BL3LD_RelA_Y+_2h_1r_Overlay.tiff</a> (372.4 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/kLUP8QLi1BUeVlqZKGjmDb37bZz.tiff\">BL3LD_RelA_0h_2r_Overlay.tiff</a> (357.3 KB)</p>", "<p>Hi Bin,</p>\n<p>Thanks, that helps!</p>\n<p>Briefly, based on the images you sent, it looks like sometimes the threshold being chosen by the algorithm for the nuclear images is much too low - I would compare the threshold value on your well-performing vs your poorly-performing images, you\u2019ll likely see it much lower in the bad ones. If it was failing a lot of the time, I\u2019d suggest changing algorithms, but since you say your current settings work well most of the time, that\u2019s exactly the place where setting a threshold minimum can often be really helpful - see the 3rd chapter (starting at ~41 min) of the video I linked in the last post. I think that should help quite a bit!</p>", "<p>Hi Beth,</p>\n<p>Thank you for your reply!  I apologize for several typing errors in my notes.  I will go back to watch the video and reset the algorithm of the pipeline used for this analysis.</p>\n<p>Another question I had is on CorrectIlluminationApply and CorrectIlluminationCalculate. I also followed the tutorial to set up algorithms for these two modules to remove background on images. However,  all objects (segmented cells) on images were gone and only black background was left after applied with two modules. I also tried different algorithms suggested by the modules and they all did not work. I did not saved algorithms as they were not useful.</p>\n<p>Do you have any general suggestions on how these modules for background correction should be set up to remove background but not segmented cells on images with low background in the case like mine?</p>\n<p>Bin.</p>", "<p>Hi Bin,</p>\n<p>Sorry to have missed this earlier! It\u2019s hard to give specific advice without knowing what you tried, but in general, larger \u201cblock\u201d/smoothing sizes will be \u201cgentler\u201d than smaller ones. Make sure also to always pair rescaled functions with \u201cDivide\u201d in the application, and non-rescaled ones with subtract!</p>"], "77124": ["<p>Hallo,</p>\n<p>i am trying to use CorrectIlluminationCalculate with All to correct the uneven fluorescence signal on my images taken from 96 well plate. And afterwards try to save the function using SaveImages by last cycle. I took same parameter in Bray2016 NatProtocol.</p>\n<p>However, I got following error afte first cycle of calculation.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/e/ee78decc83de9137deefa57911b9fab35ca176d7.png\" alt=\"grafik\" data-base62-sha1=\"y1CBhOjdFPIh4MwCiGrhy3X5oCr\" width=\"464\" height=\"189\"></p>\n<p>And if I disabled all SaveImages, got following error after first cycle of calculation.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/dfd25280a0c922efe82702901e01618e8d30748a.png\" alt=\"grafik\" data-base62-sha1=\"vW15xifPF1NtP8etGQ7Y48nnpXA\" width=\"558\" height=\"204\"></p>\n<p>The following is screenshots of the pipeline, I dont know how to get the whole pipeline in text.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1eaebfb1f5b4bbee3cea6a11e7d279bac70849fd.png\" data-download-href=\"/uploads/short-url/4nqIKfOz6H7SmAqi7qn82zrQ2jH.png?dl=1\" title=\"grafik\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eaebfb1f5b4bbee3cea6a11e7d279bac70849fd_2_690x369.png\" alt=\"grafik\" data-base62-sha1=\"4nqIKfOz6H7SmAqi7qn82zrQ2jH\" width=\"690\" height=\"369\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eaebfb1f5b4bbee3cea6a11e7d279bac70849fd_2_690x369.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eaebfb1f5b4bbee3cea6a11e7d279bac70849fd_2_1035x553.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/e/1eaebfb1f5b4bbee3cea6a11e7d279bac70849fd_2_1380x738.png 2x\" data-dominant-color=\"F0F0F1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">grafik</span><span class=\"informations\">1692\u00d7906 40.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/c/7c336d1cfa284dfaf627392ca5539deda96d729e.png\" data-download-href=\"/uploads/short-url/hIJnXKr9db09mBsTLnWzdj5hhSu.png?dl=1\" title=\"grafik\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c336d1cfa284dfaf627392ca5539deda96d729e_2_690x369.png\" alt=\"grafik\" data-base62-sha1=\"hIJnXKr9db09mBsTLnWzdj5hhSu\" width=\"690\" height=\"369\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c336d1cfa284dfaf627392ca5539deda96d729e_2_690x369.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c336d1cfa284dfaf627392ca5539deda96d729e_2_1035x553.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/c/7c336d1cfa284dfaf627392ca5539deda96d729e_2_1380x738.png 2x\" data-dominant-color=\"F1F1F1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">grafik</span><span class=\"informations\">1692\u00d7906 37.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thank you for your help in advance</p>\n<p>Best<br>\nShu</p>", "<p>The problem was solved.</p>"], "38725": ["<p>The posts about colour deconvolution today have reminded me of a question I had some time ago\u2026</p>\n<p>Color deconvolution, as described by Ruifrok and Johnston, involves generating a 3x3 stain matrix using three stain vectors.</p>\n<p>I understand that if two stains are available, then the remaining elements can be created by generating a third (pseudo)stain that is orthogonal to the first two.</p>\n<p>As far as I can tell, this third stain is generated using the cross product in several places:</p>\n<ul>\n<li>QuPath (<a href=\"https://github.com/qupath/qupath/blob/a03756328188999c0b7f12c290cda0589c50bd4b/qupath-core/src/main/java/qupath/lib/color/StainVector.java#L316\">code</a>)</li>\n<li>scikit-image (<a href=\"https://github.com/scikit-image/scikit-image/blob/23829977f23c957dbff24757069dbdc4d929248f/skimage/color/colorconv.py#L487\">code</a>)</li>\n<li>HistomicsTK (<a href=\"https://github.com/DigitalSlideArchive/HistomicsTK/blob/8e79d265c95468b8ca3f6057c06ce1402bbabaae/histomicstk/preprocessing/color_deconvolution/complement_stain_matrix.py\">code</a>)</li>\n</ul>\n<p>I wasn\u2019t able to tell what approach CellProfiler takes (I figure <a href=\"https://github.com/CellProfiler/CellProfiler/blob/39b812636f97708dffd5eb525cefc2d9f6e4b038/cellprofiler/modules/unmixcolors.py\">this code</a> is relevant, but I got lost <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> ).</p>\n<p>However, I understand that this is not exactly how it is implemented in the <a class=\"mention\" href=\"/u/gabriel\">@gabriel</a>\u2019s ImageJ/Fiji plugin. From a quick look at the code, this may be because of negative values being clipped to 0:<br>\n</p><aside class=\"onebox githubblob\">\n  <header class=\"source\">\n      <a href=\"https://github.com/fiji/Colour_Deconvolution/blob/dca9a107f1d90b2f15dd7fac4e737534984e64f1/src/main/java/sc/fiji/colourDeconvolution/StainMatrix.java#L193\" target=\"_blank\">github.com</a>\n  </header>\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/fiji/Colour_Deconvolution/blob/dca9a107f1d90b2f15dd7fac4e737534984e64f1/src/main/java/sc/fiji/colourDeconvolution/StainMatrix.java#L193\" target=\"_blank\">fiji/Colour_Deconvolution/blob/dca9a107f1d90b2f15dd7fac4e737534984e64f1/src/main/java/sc/fiji/colourDeconvolution/StainMatrix.java#L193</a></h4>\n<pre class=\"onebox\"><code class=\"lang-java\"><ol class=\"start lines\" start=\"183\" style=\"counter-reset: li-counter 182 ;\">\n<li>if (cosx[1]==0.0){ //2nd colour is unspecified</li>\n<li>  if (cosy[1]==0.0){</li>\n<li>    if (cosz[1]==0.0){</li>\n<li>      cosx[1]=cosz[0];</li>\n<li>      cosy[1]=cosx[0];</li>\n<li>      cosz[1]=cosy[0];</li>\n<li>    }</li>\n<li>  }</li>\n<li>}</li>\n<li>\n</li><li class=\"selected\">if (cosx[2]==0.0){ // 3rd colour is unspecified</li>\n<li>  if (cosy[2]==0.0){</li>\n<li>    if (cosz[2]==0.0){</li>\n<li>      if ((cosx[0]*cosx[0] + cosx[1]*cosx[1])&gt; 1){</li>\n<li>        if (doIshow)</li>\n<li>          IJ.log(\"Colour_3 has a negative R component.\");</li>\n<li>        cosx[2]=0.0;</li>\n<li>      }</li>\n<li>      else</li>\n<li>        cosx[2]=Math.sqrt(1.0-(cosx[0]*cosx[0])-(cosx[1]*cosx[1]));</li>\n<li>\n</li></ol></code></pre>\n\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>In any case, the following Groovy script written for Fiji shows that the third stain vector is <em>not</em> orthogonal using the Fiji plugin, i.e. the dot product is not zero:</p>\n<pre><code class=\"lang-groovy\">import sc.fiji.colourDeconvolution.*\ndef mat = new StainMatrix()\nmat.init(\"H&amp;E\", 0.644211000,0.716556000,0.266844000,0.09278900,0.95411100,0.28311100,0.00000000,0.00000000,0.0000000)\nmat.compute(true, false, new ij.ImagePlus(\"Anything\", new ij.process.ColorProcessor(10, 10)))\n\ndouble[] stain1 = [mat.cosx[0], mat.cosy[0], mat.cosz[0]]\ndouble[] stain2 = [mat.cosx[1], mat.cosy[1], mat.cosz[1]]\ndouble[] stain3 = [mat.cosx[2], mat.cosy[2], mat.cosz[2]]\nprintln 'Stain 1: ' + stain1\nprintln 'Stain 2: ' + stain2\nprintln 'Stain 3: ' + stain3\nprintln 'Dot product stain 1 x stain 2: ' + dot(stain1, stain2)\nprintln 'Dot product stain 1 x stain 3: ' + dot(stain1, stain3)\nprintln 'Dot product stain 2 x stain 3: ' + dot(stain2, stain3)\n\n\ndouble dot(double[] v1, double[] v2) {\n\tdouble s = 0\n\tfor (int i = 0; i &lt; v1.length; i++)\n\t\ts += v1[i] * v2[i]\n\treturn s\n}\n</code></pre>\n<p>It\u2019s not completely clear to me that negative values must be avoided in the stain matrix, and that this is more important than orthogonality.</p>\n<p>It\u2019s also not clear to me if/how much this matters.</p>\n<p>I think probably all of us benefited from <a class=\"mention\" href=\"/u/gabriel\">@gabriel</a>\u2019s implementation \u2013 I know I did, and I\u2019ve seen it referred to a lot of time in other people\u2019s code. But even though it seems to be pretty much the standard reference for many (in the absence of the original macro), I\u2019m not sure it\u2019s widely recognized that other implementations seem to have deviated a bit in this detail.</p>\n<p>In any case, I\u2019d be really interested to understand if there is a \u2018right\u2019 way to do it.</p>\n<p>I\u2019m also very interested in whether <a class=\"mention\" href=\"/u/phaub\">@phaub</a> might have any more best practice suggestions from <a href=\"https://www.nature.com/articles/srep12096\">this</a> <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>", "<p>Yes, I am aware of that implementation detail in Ruifrok\u2019s code about the 3rd residual colour. A couple of people have made that observation in the past. I will have a look at the other implementations.</p>", "<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> The Histomics TK link does not work.</p>", "<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a><br>\nHave you seen the supplement to this paper, <a href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fsrep12096/MediaObjects/41598_2015_BFsrep12096_MOESM1_ESM.pdf\">here</a>.<br>\nWe discussed this issue in S1.5_A.</p>\n<p>In case of two stains and using the 3x3 matrix approach to solve the over determined system a third \u2018pseudo vector\u2019 should be perpendicular to the first two. This can lead to negative values in the pseudo vector. The resulting 3rd cannel value is a quasi measure of the deconvolution error.<br>\nThe negative vector components have a meaning and should not be corrected.</p>\n<p>The right way to do it \u2026?<br>\nSince there are a lot of disturbances (homogeneity and stability of the illumination, background and gamma correction, quality, stability and reliability of the staining, the non-linarity of IHC preparation, ect ect ect. and last be not least the invalid assumption of a linear realionship between the polychromatic measured absorbance and the stain concentration) the color deconvolution is questionable in any case.<br>\nSolve the system of equations will not dramatically change this.</p>\n<p>So, just do it in one of the correct ways and avoid a wrong one.</p>", "<aside class=\"quote no-group\" data-username=\"gabriel\" data-post=\"3\" data-topic=\"38725\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/9de0a6/40.png\" class=\"avatar\"> gabriel:</div>\n<blockquote>\n<p><a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> The Histomics TK link does not work.</p>\n</blockquote>\n</aside>\n<p>Thanks, fixed now (hopefully).</p>\n<aside class=\"quote no-group\" data-username=\"phaub\" data-post=\"4\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/phaub/40/28688_2.png\" class=\"avatar\"> phaub:</div>\n<blockquote>\n<p>We discussed this issue in S1.5_A.</p>\n</blockquote>\n</aside>\n<p>I hadn\u2019t seen that - thanks!</p>\n<aside class=\"quote no-group\" data-username=\"phaub\" data-post=\"4\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/phaub/40/28688_2.png\" class=\"avatar\"> phaub:</div>\n<blockquote>\n<p>So, just do it in one of the correct ways and avoid a wrong one.</p>\n</blockquote>\n</aside>\n<p>Do I understand correctly that using the cross product would count as one of the correct/least wrong ways\u2026? With all the caveats regarding the use of color deconvolution generally, of course.</p>\n<p>I ask because as QuPath becomes more widely used (approaching 100k downloads now\u2026) I\u2019m keen that it should promote good practice \u2013 or at least not inadvertently makes bad practice widespread. I think this is partly a matter of implementation and partly of documentation.</p>\n<p><a class=\"mention\" href=\"/u/gabriel\">@gabriel</a> and <a class=\"mention\" href=\"/u/phaub\">@phaub</a> since you are the people I have encountered who I think know most about this topic, I particularly value your suggestions <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> I already link people to your writings, but wondering if there is more could be done \u2013 perhaps my using the UI to offer guidance that helps encourage better analysis and appropriate interpretation.</p>", "<p>Some quick tests to compute the 3rd vector via the cross product show that you one cannot create nice looking LUTs: the negative components represent impossible colours. I assume that the original implementation (which generated OD images with LUTs to represent the dyes) avoided this by generating an alternative LUT.<br>\nPerhaps the plugin should not attempt to generate LUTs in those situations (more of a cosmetic issue) and generate also 32bit images instead.</p>", "<aside class=\"quote no-group\" data-username=\"petebankhead\" data-post=\"5\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/petebankhead/40/1964_2.png\" class=\"avatar\"> petebankhead:</div>\n<blockquote>\n<p>Do I understand correctly that using the cross product would count as one of the correct/least wrong ways\u2026?</p>\n</blockquote>\n</aside>\n<p>Yes.</p>\n<p>To show the connection between a third orthogonal pseudo vector and the GaussTransformation see the following information</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/a/ca8068aed6f27bd1c7aba5069bcd34c122aabf4b.gif\" alt=\"GaussTransformation\" data-base62-sha1=\"sTpyXvCselUpUzOnYhrY39MEuPp\" width=\"628\" height=\"426\"></p>\n<p>So you can either solve the over determined systems by</p>\n<ul>\n<li>on of the projections into 2D (RG, RB, GB)</li>\n<li>usage of a third orthogonal residual vector</li>\n<li>GaussTransformation / Moore-Penrose-Inverse (<a href=\"https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse\">MPI</a>)</li>\n<li>SVD</li>\n<li>QR decomposition</li>\n</ul>\n<p>On no account you will get a error free result because of the incorrect assumption underlaying the approach. Even under theoretical perfect conditions (stoichiometric and linear staining, perfect imaging \u2026) the linear approach can not model the nonlinear signal formed by a spectral integration without errors.</p>\n<p>The extent of this error can not be estimated easily. The type of and the combination of staining strongly influences this error.</p>\n<p>Its like modelling a circle by its tangent and asking for the error of that approximation.<br>\nThe answer is: It depends.<br>\nSimplification only works under particular conditions.</p>", "<aside class=\"quote no-group\" data-username=\"gabriel\" data-post=\"6\" data-topic=\"38725\" data-full=\"true\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/9de0a6/40.png\" class=\"avatar\"> gabriel:</div>\n<blockquote>\n<p>Some quick tests to compute the 3rd vector via the cross product show that you one cannot create nice looking LUTs: the negative components represent impossible colours. I assume that the original implementation (which generated OD images with LUTs to represent the dyes) avoided this by generating an alternative LUT.<br>\nPerhaps the plugin should not attempt to generate LUTs in those situations (more of a cosmetic issue) and generate also 32bit images instead.</p>\n</blockquote>\n</aside>\n<p>QuPath generates a LUT color as described <a href=\"https://github.com/qupath/qupath/blob/85700bbb026e1e72425e266ee3105b2f39c4f4fd/qupath-core/src/main/java/qupath/lib/color/StainVector.java#L220\">here</a> (basically clipping values that end up out of range). I\u2019m not sure that\u2019s the best approach, but as you say it\u2019s really cosmetic.</p>\n<p>I do however like the option of 32-bit output, since this preserves negative values \u2013 which can help as a sanity-check / preserve useful information. One can always clip it later.</p>\n<p>However, I understand better now this is starting to move from the original NIH Macro, and a faithful interpretation of the macro is also desirable.</p>\n<p><a class=\"mention\" href=\"/u/phaub\">@phaub</a> thank you for the explanation!</p>", "<aside class=\"quote no-group\" data-username=\"gabriel\" data-post=\"6\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/9de0a6/40.png\" class=\"avatar\"> gabriel:</div>\n<blockquote>\n<p>cannot create nice looking LUTs: the negative components represent impossible colours</p>\n</blockquote>\n</aside>\n<p>For this 3rd residual vector there is no \u2018true\u2019 color. The information of this 3rd channel is the amount of error. You can simply assign a FALSE color.</p>", "<p>Thanks Peter. Did you mean \u201cnot necessarily\u201d a true colour (e.g. when there is a negative component)? If there isn\u2019t a negative value, one can generate such 3rd colour as well.</p>", "<p><a class=\"mention\" href=\"/u/gabriel\">@gabriel</a><br>\nA 3rd residual vector has in no case a color!<br>\nIts direction is only defined in a mathematical sense.<br>\nIn case all components are possitive a derived color will lead to the believe that this color has a meaning. But it doesn\u2019t have.<br>\nI personally would assign a unique color to this residual channel to indicate that this channel contains a kind of quality measure (e.g. LUT Phase or Fire).</p>", "<p>Hi all,</p>\n<p>Forgive my maybe naive question, but in case there is more than 3 colors on the image ; would it be possible to make a color deconvolution with 4, 5 or 6 components (wich is done in some commercial products like Akoya\u2019s Inform). Maybe it is written in Ruifrok and Johnston 's paper, but I fear my mathematical level is not good enough to understand it.</p>\n<p>Nico</p>", "<p>The linear approach discussed here is based on solving a system of linear equations.<br>\n</p><aside class=\"onebox wikipedia\">\n  <header class=\"source\">\n      <a href=\"https://en.wikipedia.org/wiki/System_of_linear_equations\" target=\"_blank\">en.wikipedia.org</a>\n  </header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:220/220;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/5/85630a2ac62768fa3a88cf2c35b8045fa043d5e8.png\" class=\"thumbnail\" width=\"220\" height=\"220\"></div>\n\n<h3><a href=\"https://en.wikipedia.org/wiki/System_of_linear_equations\" target=\"_blank\">System of linear equations</a></h3>\n\n<p>In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same set of variables. For example,\n is a system of three equations in the three variables x, y, z. A solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by\n since it makes all three equations valid. The word \"system\" indicates that the equations are to be ...</p>\n\n  </article>\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>To solve a SLE it needs at least as much or more equations than unknowns.</p>\n<p>Each unknown is a stain.<br>\nEach equation comes from one color channel of your image.</p>\n<p>With a color image - typically 3-channels RGB - you can \u2018separate\u2019 up to 3 stains.<br>\nIf you want to apply the linear approach to more than 3 stains than you need more color channels. The images have to be captured as multi-channel images with more then 3 channels.</p>\n<p>If you find a suitable camera it would be nice to post it here <img src=\"https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"></p>", "<p>I have heard that the camera on the Vectra systems could be used to generate a multichannel brightfield image (7 channels?), in theory. Though I have never tested it.</p>", "<p><a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>, <a class=\"mention\" href=\"/u/phaub\">@phaub</a>, That\u2019s the camera I use. And it does a multi-chanel image (the whole spectra is split in 35 images). But as I mainly use it for fluorescence, I had never imagined that the brightfield acquisition was the same.<br>\nBut the color deconvolution implemented works very well (if you have the right mono-staining sample in your library <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> ).</p>\n<p>Nico</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"14\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> Research_Associate:</div>\n<blockquote>\n<p>the Vectra systems</p>\n</blockquote>\n</aside>\n<p>is based on a liquid crystal tunable filter (LCTF).</p>", "<aside class=\"quote no-group\" data-username=\"VirtualSlide\" data-post=\"15\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/v/b5a626/40.png\" class=\"avatar\"> VirtualSlide:</div>\n<blockquote>\n<p>But as I mainly use it for fluorescence, I had never imagined that the brightfield acquisition was the same</p>\n</blockquote>\n</aside>\n<p>I would guess that it merges all of those channels into 3, as RGB is used for display on the screen. I\u2019m not sure how you would really look at a 35 channel brightfield image, but it might allow the deconvolution of more channels from the data side?</p>", "<p>I bet that a 35 channel image merged into RGB would look like\u2026 an RGB image, because displays have only 3 colours and the eye has (most commonly) only 3 sensors.<br>\nMost applications I have seen on multi and hyperspectral imaging rely on some kind of data reduction (like PCA) to extract information. Really interesting subject.</p>", "<p>The conversion of an n-channel brightfield image to RGB is described</p>\n<aside class=\"quote no-group\" data-username=\"phaub\" data-post=\"4\" data-topic=\"38725\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/phaub/40/28688_2.png\" class=\"avatar\"> phaub:</div>\n<blockquote>\n<p><a href=\"https://static-content.springer.com/esm/art%3A10.1038%2Fsrep12096/MediaObjects/41598_2015_BFsrep12096_MOESM1_ESM.pdf\">here</a></p>\n</blockquote>\n</aside>\n<p>see section S1.4</p>", "<p>For CellProfiler, the relevant function is <a href=\"https://github.com/CellProfiler/CellProfiler/blob/7d3a9d56cb67a6d55f182934fcc1fdd35ace5724/cellprofiler/modules/unmixcolors.py#L384-L413\">here</a>, which calls out to <a href=\"https://github.com/CellProfiler/CellProfiler/blob/7d3a9d56cb67a6d55f182934fcc1fdd35ace5724/cellprofiler/modules/unmixcolors.py#L439-L467\">these two other functions</a> - only those 60 lines are \u201cfunctional\u201d, everything else is display, image saving, or the estimation of custom stains.</p>"], "77638": ["<p>Hello community!</p>\n<p>I am looking at two dyes that localize to the same location (DNA). I am using one of those to track my region of interest per nuclei, and the other one I will be obtaining median intensity of the channel for each nuclei, tracked over time. I have great signal to noise ratios within the pictures below, so I am hoping I can get just the thin line of florescence that surrounds the nucleus and the nucleoli/if there\u2019s any additional fluorescence within the center (but that is not all cells, just some).</p>\n<p>I do not want to JUST use IdentifyPrimaryObjects, as that will identify the whole nucleus, I just want effectively a continuous wire where the DNA is lit up.</p>\n<p>Additionally, a complication is that photobleaching over the timespan causes a significant darkening of the image. See the images attached for reference.</p>\n<p>I have thought about creating a mask by taking the nuclei object and shrinking it by a pixel or two, and then keeping just the outside \u2018wire\u2019 that is formed, but that will exclude both nucleoli, and if there is any <em>significant</em> fluorescence on the inside of the nuclei (see the last .png for two nuclei that are opposite behaviors, but both I\u2019d like to capture as much of the signal as possible to declare my region of interest) it would completely disregard that. I need a more intelligent way to find the regions within each nucleus that has fluorescence that will not have significant dark space.</p>\n<p>The reason being is including the whole nuclei could skew the results of \u2018median intensity\u2019 for that specific object significantly (easily visualized when you compare the two nuclei in the last picture attached), despite the fluorescent portion of them being relative similar.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885.png\" data-download-href=\"/uploads/short-url/pkm5KbodsBjBLpGpkCvAzrpnmoB.png?dl=1\" title=\"1stTimeStep\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885_2_500x500.png\" alt=\"1stTimeStep\" data-base62-sha1=\"pkm5KbodsBjBLpGpkCvAzrpnmoB\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885_2_500x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885_2_750x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/1/b1836d3bb93cd81b09ce5502974987c00de58885.png 2x\" data-dominant-color=\"00002A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">1stTimeStep</span><span class=\"informations\">800\u00d7800 462 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339.png\" data-download-href=\"/uploads/short-url/11AUrHpA8cEgfNhK3bZ2AiGNgWl.png?dl=1\" title=\"LastTimeStep\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339_2_500x500.png\" alt=\"LastTimeStep\" data-base62-sha1=\"11AUrHpA8cEgfNhK3bZ2AiGNgWl\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339_2_500x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339_2_750x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/7/0730570f547be85e60c03bc713091a8c4862a339.png 2x\" data-dominant-color=\"00001B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">LastTimeStep</span><span class=\"informations\">800\u00d7800 437 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6.jpeg\" data-download-href=\"/uploads/short-url/l7N8yEaJ5q6R4DtVSLa44DIeOQm.jpeg?dl=1\" title=\"LastTimeStepLargeFluorescenceComparison\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6_2_500x500.jpeg\" alt=\"LastTimeStepLargeFluorescenceComparison\" data-base62-sha1=\"l7N8yEaJ5q6R4DtVSLa44DIeOQm\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/4/940f28094836b4aa8d468bff7f5b2016e11479e6.jpeg 2x\" data-dominant-color=\"01021A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">LastTimeStepLargeFluorescenceComparison</span><span class=\"informations\">800\u00d7800 37 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks for your help community!   <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Nick</p>", "<p>As an example of what I am looking to do in CellProfiler, is in the pictures below, where I zoomed into one nucleus and traced the flourescent signature by hand.</p>\n<p>This post may have become buried by a bunch of new posts and questions, so Beth, as you have been extraordinarily helpful regarding a few similar problems of mine, do you know how I can go about doing this/if not who could walk me through which combinations of modules can make this trace? <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a61ead9776f8a6d56d87982aa9582d45a1053d8b.png\" data-download-href=\"/uploads/short-url/nHyZelMHA7ieaagqeFCEmB2iNKP.png?dl=1\" title=\"Screenshot 2023-02-28 at 11.26.17 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/6/a61ead9776f8a6d56d87982aa9582d45a1053d8b.png\" alt=\"Screenshot 2023-02-28 at 11.26.17 AM\" data-base62-sha1=\"nHyZelMHA7ieaagqeFCEmB2iNKP\" width=\"616\" height=\"500\" data-dominant-color=\"000229\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-02-28 at 11.26.17 AM</span><span class=\"informations\">1066\u00d7864 8.85 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06d1195d2491d78df238cae1e30cd9872a142439.png\" data-download-href=\"/uploads/short-url/YiRalr3JtIER6jDGBmnenl9SxX.png?dl=1\" title=\"ROITrace\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/6/06d1195d2491d78df238cae1e30cd9872a142439.png\" alt=\"ROITrace\" data-base62-sha1=\"YiRalr3JtIER6jDGBmnenl9SxX\" width=\"616\" height=\"500\" data-dominant-color=\"010228\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ROITrace</span><span class=\"informations\">1066\u00d7864 40 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thank you!</p>\n<p>Nick</p>", "<p><a class=\"mention\" href=\"/u/lmurphy\">@lmurphy</a> Do you know of anyone that could help guide me on which modules to use to solve this problem? I feel as if this topic is likely buried under new topics, so without a direct  \u2018@\u2019 request I worry that this will be buried.</p>\n<p>Thanks,</p>\n<p>Nick</p>", "<p>Hey!</p>\n<p>I don\u2019t have a quick answer for you, but how about the following:</p>\n<ul>\n<li>First segment the nuclei. I\u2019ve had great success with <a href=\"https://www.cellpose.org/\" rel=\"noopener nofollow ugc\">CellPose</a>, especially if you\u2019re willing to train your own model. A GPU is practically mandatory, YMMV.</li>\n<li>Once you have a mask for each nucleus, shrink the ROI by a % of the total area. The .shape <a href=\"https://shapely.readthedocs.io/en/stable/reference/shapely.buffer.html?highlight=buffer\" rel=\"noopener nofollow ugc\">Shapely</a> function works well for me.</li>\n<li>Using the nuceli mask, drop the MGV of anything outside the masks to 0</li>\n<li>At this point, you should have an image that includes only the internal part of the nuclei and everything else should be 0</li>\n<li>Measure the feature directly, as you described above</li>\n</ul>\n<p>What do you think?</p>\n<p>Leo</p>", "<p>Hey Leo,</p>\n<p>Thank you for your response, that is a very interesting solution. Unfortunately, I have already spent significant time training my pipeline in CellProfiler to work with a variety of cell lines. I am hoping to tweak it just a smidge (but keep processing the same in case there are inherent biases in different softwares) to allow for this new modification.</p>\n<p>I also may be not fully understanding your answer, but I don\u2019t explicitly want just the internal part of the nucleus, I want *only the DNA containing regions of the nucleus, which are typically near the nuclear membrane, but as with some of these cells I highlighted above, can have DNA floating somewhere within the middle of the nucleus. So I would literally just want a tiny thick line tracing where the DNA has been lit up with one channel, and then measure that same location in another channel.</p>\n<p>I\u2019m sorry if my wording is confusing; if it is, please let me know!</p>\n<p>Thanks,</p>\n<p>Nick</p>", "<p>Let me rephrase:</p>\n<ul>\n<li>Bright parts of the nucleus contain DNA</li>\n<li>DNA is distributed around the edges of a nucleus and as filaments within the nucleus</li>\n<li>You\u2019d like to exclude anything below a certain threshold, which might change over time due to photobleaching</li>\n</ul>\n<p>Right?</p>\n<p>If so, the two-step process I proposed above should work. By segmenting the nuclei, you can calibrate a threshold for each frame such that it captures the feature you\u2019re interested in. Using the mask you get from setting a threshold for each frame, you can measure what you need on the other image.</p>\n<p>Unfortunately, I\u2019m not familiar enough with CellProfiler to offer anything other than conceptual tips.</p>\n<p>Leo</p>", "<p>Oh I understand now, thanks Leo for the suggestion! That can be a great backup plan to use if I find that CP is not capable of doing this, thank you!! I\u2019d like to stick with the same software that i have used for about 100 other experiments, as any internal differences in segmenting could give different population statistics (if I can).</p>\n<p>I appreciate the clarification, as that may help me move forward with this new program in the future!</p>\n<p>Thanks,</p>\n<p>Nick</p>", "<p>That makes sense. Happy to (try to) help<br>\nGood luck!</p>\n<p>Leo</p>", "<aside class=\"quote no-group\" data-username=\"EitNickS\" data-post=\"1\" data-topic=\"77638\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/eitnicks/40/54803_2.png\" class=\"avatar\"> Eit Nick S:</div>\n<blockquote>\n<p>I do not want to JUST use IdentifyPrimaryObjects, as that will identify the whole nucleus, I just want effectively a continuous wire where the DNA is lit up.</p>\n</blockquote>\n</aside>\n<p>Hmm\u2026 so one potential approach is this:</p>\n<ol>\n<li>IdentifyPrimaryObjects to identify the whole nucleus</li>\n<li>Threshold the image using the threshold module to get the rim and nucleoli</li>\n<li>Use MaskObjects to mask the nuclei to only include areas from <span class=\"hashtag\">#2</span>\n</li>\n</ol>\n<p>Then you could make measurements within those objects. You could also try Watershed on the threshold to break up the binary image into separate objects then relate them to nuclei using RelateObjects.</p>", "<p>Neat! Thanks <a class=\"mention\" href=\"/u/rebecca_senft\">@Rebecca_Senft</a> !! I will attempt to use this guidance to get to a solution! Thanks! I had been trying to threshold the image, but I always managed to pull the full nucleus. I\u2019ll try to adjust the parameters and use the watershed thresholding algorithm.</p>\n<p>Thank you! I\u2019ll send you a message if I cannot get anywhere after messing with it for a week or so.</p>\n<p>I appreciate the help <a class=\"mention\" href=\"/u/rebecca_senft\">@Rebecca_Senft</a> and <a class=\"mention\" href=\"/u/leogolds\">@leogolds</a> !</p>\n<p>Nick</p>", "<p>Thanks <a class=\"mention\" href=\"/u/rebecca_senft\">@Rebecca_Senft</a> , just an update, it appears to work decently well! The thresholding appears to fluctuate picking up portions of sections of DNA within the nucleus over each frame, which causes the data be a little noisy. I\u2019ve got to have the thresholding correction factor at 2.5 to correctly distinguish signal.</p>\n<p>But it works!!! I\u2019m going to attempt to manipulate the ROI creation channel with root square math to boost signal, but if you have any other ideas I\u2019d love to hear it! Thanks again!</p>"], "78663": ["<p>Hi,</p>\n<p>I am working with imaging mass cytometry data and see variability in my nuclear staining that I would like to minimize. I want to split the multi.tiff into it\u00b4s 25 channels, edit the DNA channel, and then save the image as H5 to import into ilastik. However, when I try splitting the channels with \u201cColorToGray\u201d - Channels, and then specifiy the channels, CP complains that the input image isn\u00b4t in color (which is fair). What are alternative ways to split these channels and edit only one of them?</p>\n<p>Thanks!</p>"], "76616": ["<p>I am quantifying immunofluorescence stainings. I have one cytoplasmic stanining (green) that I need to relate to a nuclear staining (red).<br>\nI am having trouble</p>\n<ol>\n<li>counting how many cells stained for green</li>\n<li>counting how many double positive green/red<br>\nI would like to know what settings to use in Cell Profiler for these 2 countings.<br>\nThank you!</li>\n</ol>"], "76104": ["<p>Hi all,</p>\n<p>Just a quick question about best practices of image analysis. When we export our images from the microscope, we export them as single channels so we have an individual channel for each stain. Then, taking those into CellProfiler, we\u2019ve been treating these single colour images as a greyscale image in the Names and Types section, despite the image still being coloured. Wondering whether we should be converting these into greyscale first and then processing? What difference does that make for output?</p>\n<p>Thanks for your help!<br>\nBeth</p>", "<p>Hi Beth,</p>\n<p>I\u2019m assuming you\u2019re using a fluorescence microscope. The separate channels you get are pseudocolored to look like they do through your eyepieces but in reality this coloring isn\u2019t actually there. Your camera is almost certainly monochromatic; it doesn\u2019t see color, only how much light is reflecting off your sample. Treating the images as grayscale sounds appropriate. You should not have to convert them into grayscale first. You can always do an experiment and see whether any conversion affects your output, but if you do this make sure to keep everything else (e.g., bit depth) the same.</p>\n<p>Rebecca</p>", "<p>That\u2019s correct, it is a fluorescence microscope. And that makes a lot of sense! Thanks for your help Rebecca!</p>"], "78666": ["<p>Hi<br>\nI am using the cellpose 2.2 in cell segmentation.<br>\nThis system was built in WEB platform and the cellpose was realized by using command.</p>\n<p>Firstly, while researching the cellpose, I found that cellpose provide 14 pretrained models.<br>\nBut I am not sure what the differences(and their utilizing examples) are between them because I am not an expert in this field.<br>\nPlease give me good advice about this problem.<br>\nSecondly, I also found that the cellpose provide \u201c*_output.png\u201d file when we get cell segment result by using commands.<br>\nBut sometimes there are some cases that it\u2019s not created(Because I used the same command, I think that it\u2019s related with the kind of image file).<br>\nWhich case is it created or not in?<br>\nAny help would be appreciated.</p>"], "13128": ["<p>I have labelled the brain tissue with a neuronal marker and I have also stained the slide with DAPI. DAPI has consequently labelled all the nuclei in the tissue. What I am trying to do is to create a pipeline where nuclei will be identified as primary objects (I have sorted this) and only neurons outlining the nuclei will be selected as secondary objects. For some reason this isn\u2019t working for me as CellProfiler keeps on selecting random fluorescent items in the image and it is also identifying the dendrites as individual objects.<br>\nWhich threshold method is the best in this case so only the neuron will be identified?</p>", "<p>Hi,</p>\n<p>Feel free to upload an example image and your pipeline, but I can offer these suggestions in any case:</p>\n<ul>\n<li>It is often useful to pre-process neurites.  I suggest EnhanceOrSuppressFeatures -&gt; Neurites-&gt; Tubeness</li>\n<li>For the IDSecondary Threshold Method, MCT is a method that was designed (by others) for neurons.  You may need to lower the Threshold Correction Factor depending on your image intensities.</li>\n<li>For the IDSecondary \u201cmethod to identify secondary objects\u201d I would suggest either the Propagation or \u201cDistance - B\u201d.  Note for \u201cDistance - B\u201d you need to set the \u201cNumber of pixels by which to expand the primary objects\u201d set to a large number (e.g. 200) equal to the length of the longest neurite that you expect, in pixels.</li>\n</ul>\n<p>Beyond these basic suggestions, I think you\u2019ll need to upload images and your pipeline for us to be of any further help.</p>\n<p>Cheers,<br>\nDavid</p>", "<p>Hi David,</p>\n<p>Thank you so much for offering to help me with this. Please find attached sample images and pipeline. So, I have taken the suggestion of enhancing the neurites onboard and it seems to work absolutely well. But, CP is still identifying one neuron as several different objects.</p>\n<p>I have tried to set the lower and upper threshold limits too.</p>\n<p>Any suggestion would be helpful in this case.</p>\n<p>Thanks<br>\n<a class=\"attachment\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/f/f871606d28757a40da3ccd01900b485d0f284a50.cpproj\" rel=\"nofollow noopener\">Neurons.cpproj</a> (74.5 KB)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/3/3d86a743e4cc845b6fc41cf93026fa4e581fa39f.tiff\" data-download-href=\"/uploads/short-url/8MhCokxXaj4jrdb0tex34NkDLqL.tiff?dl=1\" title=\"40X_dapi.tiff\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/3/3d86a743e4cc845b6fc41cf93026fa4e581fa39f.tiff\" width=\"668\" height=\"500\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">40X_dapi.tiff</span><span class=\"informations\">1374\u00d71028 1.5 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/5/5d8721b1dfb1e5fbef191262d399e6977c9c83af.tiff\" data-download-href=\"/uploads/short-url/dlnVdG3zcLpJGzJFhOadY3IIrbp.tiff?dl=1\" title=\"40X_cy5.tiff\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/5/5d8721b1dfb1e5fbef191262d399e6977c9c83af.tiff\" width=\"668\" height=\"500\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">40X_cy5.tiff</span><span class=\"informations\">1374\u00d71028 1.63 MB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br></p>", "<p>Hi,</p>\n<p>A few suggestions (see my attached project file for more detail):</p>\n<ul>\n<li>Zoomed in, both channel\u2019s images have a mottled or speckled appearance.  Has there been some sort of processing done on these images, or were they first saved in a lossy, non-tiff format?  It is a red flag for the image acquisition, and in fact, this may actually be causing some issues with the neurite enhancement (see attached screenshot - though Tubeness is handling it pretty well).</li>\n<li>Save your images as grayscale.  These are false color images.  In fact, your pipeline would not run for me until I chose \u201cGrayscale\u201d for the image type in NamesAndTypes (but then CP tries to be smart takes an average of the channels, I think, which may not be what you want).  The image file size will also be smaller and CP will run faster.</li>\n<li>EnhanceEdges is a nice idea, but in practice its output is not a great input into IdentifyPrimaryObjects.  I would remove it.</li>\n<li>IdentifyPrimaryObjects - switch to inputting the \u2018nuclei\u2019 image directly.  Then you need to increase the size range from 20-80 pixels or so.  The 40 pixel max setting was causing nuclei to be split too often.  Also switch to using \u201cShape\u201d as the declumping method, which is often better for round objects.</li>\n<li>A** crucial issue here** is that there appear to be more nuclei than soma.  Do these cultures have glia as well?  If so, the simplest way is to exclude them by another glia-specific marker.  The next step is to measure the Cy5 intensity in the Nuclei objects and filter them out unless they reach a minimum threshold (which you would need to determine experimentally).  Or you can not even use the nuclear channel at all and just process the Cy5 channel, doing some image morphological tricks to identify the soma.</li>\n<li>Be careful naming objects the same name as previously defined images or objects.  It may work, but it is confusing (to me at least!) and may confuse CP without any warning to you.</li>\n</ul>\n<p>I added a couple more modules - take a look at the Module Notes at the top of each module for some further comments.</p>\n<p>Good luck!  Neuron image analysis is challenging!<br>\nDavid<br>\n<a class=\"attachment\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/f/fd176f3269d102b3e6f519159988a0a82896f6d0.cpproj\">NeuronsDL2.cpproj</a> (423 KB)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/6/6fcf9b49416893c055bb43ced753991db53ee6ec.png\" data-download-href=\"/uploads/short-url/fX7Nfdc94oHBkientvxCION03p2.png?dl=1\" title=\"Screen Shot 2014-04-10 at 12.59.39 PM.png\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/6/6fcf9b49416893c055bb43ced753991db53ee6ec.png\" width=\"690\" height=\"331\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/6/6fcf9b49416893c055bb43ced753991db53ee6ec_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2014-04-10 at 12.59.39 PM.png</span><span class=\"informations\">838\u00d7402 43.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br></p>", "<p>Dear colleagues,<br>\nI am relatively new to CP (but  already love it!) and have a similar problem regarding identification of neurons versus other cell types in culture. In my image, DAPi is staining nuclei, red is staining neurons, and green is for glial cells. I want to quantify number of cell types across many images. Unfortunately, I can\u2019t get the identification of neurons working properly.<br>\nNuclei identification works nicely, but I can\u2019t seem to find a way to identify neurons.<br>\nan exemplary image is attached, maybe anyone can help?</p>\n<p>THX,<br>\nbest wishes,<br>\nandy<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/4/4b5b8f416cf373fffb335c7a1925d1bce97d90b2.JPG\" data-download-href=\"/uploads/short-url/aKDYuZE9wkQ5lDmjdsU2h4r7qRY.JPG?dl=1\" title=\"I3EN_N-0020_(c1+c2+c3).JPG\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/4/4b5b8f416cf373fffb335c7a1925d1bce97d90b2_2_667x500.JPG\" width=\"667\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/4/4b5b8f416cf373fffb335c7a1925d1bce97d90b2_2_667x500.JPG, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/4/4b5b8f416cf373fffb335c7a1925d1bce97d90b2_2_1000x750.JPG 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/4/4b5b8f416cf373fffb335c7a1925d1bce97d90b2_2_1334x1000.JPG 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/4/4b5b8f416cf373fffb335c7a1925d1bce97d90b2_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">I3EN_N-0020_(c1+c2+c3).JPG</span><span class=\"informations\">1388\u00d71040 571 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div><br></p>", "<p>Hi Andy,</p>\n<p>How do you identify the neurons by eye in this image?  This doesn\u2019t look that easy for me to tell by eye the cell types of each, but maybe you are better than me!  (This was also why I didn\u2019t jump at answering this one so fast)<br>\nMAP2 (is that the red marker?) is usually present on the soma, and it looks like it is mostly the case here.  And the Green marker looks to be in the cytoplasm, though there are some cells which seem to have even neurites with green label.  So you can measure those compartments, measure their intensities of the respective channel, and filter out cells by thresholds of those intensity measurements.</p>\n<p>I am attaching a stub of a pipeline, which does the above procedure for the glia, and you can adapt it for the neuron detection (measure red channel in nuclear object, or maybe expand a a couple pixels from the nucleus too).</p>\n<p>A couple other notes<br>\n** In Image, load the 3 color JPG.<br>\n** Side note - we recommend you avoid JPG since they are lossy formats.<br>\n** Also, there is a large illumination artifact in the lower right and upper left here which will adversely affect segmentation</p>\n<p>Hope this helps,<br>\nDavid<br>\n<a class=\"attachment\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/c/cb853f417312af6300a55ecd5f8cacfa62fac6a5.cppipe\" rel=\"nofollow noopener\">DLcount_neurons.cppipe</a> (10.6 KB)</p>", "<p>Hello <a class=\"mention\" href=\"/u/kkapoor\">@kkapoor</a> ! Could you please upload the two images again?<br>\nBest<br>\nAlessio</p>"], "81229": ["<p>I have one Green channel and a phase contrast image of yeast cells from the same spot. From the Green channel, I need to find Foci in the nucleus and And total no of cells present in the frame. While loading the images and I am getting an error \u201cYour pipeline doesn\u2019t produce any valid image sets as currently configured\u201d. This is the first time I am using cell profiler please help!<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/8/b818b835e95d7595ee8df351490b849f06b304fd.jpeg\" data-download-href=\"/uploads/short-url/qgAOAIin0NLnc0zh56GoNXDqopf.jpeg?dl=1\" title=\"_001\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/8/b818b835e95d7595ee8df351490b849f06b304fd_2_690x458.jpeg\" alt=\"_001\" data-base62-sha1=\"qgAOAIin0NLnc0zh56GoNXDqopf\" width=\"690\" height=\"458\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/8/b818b835e95d7595ee8df351490b849f06b304fd_2_690x458.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/8/b818b835e95d7595ee8df351490b849f06b304fd_2_1035x687.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/8/b818b835e95d7595ee8df351490b849f06b304fd_2_1380x916.jpeg 2x\" data-dominant-color=\"050F00\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">_001</span><span class=\"informations\">4908\u00d73264 891 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce5a9173aab879f8a190cced2bd6dfb114826048.jpeg\" data-download-href=\"/uploads/short-url/truoOQvhYrhG8SmVnYjP3QrZVEI.jpeg?dl=1\" title=\"_003\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce5a9173aab879f8a190cced2bd6dfb114826048_2_690x458.jpeg\" alt=\"_003\" data-base62-sha1=\"truoOQvhYrhG8SmVnYjP3QrZVEI\" width=\"690\" height=\"458\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce5a9173aab879f8a190cced2bd6dfb114826048_2_690x458.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce5a9173aab879f8a190cced2bd6dfb114826048_2_1035x687.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce5a9173aab879f8a190cced2bd6dfb114826048_2_1380x916.jpeg 2x\" data-dominant-color=\"6E6E6E\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">_003</span><span class=\"informations\">1920\u00d71277 115 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "78163": ["<p>Hi all!</p>\n<p>I\u2019m designating 2 areas in an image as ROIs and applying them as masks. For some reason, one of these masks (_sVZ) alters the pixel intensity and the other does not (_CP). Does anyone know why this is happening or have any ideas on how to circumvent this?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f.png\" data-download-href=\"/uploads/short-url/3S66KtX49XLHXf2Lpijjl4hNiBV.png?dl=1\" title=\"Screen Shot 2023-03-06 at 3.24.59 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f_2_689x419.png\" alt=\"Screen Shot 2023-03-06 at 3.24.59 PM\" data-base62-sha1=\"3S66KtX49XLHXf2Lpijjl4hNiBV\" width=\"689\" height=\"419\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f_2_689x419.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f_2_1033x628.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/b/1b240a7520eece48eb0db7924cb7db5b6536c83f.png 2x\" data-dominant-color=\"606060\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-06 at 3.24.59 PM</span><span class=\"informations\">1283\u00d7780 285 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c.png\" data-download-href=\"/uploads/short-url/oLue8PSGmfMLmBRpMKX9UgizBik.png?dl=1\" title=\"Screen Shot 2023-03-06 at 3.25.24 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c_2_690x377.png\" alt=\"Screen Shot 2023-03-06 at 3.25.24 PM\" data-base62-sha1=\"oLue8PSGmfMLmBRpMKX9UgizBik\" width=\"690\" height=\"377\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c_2_690x377.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c_2_1035x565.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/d/ad92623cd178c19edea75c0ebdcab5cc4b56a78c.png 2x\" data-dominant-color=\"575858\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-03-06 at 3.25.24 PM</span><span class=\"informations\">1289\u00d7706 291 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks in advance!<br>\npdg</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/lJEy3eii9iHCoz8JxjRKK0MJIpu.cpproj\">PDG Zones TBR1 Ver. 23.03.02 copy.cpproj</a> (731.2 KB)</p>"], "12628": ["<p>Hey I\u2019m using a macbook pro mid 2012 running 10.1.4 with 16GB of RAM.  Every time I try to open the latest Stable (2.2.1)  release from April 18, 2016 I get this error message.<br>\n<img src=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/0/0c0a33efdefec211ae1877b382873db19a528a02.png\" width=\"422\" height=\"154\"></p>\n<p>This is what the console shows<br>\n28/04/2016 12:25:37.889 sharedfilelistd[401]: [default] [&lt;CFString 0x7fff7640de00 [0x7fff764c5440]&gt;{contents = \u201ccom.apple.LSSharedFileList.RecentApplications\u201d}] List write failed invalid info items: (null) properties: (null)<br>\n28/04/2016 12:25:37.889 sharedfilelistd[401]: -[ListStore writeListItems:properties:withListIdentifier:notificationHander:] [com.apple.LSSharedFileList.RecentApplications] List write failed invalid info items: (null) properties: (null)<br>\n28/04/2016 12:25:40.389 CellProfiler[9482]: Could not find Java JRE compatible with x86_64 architecture<br>\n28/04/2016 12:25:40.551 CellProfiler[9482]: Plugin directory doesn\u2019t point to valid folder: /Applications/CellProfiler.app/Contents/Resources/plugins<br>\n28/04/2016 12:25:40.591 CellProfiler[9482]: Could not find Java JRE compatible with x86_64 architecture<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Could not find Java JRE compatible with x86_64 architecture<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Failed to execute \u201cfind\u201d when searching for libjvm<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 115, in _find_mac_lib<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]:   File \u201cposixpath.pyc\u201d, line 122, in dirname<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: AttributeError: \u2018NoneType\u2019 object has no attribute \u2018rfind\u2019<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Failed to execute \u201cfind\u201d when searching for libjvm<br>\n28/04/2016 12:25:40.627 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.628 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 115, in _find_mac_lib<br>\n28/04/2016 12:25:40.628 CellProfiler[9482]:   File \u201cposixpath.pyc\u201d, line 122, in dirname<br>\n28/04/2016 12:25:40.628 CellProfiler[9482]: AttributeError: \u2018NoneType\u2019 object has no attribute \u2018rfind\u2019<br>\n28/04/2016 12:25:40.628 CellProfiler[9482]: Failed to find libjvm<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Could not find Java JRE compatible with x86_64 architecture<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Failed to execute \u201cfind\u201d when searching for libjli<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 115, in _find_mac_lib<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201cposixpath.pyc\u201d, line 122, in dirname<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: AttributeError: \u2018NoneType\u2019 object has no attribute \u2018rfind\u2019<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Failed to execute \u201cfind\u201d when searching for libjli<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 115, in _find_mac_lib<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201cposixpath.pyc\u201d, line 122, in dirname<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: AttributeError: \u2018NoneType\u2019 object has no attribute \u2018rfind\u2019<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Failed to find libjli<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 274, in start_thread<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Exception: Javabridge failed to find JVM library<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Failed to create Java VM<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]: Traceback (most recent call last):<br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/<strong>boot</strong>.py\u201d, line 355, in <br>\n28/04/2016 12:25:40.665 CellProfiler[9482]:     _run()<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/<strong>boot</strong>.py\u201d, line 336, in _run<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     exec(compile(source, path, \u2018exec\u2019), globals(), globals())<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/CellProfiler.py\u201d, line 4, in <br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     cellprofiler.<strong>main</strong>.main()<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/cellprofiler/<strong>main</strong>.py\u201d, line 176, in main<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     cp_start_vm()<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/cellprofiler/utilities/cpjvm.py\u201d, line 167, in cp_start_vm<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     max_heap_size = heap_size)<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:   File \u201c/Applications/CellProfiler.app/Contents/Resources/lib/python2.7/javabridge/jutil.py\u201d, line 312, in start_vm<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]:     raise RuntimeError(\u201cFailed to start Java VM\u201d)<br>\n28/04/2016 12:25:40.666 CellProfiler[9482]: RuntimeError: Failed to start Java VM<br>\n28/04/2016 12:25:40.733 CellProfiler[9482]: CellProfiler Error<br>\n28/04/2016 12:25:40.733 CellProfiler[9482]: 2016-04-28 12:25:40.732 CellProfiler[9482:471412] CellProfiler Error</p>\n<p>I thought it was Java, so I updated it to Java SE Runtime Environment 8u92 downloaded from here <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html\" rel=\"nofollow noopener\">http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html</a> . It doesn\u2019t specify 64x for OSX like the others on the list but I believe it is. After I have done this, restarted the computer and everything I get the same error message.</p>\n<p>Any idea what is causing this?</p>\n<p>THanks</p>", "<p>I have the exact same problem on OS X 10.11.4 and 10.10.5 running JRE 1.8.91 and Python 2.7.10.</p>", "<p>We have other reports like this.  Though it works for some (me, for example).  It is possible you have two Java installations and the wrong one might being called(?).  Can you both please verify your Java version:</p>\n<ol>\n<li>open Terminal</li>\n<li>type \u201cjava -version\u201d (without the quotes)</li>\n</ol>\n<p>Mine reports this and CP 2.2.0 works on my Mac:</p>\n<blockquote>\n<p>java version \"1.8.0_60\"<br>\nJava\u2122 SE Runtime Environment (build 1.8.0_60-b27)<br>\nJava HotSpot\u2122 64-Bit Server VM (build 25.60-b23, mixed mode)</p>\n</blockquote>\n<p>Ensure that the first line has \u201c1.8\u201d and the last line says \u201c64-bit\u201d.  Paste the output here if you like.</p>", "<p>Here is what I get.</p>\n<p>java version \u201c1.6.0_65\u201d<br>\nJava\u2122 SE Runtime Environment (build 1.6.0_65-b14-468-11M4833)<br>\nJava HotSpot\u2122 64-Bit Server VM (build 20.65-b04-468, mixed mode)</p>\n<p>How can I get it to call the right version of Java?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0.png\" data-download-href=\"/uploads/short-url/mPcbPsnmwKKrBMhgaB4LbFDX9Zu.png?dl=1\" title=\"Screen Shot 2016-04-28 at 15.35.45.png\" rel=\"nofollow noopener\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0_2_503x500.png\" width=\"503\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0_2_503x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/2X/9/9ff95ebfc4a08b63e6f03f0b9d429ef120e57bc0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2016-04-28 at 15.35.45.png</span><span class=\"informations\">594\u00d7590 37.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>When I look at JRE settings on the preferences panel, it shows 1.8, so I have no idea what\u2019s going on</p>", "<p>I also get Java version \u201c1.6.0_65\u201d but that\u2019s because we both have installed leagcy java for OS X which is required for other programs like Adobe Photoshop CS6 for example.</p>\n<p><a href=\"https://support.apple.com/kb/DL1572?locale=en_US\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://support.apple.com/kb/DL1572?locale=en_US</a></p>\n<p>I also have Java version  1.8.91 installed.</p>", "<p>That\u2019s right, I also have the legacy (version 6 I think?) for OSX installed, I think its required for older versions of CP.</p>", "<p>Okay, it works for me now. You need to install Java SE Development Kit (JDK)</p>\n<p>You can download it here:</p>\n<p><a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>", "<p>Thanks henber! Totally worked!</p>", "<p><img src=\"//upload-forum-cellprofiler-org.s3-us-west-2.amazonaws.com/original/2X/c/c040fddb7269010bab814941b3d60708ddb5d49b.png\" width=\"451\" height=\"57\"></p>\n<p>Could possibly link to <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\" rel=\"nofollow noopener\">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a> in addition on the download page to help others out.</p>", "<p>No problem.</p>\n<p>Should actually say:<br>\nRequirements: Java SE Development Kit 8 (64-bit)<br>\nDevelopment kit also includes Runtime Enviroment.</p>\n<p>Have a nice day =)</p>", "<p>ava version \u201c19.0.2\u201d 2023-01-17</p>\n<p>Java\u2122 SE Runtime Environment (build 19.0.2+7-44)</p>\n<p>Java HotSpot\u2122 64-Bit Server VM (build 19.0.2+7-44, mixed mode, sharing)</p>", "<p>Hey I\u2019m using a macbook pro 2.3 GHz 8-Core Intel Core i9 Intel UHD Graphics 630 1536 MB ventura 13.2.1 .  Every time I try to install bioformats.  I get this error message</p>\n<p>(tuto-env) (base) titan@Titans-MacBook-Pro Documents % pip install python-bioformats<br>\nCollecting python-bioformats<br>\nUsing cached python_bioformats-4.0.7-py3-none-any.whl (40.6 MB)<br>\nCollecting python-javabridge==4.0.3<br>\nUsing cached python-javabridge-4.0.3.tar.gz (1.3 MB)<br>\nPreparing metadata (setup.py) \u2026 error<br>\nerror: subprocess-exited-with-error</p>\n<p>\u00d7 python setup.py egg_info did not run successfully.<br>\n\u2502 exit code: 1<br>\n\u2570\u2500&gt; [9 lines of output]<br>\nCould not find Java JRE compatible with x86_64 architecture<br>\nTraceback (most recent call last):<br>\nFile \u201c\u201d, line 2, in <br>\nFile \u201c\u201d, line 34, in <br>\nFile \u201c/private/var/folders/4_/n6nzlmcd4b719txgbffmwsgr0000gn/T/pip-install-73y8gsxo/python-javabridge_35e6b47ca93d435cb6c56bb3134a6689/setup.py\u201d, line 412, in <br>\next_modules=ext_modules(),<br>\nFile \u201c/private/var/folders/4_/n6nzlmcd4b719txgbffmwsgr0000gn/T/pip-install-73y8gsxo/python-javabridge_35e6b47ca93d435cb6c56bb3134a6689/setup.py\u201d, line 98, in ext_modules<br>\nraise Exception(\u201cJVM not found\u201d)<br>\nException: JVM not found<br>\n[end of output]</p>\n<p>note: This error originates from a subprocess, and is likely not a problem with pip.<br>\nerror: metadata-generation-failed</p>\n<p>\u00d7 Encountered error while generating package metadata.<br>\n\u2570\u2500&gt; See above for output.</p>\n<p>note: This is an issue with the package mentioned above, not pip.<br>\nhint: See above for details</p>", "<p>Looks like your env doesn\u2019t have java available? pip can\u2019t install that.<br>\nIt looks like you might be using conda to manage the env? if so, try to install openjdk from conda.</p>"], "75096": ["<p>An end-of-year gift for you all - a new bugfix release of CellProfiler! Hopefully your CellProfiler experience has been relatively bug-free, but if not, you may want to <a href=\"https://cellprofiler.org/releases\">check it out here</a> . A big thanks to <a class=\"mention\" href=\"/u/barbara_diaz-rohrer\">@Barbara_Diaz-Rohrer</a> , <a class=\"mention\" href=\"/u/ctromanscoia\">@ctromanscoia</a>, and <a class=\"mention\" href=\"/u/mas515\">@Mas515</a> for contributing fixes! And now, the work on CellProfiler 5 continues :). The happiest of holidays and the most restful of new year\u2019s to all!</p>"], "74588": ["<p>I want to quantify and classify ovarian follicles at different stages of development in images of H&amp;E images, any experience with that please?</p>", "<p>Can you please provide an example image of each stage of follicular development?</p>", "<p>Thanks <a class=\"mention\" href=\"/u/rdbell3\">@rdbell3</a> please see attached photo<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/d/7d1ed3ebb94e1fd368cae4d60272358f63fb7881.jpeg\" data-download-href=\"/uploads/short-url/hQRJob5mkYXy9bRofUt0ZPnnA1r.jpeg?dl=1\" title=\"Screenshot 2022-12-02 at 23.00.33\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d1ed3ebb94e1fd368cae4d60272358f63fb7881_2_690x262.jpeg\" alt=\"Screenshot 2022-12-02 at 23.00.33\" data-base62-sha1=\"hQRJob5mkYXy9bRofUt0ZPnnA1r\" width=\"690\" height=\"262\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d1ed3ebb94e1fd368cae4d60272358f63fb7881_2_690x262.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d1ed3ebb94e1fd368cae4d60272358f63fb7881_2_1035x393.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d1ed3ebb94e1fd368cae4d60272358f63fb7881_2_1380x524.jpeg 2x\" data-dominant-color=\"D2BBC8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2022-12-02 at 23.00.33</span><span class=\"informations\">1718\u00d7654 129 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>What do you want to quantify about each stage? or simply classify if a given follicle is in a stage?</p>", "<p>Thanks <a class=\"mention\" href=\"/u/rdbell3\">@rdbell3</a> I want their counts according to the stage of development in different sections</p>", "<p>So just to make sure i understand. You want to know, for example, on slide 1 there are 5 - stage 1, 11 stage 2, 3 stage 3 and 6 stage 4 follicles? and then on slide 2, 3, 4 etc.</p>\n<p>How many example images (roughly, 10s, 100s, 1000s) of each stage do you have in you data set and how consistent (histology artifacts, staining batch effect) are your images across you data set? The more examples the better and the smaller the variance in image quality/staining the better.</p>\n<p>I am not sure qupath is the best tool for this problem, i think a deep learning image classifier might be better (ie ResNet) but you should definitely try. You will need to provide annotations to qupath telling it where the examples of each stage is (10-100 examples each, maybe more <img src=\"https://emoji.discourse-cdn.com/twitter/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\">), you can follow <a class=\"mention\" href=\"/u/petebankhead\">@petebankhead</a> <a href=\"https://www.youtube.com/watch?v=J-47tzXAFdE&amp;list=PL4ta8RxZklWkPB_pwW-ZDVAGPGktAlE5Y&amp;ab_channel=PeteBankhead\" rel=\"noopener nofollow ugc\">tutorial 1</a> <a href=\"https://www.youtube.com/watch?v=4An5n6Y_rRI&amp;t=743s&amp;ab_channel=NEUBIAS\" rel=\"noopener nofollow ugc\">tutorial 2</a> or other resources on how to annotate in quapth. You also will want to annotate the surrounding stroma as well. Then you build a pixel classifier \u201cClassify \u2192  Pixel Classification \u2192 Train Pixel Classifier\u201d to try and classify the folicles at each stage. I like the using the settings for the classifier as: Classifier =  Random Trees; Res = Moderate or High; Features = Hit Edit and select the RGB channels, Scales = 0.5, 2.0 and 8.0, and Features = Gaussian, Laplacian of Gaussian, Structure tensor max eigenvalue, and Hessian max eigenvalue.  After you have a few examples are annotated you can do the live prediction to see how its working. Add more examples if it is not good enough.</p>\n<p>Then you use the classifier to create new annotations on new slides, and then export the spreadsheet of the annotations on each slide to count. Since follicles are organized on the multicellular level it is likely the pixel classifier will not perform that well. But like i said, you should give it a shot and see if it works. Maybe start small and try to differentiate the follicles (irregardless of stage) from the surround stroma.</p>\n<p>Hope this help!<br>\nRichard</p>", "<p>Maybe object detection in QuPath\u2026 soon? <a href=\"https://twitter.com/petebankhead/status/1598012920583311361?t=Nl0tS44ow0lMPR4N8-Wavw&amp;s=19\">https://twitter.com/petebankhead/status/1598012920583311361?t=Nl0tS44ow0lMPR4N8-Wavw&amp;s=19</a></p>", "<p>Was also thinking using SLIC to chunk the data and count cells (with other feature extractions) in different regions may help as cellularity in different regions through the stages seems informative but setting that up is a bit difficult\u2026would love to see a CNN in qupath in any form</p>", "<p>Yep. No clue how to use it though.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/e/3ec4d284f1da714e3722c662ec2589eb1931e77d.png\" data-download-href=\"/uploads/short-url/8XhiaDgkNgpEq6wQ2ljc9MQ780l.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3ec4d284f1da714e3722c662ec2589eb1931e77d_2_690x108.png\" alt=\"image\" data-base62-sha1=\"8XhiaDgkNgpEq6wQ2ljc9MQ780l\" width=\"690\" height=\"108\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3ec4d284f1da714e3722c662ec2589eb1931e77d_2_690x108.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3ec4d284f1da714e3722c662ec2589eb1931e77d_2_1035x162.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3ec4d284f1da714e3722c662ec2589eb1931e77d_2_1380x216.png 2x\" data-dominant-color=\"1E232B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2127\u00d7334 52.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Thanks much very helpful will let you know</p>"], "76128": ["<p>Hello,</p>\n<p>I would like to annotate particular region/area in my image and analyse that area. How should I proceed using cell profiler or other tools?</p>\n<p>To be more specific, I want to check muscle fibre sizes from particular regions of the image. They have been stained with laminin.</p>\n<p>Thanks in advance!</p>", "<p>Hi Swagat,</p>\n<p>Can you post an image of what you\u2019re trying to identify? It\u2019s hard to give specific advice without seeing an image, but I definitely recommend you check out our tutorials here: <a href=\"https://tutorials.cellprofiler.org/\">https://tutorials.cellprofiler.org/</a></p>\n<p>Rebecca</p>", "<p>For QuPath (and most software), annotations could be manual or programmatic, if the staining is specific to the region you are interested in.<br>\nRegardless, I would recommend trying CellPose plugins for whatever you attempt as I generally see them having the best results for things like muscle cell outlines.</p>\n<p>Manual</p><div class=\"youtube-onebox lazy-video-container\" data-video-id=\"FAoUHu40dk0\" data-video-title=\"4 - Simple annotations\" data-provider-name=\"youtube\">\n  <a href=\"https://www.youtube.com/watch?v=FAoUHu40dk0\" target=\"_blank\" rel=\"noopener\">\n    <img class=\"youtube-thumbnail\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/d/5df26214883d577b8edfa496ffc07c73d565c6ba.jpeg\" title=\"4 - Simple annotations\" width=\"690\" height=\"388\">\n  </a>\n</div>\n<p>\nmore manual tips and tricks <a href=\"https://twitter.com/petebankhead/status/1295965138068144129?s=20\">https://twitter.com/petebankhead/status/1295965138068144129?s=20</a></p>\n<p>Automatic <a href=\"https://qupath.readthedocs.io/en/stable/docs/tutorials/pixel_classification.html\" class=\"inline-onebox\">Pixel classification \u2014 QuPath 0.4.3 documentation</a></p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/9/f9cbc2198f8427002d2fddc50a7611347c636ea5.jpeg\" data-download-href=\"/uploads/short-url/zDNrhEL9U2o8KGPEd5W6sClInWd.jpeg?dl=1\" title=\"Test_Imagesc\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9cbc2198f8427002d2fddc50a7611347c636ea5_2_498x499.jpeg\" alt=\"Test_Imagesc\" data-base62-sha1=\"zDNrhEL9U2o8KGPEd5W6sClInWd\" width=\"498\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9cbc2198f8427002d2fddc50a7611347c636ea5_2_498x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9cbc2198f8427002d2fddc50a7611347c636ea5_2_747x748.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/9/f9cbc2198f8427002d2fddc50a7611347c636ea5_2_996x998.jpeg 2x\" data-dominant-color=\"26265C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Test_Imagesc</span><span class=\"informations\">1920\u00d71925 261 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thanks! So, I am working with extraocular muscle. There are different muscle groups present in the image.<br>\nI would like to quantify the muscle fiber size in the region marked with white for example and also count how many of those are positive for green signal and how many for red signal.<br>\nHopefully the question is clearer now.</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6c13cf16add3fb45dfd34f23bb3b028f5258d9a.jpeg\" data-download-href=\"/uploads/short-url/wVlPo2wtDpgeAqdjzHafe0o6I6C.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6c13cf16add3fb45dfd34f23bb3b028f5258d9a_2_433x500.jpeg\" alt=\"image\" data-base62-sha1=\"wVlPo2wtDpgeAqdjzHafe0o6I6C\" width=\"433\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/6/e6c13cf16add3fb45dfd34f23bb3b028f5258d9a_2_433x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6c13cf16add3fb45dfd34f23bb3b028f5258d9a.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/6/e6c13cf16add3fb45dfd34f23bb3b028f5258d9a.jpeg 2x\" data-dominant-color=\"69696A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">634\u00d7731 84.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>With Laminin, I can see the outline of the fibers somewhat clearly.</p>", "<p>Hi again,</p>\n<p>I second what <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> has already said and definitely encourage you to try out cellpose. With Cellpose 2.0 there\u2019s human in the loop training that can further refine the already great models. Give that a shot and if you do come up with a model you like, you can always import it into an existing CellProfiler pipeline and make measurements.</p>\n<p>For annotating specific regions, you could manually define regions using IdentifyObjectsManually in CellProfiler and then identify muscle fibers everywhere, make any measurements, then relate them as child objects to the region parents using RelateObjects. The result will be that the muscle fibers within a region will be \u2018grouped\u2019 into that region, which makes things like looking at counts, average diameter, etc. easier.</p>\n<p>Rebecca</p>", "<p>Thank you very much Rebecca! I will try it out.</p>", "<p>Hi Rebecca, I am interested in doing something like this. I have tissue images, where I segmented cells based on my model in cellpose. Now I would like to use the segmentation obtained in cellpose as a secondary object in cellprofiler. However, I have not figured out how to import the segmentation into cellprofiler.<br>\nThankss</p>", "<p>Hi Paola,</p>\n<p>So there are two ways to approach this:</p>\n<ol>\n<li>Import the model you used to segment cells into CellProfiler and run the runCellpose plugin as part of a larger pipeline</li>\n<li>If you can generate label matrices of your objects (where 0 is background and pixels belonging to an object are set to that object\u2019s number), you can load these as images in CellProfiler then convert them to objects. They will be primary objects, but you can relate them to other objects you detect using RelateObjects.</li>\n</ol>", "<p>Thanks!<br>\nThis is very clear. I have all the images already segmented so I will try to generate the masks.</p>"], "64868": ["<p>I am exporting Cellprofiler data to sqlite db. Output is not having any information on meta data other-than Image number.</p>\n<pre><code class=\"lang-auto\">db_8Gene &lt;- DBI::dbConnect(RSQLite::SQLite(), \"DefaultDB_V5_WCH_Test.db\")\n# All tables in the db\ndbListTables(db_8Gene)\n</code></pre>\n<pre><code class=\"lang-auto\">'Experiment''Experiment_Properties''analysis_Per_Cells''analysis_Per_Cytoplasm''analysis_Per_Experiment''analysis_Per_Image''analysis_Per_Nuclei''analysis_Per_Object''sqlite_sequence'\n</code></pre>\n<pre><code class=\"lang-auto\">head(dbGetQuery(db_8Gene, \"SELECT * FROM analysis_Per_Object\"))\n# \nImageNumber\tObjectNumber\tCells_Number_Object_Number\tCells_AreaShape_Area\tCells_AreaShape_BoundingBoxArea\tCells_AreaShape_BoundingBoxMaximum_X\tCells_AreaShape_BoundingBoxMaximum_Y\tCells_AreaShape_BoundingBoxMinimum_X\tCells_AreaShape_BoundingBoxMinimum_Y\tCells_AreaShape_Center_X\t\u22ef\tNuclei_RadialDistribution_RadialCV_AGP_3of4\tNuclei_RadialDistribution_RadialCV_AGP_4of4\tNuclei_RadialDistribution_RadialCV_DNA_1of4\tNuclei_RadialDistribution_RadialCV_DNA_2of4\tNuclei_RadialDistribution_RadialCV_DNA_3of4\tNuclei_RadialDistribution_RadialCV_DNA_4of4\tNuclei_RadialDistribution_RadialCV_RNA_1of4\tNuclei_RadialDistribution_RadialCV_RNA_2of4\tNuclei_RadialDistribution_RadialCV_RNA_3of4\tNuclei_RadialDistribution_RadialCV_RNA_4of4\n&lt;int&gt;\t&lt;int&gt;\t&lt;int&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t\u22ef\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\t&lt;dbl&gt;\n1 | 1 | 1 | 3927 | 6656 | 991 | 79 | 887 | 15 | 935.2327 | \u22ef | 0.16553513 | 0.1990374 | 0.03438850 | 0.03144753 | 0.04813362 | 0.03801115 | 0.05233609 | 0.09547186 | 0.16553513 | 0.1990374\n1 | 2 | 2 | 3893 | 5724 | 776 | 79 | 670 | 25 | 726.2908 | \u22ef | 0.33165782 | 0.4625496 | 0.04961594 | 0.06391294 | 0.07341505 | 0.02333765 | 0.02945578 | 0.14954596 | 0.33165782 | 0.4625496\n1 | 3 | 3 | 4928 | 9296 | 910 | 122 | 798 | 39 | 848.3827 | \u22ef | 0.11510254 | 0.2674090 | 0.10981347 | 0.10331749 | 0.06733172 | 0.03768227 | 0.10235940 | 0.13507492 | 0.11510254 | 0.2674090\n1 | 4 | 4 | 6419 | 10602 | 703 | 171 | 589 | 78 | 642.9593 | \u22ef | 0.30304762 | 0.5523133 | 0.07199652 | 0.07878089 | 0.05001869 | 0.02598917 | 0.17117269 | 0.19070413 | 0.30304762 | 0.5523133\n1 | 5 | 5 | 3521 | 5865 | 1015 | 131 | 930 | 62 | 968.6757 | \u22ef | 0.14277920 | 0.1784850 | 0.03121570 | 0.08528253 | 0.08466956 | 0.05445241 | 0.10473721 | 0.16204106 | 0.14277920 | 0.1784850\n</code></pre>\n<p>None of the output tables has the well or other meta data available, that is already fed in the pipeline on the meta data session.</p>\n<p>All the mete data information is there in the text output. Is there anything I am doing wrong that prevent the metadata not getting write to the db</p>\n<p>ExportToDatabase options<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/72f3c7f39d39c9a88c03827707f2ea5e6873c962.png\" data-download-href=\"/uploads/short-url/goUJgG8usJgsUuEijNv4tacbqPU.png?dl=1\" title=\"Screen Shot 2022-03-23 at 3.13.32 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72f3c7f39d39c9a88c03827707f2ea5e6873c962_2_690x335.png\" alt=\"Screen Shot 2022-03-23 at 3.13.32 PM\" data-base62-sha1=\"goUJgG8usJgsUuEijNv4tacbqPU\" width=\"690\" height=\"335\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72f3c7f39d39c9a88c03827707f2ea5e6873c962_2_690x335.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72f3c7f39d39c9a88c03827707f2ea5e6873c962_2_1035x502.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72f3c7f39d39c9a88c03827707f2ea5e6873c962_2_1380x670.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/2/72f3c7f39d39c9a88c03827707f2ea5e6873c962_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-03-23 at 3.13.32 PM</span><span class=\"informations\">1450\u00d7704 93.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Cellprofiler version: 4.2.1<br>\nOS: MacOS BigSur</p>\n<p>Thanks,<br>\nShams</p>", "<p>Hi, Shams! Did you figure this one out? I am having the same problem as you!</p>\n<p>Thanks in advance,<br>\nJulie</p>", "<p>Hi folks,</p>\n<p>Sorry, this sounds frustrating! Can you post some images and pipelines so we can try to reproduce? Thanks!</p>", "<p>Hi Beth!</p>\n<p>I found the metadata in the \u201cImage\u201d output instead, and just combined the datasets in R based on ImageNumber. I guess I was just confused switching from ExportToSpreadsheet (where you would get the metadata in the object output files) to ExportToDatabase (where you don\u2019t).</p>\n<p>Julie</p>"], "52583": ["<p>Hello,<br>\nI am trying to learn how to run CellProfiler on a computing cluster with batch processing, but I am running into a problem. After I run<br>\n\u201ccellprofiler -p batch file location/Batch_data.h5 -c -r -f 1 -l 4 -o Output Folder\u201d I get the error \u201cOSError: Test for access to directory failed. Directory: ///Y:/TestImages\u201d. I have the cluster storage mounted on my local computer as the Y drive, and CellProfiler is trying to access the images from there rather than the images\u2019 location in the cluster.<br>\nThe pipeline I\u2019m using (attached at end) has CreateBatchFiles at the end, and I added the appropriate local and cluster root paths for the image locations. I thought that my problem was similar to <a href=\"https://forum.image.sc/t/cellprofiler-v4-0-4-batch-processing/48370\" class=\"inline-onebox\">Cellprofiler_v4.0.4 batch processing</a> where CP is using the local root path instead of the cluster root path. However, when I changed both the local and cluster root paths to the cluster root path just to see what would happen, I get the same error. This leads me to believe that CP is having a problem with reading the filepaths from the image module (since I drag and drop images from the Y drive there for making the batch file).</p>\n<p>I\u2019m not sure what I\u2019m doing incorrectly. I\u2019m new to cluster computing and batch file processing, so any help would be appreciated!<br>\nThank You!<br>\n<a class=\"attachment\" href=\"/uploads/short-url/w6dzVn6o4qaF2koIRhGTUDdRsEO.cpproj\">PracticePipeline_05102021.cpproj</a> (105.9 KB)</p>", "<p>Hmmm, that\u2019s weird, it looks like the find-and-replace that CreateBatchFiles SHOULD be doing from your Y path is not being applied.  It looks like CellProfiler is internally storing that path as <code>///Y:/</code> ; can you try putting that (and/or just <code>Y:/</code> ) in the <code>Local root path</code> part of CreateBatchFiles to see if those batch files seem to be updating the paths correctly?</p>\n<p>On our hand, we\u2019ll work to figure out why it\u2019s storing the paths this way.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45.png\" data-download-href=\"/uploads/short-url/nXNTyls0ohSzblTPKBmhPy2VIOx.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45_2_690x110.png\" alt=\"image\" data-base62-sha1=\"nXNTyls0ohSzblTPKBmhPy2VIOx\" width=\"690\" height=\"110\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45_2_690x110.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45_2_1035x165.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45.png 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7f4a6312646e76b0b2a25c4755e146f73479e45_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1074\u00d7172 62.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/3/33f34a460144c48ec762278720369438fee5f518.png\" alt=\"image\" data-base62-sha1=\"7pzB3xcwxmkLMkOWKGkwg6SSHz2\" width=\"556\" height=\"188\"></p>", "<p>Hi CP Team,<br>\nJust FYI - the solutions suggested to me in <a href=\"https://forum.image.sc/t/cellprofiler-v4-0-4-batch-processing/48370\">Cellprofiler_v4.0.4 batch processing</a> didn\u2019t fix the issue for me either. I ran out of time and ended up reverting to the version 3.1.3 we have installed on our cluster (which works fine!).<br>\nKarla</p>", "<p>Thank you! I made the local root path ///Y:/ and kept the Cluster root path the same as you suggested, and it worked! Cellprofiler was able to go through all the modules for each image.</p>", "<p>I had a similar issue in CellProfiler 4.2.1, when trying to prototype the pipeline on Windows, then moving to a Linux-based cluster.</p>\n<p>I found using that using this mapping in the \u2018CreateBatchFiles\u2019 module:</p>\n<p>Local root path: C:\\images\\project_folder<br>\nCluster root path: /cluster/user/project_folder</p>\n<p>didn\u2019t work, but this did:</p>\n<p>Local root path: ///C:/images/project_folder<br>\nCluster root path: /cluster/user/project_folder</p>\n<p>where I\u2019ve copied the project_folder containing the images onto the cluster server, rather than referencing the same location from both my local Windows machine and the cluster.</p>\n<p>(Diagnosed by looking at the standard error output from the cluster and seeing what CellProfiler thought the filepaths were)</p>", "<p>Hi,</p>\n<p>I am confused about the Local root path. I want to run parallel jobs on Linux cluster where my data is stored. Why it needs the local root path when it not linked to the local computer? Thanks</p>", "<p>Just wondering if anyone can help</p>"], "80236": ["<p>How could I automatically segment these two marked regions, the filaments and the cells? The goal is to measure only the filaments</p>\n<p>Could I do it with Fiji, CellProfiler or Python? Is there a more efficient option?</p>\n<p>Thank you very much, I am attentive to any questions</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/86de9a3ab2bec368b15caa2146c389eadb87296c.png\" data-download-href=\"/uploads/short-url/jf6U5jexGCIJ4eoNZtfdtKyxaK8.png?dl=1\" title=\"Ejemeplo_sc\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86de9a3ab2bec368b15caa2146c389eadb87296c_2_680x500.png\" alt=\"Ejemeplo_sc\" data-base62-sha1=\"jf6U5jexGCIJ4eoNZtfdtKyxaK8\" width=\"680\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/6/86de9a3ab2bec368b15caa2146c389eadb87296c_2_680x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/86de9a3ab2bec368b15caa2146c389eadb87296c.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/6/86de9a3ab2bec368b15caa2146c389eadb87296c.png 2x\" data-dominant-color=\"191919\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Ejemeplo_sc</span><span class=\"informations\">894\u00d7657 80.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>The easiest would probably be a fluorescent marker that localizes only to the cell body.</p>"], "77677": ["<p>Hi,</p>\n<p>I am trying to design CP pipelines to perform the following steps:</p>\n<ol>\n<li>Segment cells and make object measurements (MeasureSizeShape, MeasureIntensity etc), and then save the resulting objects and measurements</li>\n<li>After having used those exported object measurements to produce a classifier in CellProfiler Analyst (using only a subset of that data), I would then like to load all saved objects and their associated measurements into CP and apply the classifier with ClassifyObjects.</li>\n</ol>\n<p>I already have all of the above steps working great, apart from loading objects and their associated data back into CellProfiler in \u2018pipeline 2\u2019. As I want to load measurement data, I think I need to use the LoadData module - but is it even possible to load Object data this way? The documentation says that per-image data can be loaded, and that images/object images can be loaded, but does not mention per-object measurement data.</p>\n<p>I\u2019m hoping it is possible, as otherwise the only solution would be to reload the images and saved objects from the first pipeline, which should be straightforward with the normal input modules, and then rerun all of the measurement modules before applying the classifier. I would like to avoid this if possible, as it will be a very large dataset and doing this will add a lot of computation time on top of the considerable amount there will be already. In addition, I would like to take a look at the different batches of this experiment that pop out prior to doing a final analysis with a shared analysis of all batches with same classifier at the end. If LoadData won\u2019t work, then I\u2019ll have to rerun all the measurement modules every time I want to iterate on the classification step and check overall progress\u2026</p>\n<p>Any help would be really appreciated! Thanks</p>", "<p>Hi,</p>\n<p>So you don\u2019t need to load the measurements again. Instead, you should include the necessary measurement modules to obtain those measurements for each object in your images. I don\u2019t see how this would work any other way unless you\u2019ve already made measurements for every single object across all of your batches already, in which case you could do the classification outside of CellProfiler (e.g., in CellProfiler analyst). In order to classify cells in an image, CellProfiler needs to be able to have their measurements. How else would you want this to work? I\u2019m having trouble imagining the intended behavior here.</p>\n<p>Rebecca</p>", "<p>Hi Rebecca, thanks for the reply.</p>\n<p>I think my original idea is that I would have measurements made via the first pipeline for every object. And then, after training a model, be able to feed these back into cellprofiler to use ClassifyObjects on with that model. However, you\u2019re absolutely correct that I could just run the classification directly in CellProfiler Analyst at that point, and I was overcomplicating it - due to the nature of some previous work, I am used to using CPA as a training tool for a model rather than running the analysis on the whole dataset in CPA as well.</p>\n<p>I\u2019ll add, though, that for a future analysis I think I would want to take these classified objects, filter them for a particular class, and then do follow-up analysis of (for example), morphology of that particular cell type. If the classification was done in CPA (or CellProfiler for that matter) and stored in a .csv somewhere, then is it possible to load the objects and classifications back into CellProfiler for follow-up analysis? Or would I need to take the original segmented objects, make all measurements again, classify again but within CellProfiler using the saved model, and then FilterObjects etc etc to perform that type of analysis?</p>\n<p>Thanks!</p>", "<aside class=\"quote no-group\" data-username=\"timjyb\" data-post=\"3\" data-topic=\"77677\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/timjyb/40/59767_2.png\" class=\"avatar\"> Timjyb:</div>\n<blockquote>\n<p>is it possible to load the objects and classifications back into CellProfiler for follow-up analysis?</p>\n</blockquote>\n</aside>\n<p>Hmmm\u2026 good question. You can certainly use a csv to load metadata for images, but this is on the level of the image, not on objects. What type of analysis are you imagining? In general, I think you\u2019re going to want to re-measure, classify, and then filter all in the same pipeline. Potentially you could avoid going back to CellProfiler if you have the classifications already and just want to, say look at whether a measurement you\u2019ve already made differs between the groups. You can do this in CellProfiler Analyst, excel or any other software by filtering the measurements on the classification column.</p>", "<p>Yes it would be easiest to have it all in one continuous pipeline, but as I\u2019m training a model on the dataset for classification then unfortunately that\u2019s not possible for my case. The main thing I have in mind is morphology analysis on a particular cell type (for which the cells were originally stained for a membrane stain for that cell type). But after having used a trained classifier originally to classify nuclei across the multiple classes of cells in these cultures. So this would have to be an image-based analysis (was imagining FilterObjects by the desired class to isolate nuclei of this cell type, then IdentifySecondaryObjects or similar to then essentially do a marker-based watershed to obtain cell masks for shape measurements), rather than being based on existing measurements unfortunately.</p>\n<p>Of course, this is a very niche case! Just wanted to see if it was possible, perfectly understandable if it isn\u2019t. Sounds like the best alternative would be to reload nuclei objects, make all measurements and classify again and then work from there. All just adds to computing time of course</p>", "<p>I see! Yes I don\u2019t know of a way do this currently, sorry. The alt approach you mention is probably the best way I know of for now.</p>"], "72556": ["<p>Hi, there is currently no way to use extracted metadata for saving numerical data in CP, only for images. It would be very useful to be able to save the txt files using metadata when doing execution on a cluster. The data gets parsed to different nodes and then needs to be saved in different folders as they cannot be individually named with a variable. Would be nice to implement this. Thanks</p>", "<p>Hi Marc,</p>\n<p>You can instead of writing your output spreadsheets to Default Output Folder, set them to write to Default Output Folder Subfolder, and then create subfolders that are metadata structured to match your grouping. I hope that helps!</p>", "<p>Hi Beth,</p>\n<p>sorry for the late reply. What you propose should indeed work, I am now doing exactly that, but in an external python script that is used by snakemake to dispatch the jobs. The \u201cissue\u201d is that one ends up with a lot of files with exactly the same name but different paths. I always worry about people accessing data manually and moving stuff around, so that the origin of the data becomes cloudy. I always associate the metadata with objects, so opening the file rectifies this, but requires opening the files. Thanks.</p>", "<p>Hey Marc,</p>\n<p>Feel free to make a GH issue for a feature request if there isn\u2019t one there already!</p>", "<p>Hi Beth,</p>\n<p>it sounds so easy when you say that. How do I do a request where? Is that in the forum under the Development category? Thanks.<br>\nMarc</p>", "<p>Hi <a class=\"mention\" href=\"/u/marckobudo\">@MarcKobudo</a></p>\n<p>You can request your feature on our GitHub account under issues here: <a href=\"https://github.com/CellProfiler/CellProfiler/issues\" class=\"inline-onebox\">Issues \u00b7 CellProfiler/CellProfiler \u00b7 GitHub</a></p>\n<p>In this page select New Issue (green button) and than select Feature Request.</p>\n<p>Best,<br>\nMario</p>", "<p>Hi Mario,<br>\nI have done just that, Thanks.<br>\nMarc</p>"], "80752": ["<p><strong>Background</strong><br>\nIn our research, we aim to determine the sex differences in coronary vascularization and innervation in cardiac tissue samples using immunohistochemistry.</p>\n<p>In the figure, you can appreciate the vessels that are immunohistochemically stained with anti-von Willebrand factor. Some arteries have a large diameter and can be characterized as macrovasculature, whereas other vessels are small and are categorized as microvasculature. Of all vessels, some are perfectly circular, whereas others are more ellipse.</p>\n<h3>\n<a name=\"analysis-goals-1\" class=\"anchor\" href=\"#analysis-goals-1\"></a>Analysis goals</h3>\n<p>To determine the vascularization differences between males and females, we aim to determine the number of vessels in the cardiac samples we\u2019ve obtained as well as the diameter of the vessels. The diameter of the vessels is done to categorize the vessels in macro- and microvasculature, which is further needed for the determination of sex differences.</p>\n<h3>\n<a name=\"challenges-2\" class=\"anchor\" href=\"#challenges-2\"></a>Challenges</h3>\n<p>We experience some trouble calculating the diameter of the vessels in the cardiac samples, as we want to correct for the fact that a vessel/object might not be perfectly circular. From CellProfiler, we determined the objects by the IdentifyPrimaryObjects. We\u2019ve obtained multiple data characteristics from the MeasureObjectSizeandShape module, such as area, perimeter, formfactor, eccentricity, majoraxislength, minoraxislength, among others. We thought, especially the eccentricity and formfactor are suitable to correct for non-circular objects. However, we thought we should also use the majoraxislength and minoraxislength in the calculation. We saw some articles referring to an ellipse equation for measuring the diameter. However, this method is complex and therefore we think this might not be the easiest way to calculate the diameter of these vessels using CellProfiler. Besides, we also did not find any related forum topics. Attached to this post, we added the CellProfiler pipeline we\u2019ve created to measure the vessel diameter.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/f/6f3d9680b9be2aa73c9edf3089f613b276e5264a.jpeg\" data-download-href=\"/uploads/short-url/fS4WWscQC4Q8ittWYrVaq0PAC4G.jpeg?dl=1\" title=\"H1_vWF_85_p1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/f/6f3d9680b9be2aa73c9edf3089f613b276e5264a_2_499x375.jpeg\" alt=\"H1_vWF_85_p1\" data-base62-sha1=\"fS4WWscQC4Q8ittWYrVaq0PAC4G\" width=\"499\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/f/6f3d9680b9be2aa73c9edf3089f613b276e5264a_2_499x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/f/6f3d9680b9be2aa73c9edf3089f613b276e5264a_2_748x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/f/6f3d9680b9be2aa73c9edf3089f613b276e5264a_2_998x750.jpeg 2x\" data-dominant-color=\"DFD4DE\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">H1_vWF_85_p1</span><span class=\"informations\">1920\u00d71440 241 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Does anyone have experience calculating (vessel) diameter correcting for non-circular objects in CellProfiler?</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/vrIFnWyd83pTuOgo8MhrNaXrFHV.cpproj\">Vessel diameter_Custom.cpproj</a> (548.7 KB)</p>"], "80242": ["<p>I am new to CP and tried to find the answer to my question on this forum earlier, apologies if it\u2019s already been answered. I am trying to analyze the colocalization of pre and post-synaptic puncta, approximately localized in the neurites. I stained my neurites with B3-tubulin/Tuj1, which gave me a somewhat punctate staining . How do I turn this punctate staining into a continuous mask? This is an example of what my neurites look like.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/4/c4fe665f5884ece7af91a7fffb12fb37e9f16c94.jpeg\" data-download-href=\"/uploads/short-url/s6GDh27QQLVgtfSDjk0MWIm2bcM.jpeg?dl=1\" title=\"GT_iN12_synapse_810_1_3_Tuj1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4fe665f5884ece7af91a7fffb12fb37e9f16c94_2_500x500.jpeg\" alt=\"GT_iN12_synapse_810_1_3_Tuj1\" data-base62-sha1=\"s6GDh27QQLVgtfSDjk0MWIm2bcM\" width=\"500\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4fe665f5884ece7af91a7fffb12fb37e9f16c94_2_500x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4fe665f5884ece7af91a7fffb12fb37e9f16c94_2_750x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/4/c4fe665f5884ece7af91a7fffb12fb37e9f16c94_2_1000x1000.jpeg 2x\" data-dominant-color=\"170117\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">GT_iN12_synapse_810_1_3_Tuj1</span><span class=\"informations\">1460\u00d71460 324 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>hi <a class=\"mention\" href=\"/u/gt632\">@gt632</a>,</p>\n<p>the EnhanceorSupressFeatures module has a \u201cneurite\u201d specification under \u201cfeature type\u201d- can\u2019t hurt to give it a try!</p>\n<p>-tatiana</p>"], "77683": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c81a31f742afd69a618a9ed36272a265287625d2.png\" data-download-href=\"/uploads/short-url/sybCfKVdaxDLrDoAk1dvFNSsxOi.png?dl=1\" title=\"analyst_variables\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c81a31f742afd69a618a9ed36272a265287625d2_2_324x500.png\" alt=\"analyst_variables\" data-base62-sha1=\"sybCfKVdaxDLrDoAk1dvFNSsxOi\" width=\"324\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c81a31f742afd69a618a9ed36272a265287625d2_2_324x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c81a31f742afd69a618a9ed36272a265287625d2_2_486x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c81a31f742afd69a618a9ed36272a265287625d2_2_648x1000.png 2x\" data-dominant-color=\"ECEDEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">analyst_variables</span><span class=\"informations\">874\u00d71348 180 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/1/91ce4e08252d5542abbdba26932c824f007006b1.png\" data-download-href=\"/uploads/short-url/kNRePCBnaiwckdvtjqJs08fBFGp.png?dl=1\" title=\"profiler_1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/1/91ce4e08252d5542abbdba26932c824f007006b1_2_690x258.png\" alt=\"profiler_1\" data-base62-sha1=\"kNRePCBnaiwckdvtjqJs08fBFGp\" width=\"690\" height=\"258\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/1/91ce4e08252d5542abbdba26932c824f007006b1_2_690x258.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/1/91ce4e08252d5542abbdba26932c824f007006b1.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/1/91ce4e08252d5542abbdba26932c824f007006b1.png 2x\" data-dominant-color=\"E9E9EA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">profiler_1</span><span class=\"informations\">954\u00d7358 49.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/1/e1a9681f422a396b36540ddeadaeb1ed6de7e6a7.png\" alt=\"profiler_error\" data-base62-sha1=\"wcinKAZiF7TlNcEs1RQ2rbMGhCv\" width=\"639\" height=\"298\"><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1418e75c6d017c1c68282e16e723163d2a4a0fc.png\" data-download-href=\"/uploads/short-url/yqfwwaK4iwALfAbjYQq0orYdA60.png?dl=1\" title=\"profiler_2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1418e75c6d017c1c68282e16e723163d2a4a0fc_2_690x493.png\" alt=\"profiler_2\" data-base62-sha1=\"yqfwwaK4iwALfAbjYQq0orYdA60\" width=\"690\" height=\"493\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/1/f1418e75c6d017c1c68282e16e723163d2a4a0fc_2_690x493.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1418e75c6d017c1c68282e16e723163d2a4a0fc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/1/f1418e75c6d017c1c68282e16e723163d2a4a0fc.png 2x\" data-dominant-color=\"D4DFE9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">profiler_2</span><span class=\"informations\">982\u00d7703 93.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>When I open my properties file on CellProfiler Analyst, I am only given the option to select from variables for one selected group that was chosen during pipeline construction on CellProfiler. However, I want to be able to select variables from all 5 groups on CellProfiler Analyst but when I try to choose all 5 groups on CellProfiler and click Analyze, I receive an error.</p>\n<p>Any suggestions for what I could do? Thank you.</p>", "<p>Have you tried making a single table per object? That is what the error code suggests doing, but in your screenshot, I see you have selected \u201cSingle object table\u201d which will put all the objects into a single table.</p>", "<p>When I try running it with one table per object type, I receive a similar message.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/c/8cd71efc063a61711cd5b73d9fb74daf7d288347.png\" alt=\"new error\" data-base62-sha1=\"k5VJUFWWDC3fW0P9FnGF5qrP2HZ\" width=\"655\" height=\"301\"></p>", "<p>Can you show me the export module settings for when you got that error?</p>", "<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24724d2223a96907d11eed724b4ea2728c15b1f3.png\" data-download-href=\"/uploads/short-url/5cq4RacwyO9hM7A0iu2SgZyvmp5.png?dl=1\" title=\"export1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24724d2223a96907d11eed724b4ea2728c15b1f3_2_690x419.png\" alt=\"export1\" data-base62-sha1=\"5cq4RacwyO9hM7A0iu2SgZyvmp5\" width=\"690\" height=\"419\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24724d2223a96907d11eed724b4ea2728c15b1f3_2_690x419.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24724d2223a96907d11eed724b4ea2728c15b1f3_2_1035x628.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24724d2223a96907d11eed724b4ea2728c15b1f3_2_1380x838.png 2x\" data-dominant-color=\"F0F0F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">export1</span><span class=\"informations\">2245\u00d71365 366 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/0/10ff511dab3a2e83a3f880169367f8aebd0094fe.png\" data-download-href=\"/uploads/short-url/2qmEvzudfpYhXs8bhGF9z4EZdmu.png?dl=1\" title=\"export2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10ff511dab3a2e83a3f880169367f8aebd0094fe_2_690x396.png\" alt=\"export2\" data-base62-sha1=\"2qmEvzudfpYhXs8bhGF9z4EZdmu\" width=\"690\" height=\"396\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10ff511dab3a2e83a3f880169367f8aebd0094fe_2_690x396.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10ff511dab3a2e83a3f880169367f8aebd0094fe_2_1035x594.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/0/10ff511dab3a2e83a3f880169367f8aebd0094fe_2_1380x792.png 2x\" data-dominant-color=\"F0F0F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">export2</span><span class=\"informations\">2230\u00d71281 373 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/b/6b1539826e153afdd3760e438ba76996d9483c76.png\" data-download-href=\"/uploads/short-url/fhiyTUMoeNp02i5As01UoHjBrXo.png?dl=1\" title=\"export3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1539826e153afdd3760e438ba76996d9483c76_2_690x426.png\" alt=\"export3\" data-base62-sha1=\"fhiyTUMoeNp02i5As01UoHjBrXo\" width=\"690\" height=\"426\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1539826e153afdd3760e438ba76996d9483c76_2_690x426.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1539826e153afdd3760e438ba76996d9483c76_2_1035x639.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1539826e153afdd3760e438ba76996d9483c76_2_1380x852.png 2x\" data-dominant-color=\"E7EBEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">export3</span><span class=\"informations\">2253\u00d71393 422 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/4/e47757aad676f7d4372689e0fda47a23ba9ac41c.png\" data-download-href=\"/uploads/short-url/wB6yiOVyjTNw6kshHAyMunkkuXq.png?dl=1\" title=\"export4\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e47757aad676f7d4372689e0fda47a23ba9ac41c_2_690x264.png\" alt=\"export4\" data-base62-sha1=\"wB6yiOVyjTNw6kshHAyMunkkuXq\" width=\"690\" height=\"264\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e47757aad676f7d4372689e0fda47a23ba9ac41c_2_690x264.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e47757aad676f7d4372689e0fda47a23ba9ac41c_2_1035x396.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e47757aad676f7d4372689e0fda47a23ba9ac41c_2_1380x528.png 2x\" data-dominant-color=\"D0DEEA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">export4</span><span class=\"informations\">1765\u00d7676 88.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Thanks for including this! This is very helpful. I would like to be able to try this out to see if the issue replicates and troubleshoot this further. Can you attach a minimal example (1 image and your CellProfiler pipeline)?</p>", "<p><a class=\"attachment\" href=\"/uploads/short-url/eCFLGjUXQI2XylOap56TPK4pqeX.cpproj\">Untreated all channels.cpproj</a> (1.2 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/cqazOeY0GlHKWi9ukA2BVLKYQwb.tif\">U_1_1.tif</a> (5.9 MB)</p>\n<p>Thank you so much! Please let me know if I need to provide anything else for you.</p>"], "81270": ["<p>Hello,</p>\n<p>In CellProfiler 4.2.1, is it possible to group three wells together and calculate the average of their output data (e.g., MeasureObjectIntensity)? I need to compare the negative control, treatment group, and positive control, each of which consists of technical triplicates.</p>\n<p>Thank you,<br>\nSaana</p>"], "76663": ["<p><a class=\"attachment\" href=\"/uploads/short-url/kIiShhVImD2GoU8EIXV3ZqqSpKi.cpproj\">SatCell To forum - Threshold and discard based on size.cpproj</a> (1.3 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/cgzcAVVnEkzJ41h4bakvRANJty6.TIF\">TSCC_03_SC_2-8_c5.TIF</a> (2.8 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/9z83VPmuh5rRYBwg30nDC9RyzqD.TIF\">TSCC_03_SC_2-8_c2.TIF</a> (2.8 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/6uVLi4SuNJGk1SlXE6o8cPhfffg.TIF\">TSCC_03_SC_2-8_c3.TIF</a> (2.8 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/4RKIG7cEscx8o549kAUbCOXuQQd.TIF\">TSCC_03_SC_2-8_c1.TIF</a> (2.8 MB)</p>\n<h3>\n<a name=\"background-1\" class=\"anchor\" href=\"#background-1\"></a>Background</h3>\n<p>Hello, I currently analyse pictures from muscle biopsies in VL. Attached is a sample picture and a simple pipeline.</p>\n<h3>\n<a name=\"analysis-goals-2\" class=\"anchor\" href=\"#analysis-goals-2\"></a>Analysis goals</h3>\n<p>I want to count the amount of satellite cells based on colocation of Pax7 positive (Chanel 1) and DAPI positive [Nuclei] (Chanel 5).</p>\n<p>These semi-automated resulted will, at least for now, be validated by manuel counting. So I\u2019ll rather have false positives (too low of a threshold), than false negatives.</p>\n<h3>\n<a name=\"challenges-3\" class=\"anchor\" href=\"#challenges-3\"></a>Challenges</h3>\n<p>Due to the low threshold, a large amount of single-to-few-pixel \u201cobjects\u201d are being converted to objects (See screenshot below).</p>\n<p>My question is: <strong>Is it possible to discard objects (very small) based on size after the ConvertImageToObjects module?</strong></p>\n<p>Any help will be greatly appreciated,</p>\n<ul>\n<li>Jonas S. Mathiesen, University of Southern Denmark</li>\n</ul>\n<p>A screenshot of the problem with the Threshold Module is attached below<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcdcec52a5c667bdc02a53b0986a52e69e43d345.png\" data-download-href=\"/uploads/short-url/qWL5AgIHre5nkIRooP37MPjmYzX.png?dl=1\" title=\"Screenshots of Threshold Module (non-zoomed)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcdcec52a5c667bdc02a53b0986a52e69e43d345_2_690x452.png\" alt=\"Screenshots of Threshold Module (non-zoomed)\" data-base62-sha1=\"qWL5AgIHre5nkIRooP37MPjmYzX\" width=\"690\" height=\"452\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/c/bcdcec52a5c667bdc02a53b0986a52e69e43d345_2_690x452.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcdcec52a5c667bdc02a53b0986a52e69e43d345.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/c/bcdcec52a5c667bdc02a53b0986a52e69e43d345.png 2x\" data-dominant-color=\"8C8C8C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshots of Threshold Module (non-zoomed)</span><span class=\"informations\">865\u00d7567 72.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/b/3b1813b076a4004fc2cf6a7d56e39474dfb971a1.png\" data-download-href=\"/uploads/short-url/8qLKPnqfiY37XLHsPrk23BRvQhX.png?dl=1\" title=\"Screenshot of threshold Module (zoomed)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/b/3b1813b076a4004fc2cf6a7d56e39474dfb971a1_2_683x500.png\" alt=\"Screenshot of threshold Module (zoomed)\" data-base62-sha1=\"8qLKPnqfiY37XLHsPrk23BRvQhX\" width=\"683\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/b/3b1813b076a4004fc2cf6a7d56e39474dfb971a1_2_683x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/b/3b1813b076a4004fc2cf6a7d56e39474dfb971a1.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/b/3b1813b076a4004fc2cf6a7d56e39474dfb971a1.png 2x\" data-dominant-color=\"959595\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot of threshold Module (zoomed)</span><span class=\"informations\">879\u00d7643 114 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<hr>", "<p>If you can\u2019t improve the segmentation to get fewer false positives, I would suggest to do the filtering as part of the downstream analysis. You may want to have a closer look at those small objects. Otherwise, have a look at the CP FilterObjects module.</p>", "<p>Hi,</p>\n<p>Have you tried using IdentifyPrimaryObjects rather than a Threshold+ConvertImageToObjects module-pairing? That would allow you to set a minimum object diameter, and have everything in one module rather than two. If for whatever reason you have and that isn\u2019t working, you can measure the object sizes with MeasureObjectSizeAndShape and then throw out the small ones with FilterObjects.</p>\n<p>Good luck!</p>", "<p>Hello Beth and others</p>\n<p>Due to the noise-signal ratio, IdentifyPrimaryObjects was not feasible.<br>\nMeasureObjectSizeShape+FilterObjects was on the other hand a succes. I had not previously used this module to filter for size, only intensity for fiber typing.</p>\n<p>Thanks for help, and keep up the good work in this forum. It is greatly appreciated for inexperienced CP users like myself.</p>\n<p><em>Jonas S Mathiesen</em></p>"], "79224": ["<p>Hi cellprofiler team and enthusiasts!</p>\n<p>I have been using CP for single-cell image analysis where I have 150+ images of the same field, x16 animals x15 fields per animal. It is a large dataset.</p>\n<p>When I analyze these data in CP, I have my regex for the \u201cmetadata\u201d working. <strong>My question:</strong> is there any way to use the metadata to <strong>automatically</strong> derive every unique channel for the \u201cNames and Types\u201d module? To date, I am manually adding each channel, but, as you can imagine it is quite onerous.</p>\n<p>For example:<br>\nRSI_CD_6_A_1_M_1}R_10_K_561_3NT_L_3-NT_C4 (series 01).tif<br>\nRSI_CD_6_A_1_M_1}R_11_K_561_3NT_L_DAPI_C4 (series 01).tif<br>\nI use metadata/regex to extract the \u201cchannel\u201d which would be:<br>\nL_3-NT_C4<br>\nL_DAPI_C4<br>\n\u2026 for these two</p>\n<p>Now it would be amazing if I could specify in the names and types module that every unique value in this metadata field is my name / channel, so I don\u2019t have to manually input all 150+ channel names. Thoughts?</p>\n<p>Thanks as always for this amazing software!</p>"], "71035": ["<p>Hello,</p>\n<p>I\u2019m relatively new to image analysis with  CellProfiler. With some help from the forum I\u2019ve managed to construct an analysis pipeline for mutlichannel fluorescent images of tissue sections stained with multiple markers for cell identification and counting (yay!).</p>\n<p>The pipeline does the following:</p>\n<ol>\n<li>Separates the merged .ND2 file into its constituent channels using ColortoGray</li>\n<li>Identifies 3 primary objects - nuclei (DAPI), immune cells (cell surface marker), and proliferating nuclei (Ki-67)</li>\n<li>Applies an image-specific mask generated in Fiji to confine the analysis to nuclei/cells within a specific region of interest</li>\n<li>Uses the immune cell marker as a mask to identify the nuclei belonging strictly to these cells</li>\n<li>Subsequently uses the immune cell nuclei as a mask to apply to the proliferating objects to identify proliferating immune cells.</li>\n</ol>\n<p>From this I am interested in/able to extract the following data generated from the pipeline:</p>\n<ol>\n<li>Total number of nuclei</li>\n<li>Total number of proliferating cells (i.e. Ki-67 masked with DAPI)</li>\n<li>Total number of nuclei belonging to immune cells (i.e. DAPI masked with cell surface marker)</li>\n<li>Total number of immune cell nuclei that are Ki-67 positive (Ki-67 masked within immune cell-associated nuclei)</li>\n</ol>\n<p>Using the \u2018ExportToSpreadsheet\u2019 module I\u2019m able to extract these data, however my problem lies in that CP generates individual CSV\u2019s for each object type, and each of these files contains a row for every individual object generated. At the present time I\u2019m only interested in having one summary table of the total object count (i.e. each row is an image, and each column is the total count for the object type - nuclei, proliferating cells etc.).</p>\n<p>I could curate this data manually from the exported csv\u2019s using a pivot table, and I imagine there might be a way to do curate this using CellProfiler Analyst (?), but I\u2019m wondering if there is quick a way to obtain this type of summary directly from the CP pipeline in a .csv format?</p>\n<p>Thanks for all the help thus far!</p>\n<p>Christian</p>", "<aside class=\"quote no-group\" data-username=\"ChristianBellissimo\" data-post=\"1\" data-topic=\"71035\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/christianbellissimo/40/60957_2.png\" class=\"avatar\"> Christian Bellissimo:</div>\n<blockquote>\n<p>each row is an image, and each column is the total count for the object type - nuclei, proliferating cells etc</p>\n</blockquote>\n</aside>\n<p>CellProfiler can definitely export this type of table. To do this, use these settings in ExportToSpreadsheet:</p>\n<ul>\n<li>Near the bottom, select \u201cNo\u201d to \u201cExport all measurement types\u201d</li>\n<li>This will make a new dropdown appear called \u201cData to Export\u201d. Select \u201cImage\u201d</li>\n<li>If you also want per-image mean or median measurements, you can select \u201cYes\u201d to the \u201cCalculate the per-image\u2026measurements\u201d</li>\n</ul>", "<p>This worked perfectly, thanks Rebecca!</p>", "<p>I apply a pipeline where in the end I use filter object module which gives me an image and a data table. and I want to save this data table. But when I select to save the filterobject from the drop down menu, no values are saved. Just the boxes with labels.</p>"], "77181": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p><a class=\"attachment\" href=\"/uploads/short-url/ftbCwHurbHTJ1FA6M3hVEfO20tf.zip\">Sample Images.zip</a> (7.8 MB)</p>\n<h3>\n<a name=\"backgroundanalysis-goals-2\" class=\"anchor\" href=\"#backgroundanalysis-goals-2\"></a>Background/Analysis goals</h3>\n<p>Hello!</p>\n<p>I had a question regarding the feasibility of a pipeline approach. I would like to analyze the levels of autophagy for cells, whose autophagosomes have been tagged with an RFP-GFP-LC3 tandem protein.</p>\n<p>I was wondering if there was a way in which I could count the number of autophagosomes per cell. I had tried the IdentifySecondaryObjects module, but from my understanding, it only works for the use of a primary object as a seed to aid in the identification of a larger, secondary object. Furthermore, at times the signal can be quite diffused in some cells and the individual autophagosomes can be fairly small.</p>\n<p>Because of this, I am uncertain if I have the right approach. I am still fairly new to the realm of image analysis, so I would appreciate any guidance on the problem. I have also attached a sample image set for context. The image tagged at the end with w2 is the RFP channel, while the one labeled w3 is the GFP channel.</p>\n<p>Thank you!</p>", "<p>Are the autophagosomes the spots in these cells?<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/f/4f425eb3cd64aa7534bb5a58e3a769de41d29eeb.png\" alt=\"image\" data-base62-sha1=\"bj9URhvksLUVUrFa3bMx1KFV1h1\" width=\"622\" height=\"390\"></p>\n<p>In general, to count something within a cell, the approach is usually:</p>\n<ol>\n<li>Identify the nucleus (IdentifyPrimaryObjects)</li>\n<li>Identify the cell (IdentifySecondaryObjects)</li>\n<li>Identify the spots/small objects (IdentifyPrimaryObjects)</li>\n<li>Make measurements</li>\n<li>Relate the spots/small objects to the cell (RelateObjects)</li>\n</ol>\n<p>If you don\u2019t have a nuclear marker, you can still identify cells with IdentifyPrimaryObjects then identify the autophagosomes with IdentifyPrimaryObjects and relate them to each other.</p>", "<p>Hi Rebecca,</p>\n<p>Thank you so much for the advice! Yes, the spots in the image are the autophagosomes. Because of their size, would it help to use speckle enhancement to better segment them, or would setting a low size range in IdentifyPrimaryObjects be sufficient? I will make sure to try the approach you suggested and reach out if I have any additional questions/issues.</p>", "<p>Hi again, since it\u2019s easier I would just try playing around with IdentifyPrimaryObject settings first. But yes a really common way we deal with images like this is to use EnhanceOrSuppressFeatures to enhance spots before attempting IdentifyPrimaryObjects. I would probably try both approaches and see what works better!</p>"], "76670": ["<p>Hello,</p>\n<p>The Counting and Scoring CellProfiler example is very helpful. I am looking to count the percentage of positive cells for two colors (number of AF647 positive cells and number of AF555 positive cells, counted independently, DAPI to identify primary objects). Is there an example pipeline or tutorial that would be recommended to do this?</p>\n<p>Thank you,</p>\n<p>John</p>"], "76158": ["<p>The CellProfiler wiki has a <a href=\"https://github.com/CellProfiler/CellProfiler/wiki/s3\" rel=\"noopener nofollow ugc\">page</a> from 2018 that says CellProfiler can load images, pipelines, and metadata CSV\u2019s from s3. I cannot find any documentation for this on Google. Does anyone do this with the standard CP package? If so, how?</p>", "<p>I don\u2019t know if CellProfiler now has a built-in way of accessing S3 but you can use <a href=\"https://github.com/s3fs-fuse/s3fs-fuse\" rel=\"noopener nofollow ugc\">s3fs-fuse</a> or <a href=\"https://github.com/kahing/goofys\" rel=\"noopener nofollow ugc\">goofys</a> to \u201cpseudo-mount\u201d your bucket.<br>\nIf you want to give this a try with your images already in an S3 bucket, you can use CellProfiler in <a href=\"https://band.embl.de\" rel=\"noopener nofollow ugc\">BAND</a>, our virtual desktop platform. You\u2019ll have to go to Applications &gt; Data Access &gt; Mount S3 Bucket then find CellProfiler under Applications &gt; Image Analysis.</p>", "<p>Definitely check out Distributed CellProfiler. This is what we use in production to connect to S3 and analyze images in the cloud. Here\u2019s our documentation for DCP: <a href=\"https://distributedscience.github.io/Distributed-CellProfiler/overview.html\" class=\"inline-onebox\">What is Distributed-CellProfiler? \u2014 Documentation</a></p>"], "53120": ["<p>Hello everyone.<br>\nI am new to cellprofiler.<br>\nI would like to use a cellprofiler to identify cells and count cells with positive PI.<br>\nis this possible?<br>\nAny help would be appreciated.<br>\nThank you.</p>\n<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<ul>\n<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox) \u2013 (make sure however that you are allowed to share the image data publicly under the conditions of this forum).</li>\n<li>Share a <a href=\"https://en.wikipedia.org/wiki/Minimal_working_example\" rel=\"noopener nofollow ugc\">minimal working example</a> of your macro code.</li>\n</ul>\n\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<ul>\n<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific \u201cjargon\u201d.</li>\n</ul>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<ul>\n<li>What information are you interested in getting from this image?</li>\n</ul>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<ul>\n<li>What stops you from proceeding?</li>\n<li>What have you tried already?</li>\n<li>Have you found any related forum topics? If so, cross-link them.</li>\n<li>What software packages and/or plugins have you tried?</li>\n</ul>", "<p>Hi <a class=\"mention\" href=\"/u/biy\">@BIY</a>,</p>\n<p>Yes. It should be possible to segment. Since you are new to the CellProfiler, the following tutorials could be useful &amp; check out the <a href=\"https://cellprofiler.org/examples\" rel=\"noopener nofollow ugc\">example pipelines</a>.<br>\nPlease share your sample image &amp; details about your problem in the above given format, so that we could help you better.</p>\n<div class=\"youtube-onebox lazy-video-container\" data-video-id=\"eriZdORpFxs\" data-video-title=\"CellProfiler Workshop\" data-provider-name=\"youtube\">\n  <a href=\"https://www.youtube.com/watch?v=eriZdORpFxs\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n    <img class=\"youtube-thumbnail\" src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/2/724b1391c9fb4413aa5200c904ce72c981dba1bc.jpeg\" title=\"CellProfiler Workshop\" width=\"480\" height=\"360\">\n  </a>\n</div>\n\n<p>Regards,<br>\nLakshmi<br>\n<a href=\"http://www.wakoautomation.com/\" rel=\"noopener nofollow ugc\">www.wakoautomation.com</a></p>", "<aside class=\"quote no-group\" data-username=\"lakshmi\" data-post=\"2\" data-topic=\"53120\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/l/ea5d25/40.png\" class=\"avatar\"> Lakshmi Balasubramanian:</div>\n<blockquote>\n<p>Lakshmi</p>\n</blockquote>\n</aside>\n<p>Hi Lakshmi,  I have a question, would you pleas help me.</p>\n<p>I want to count the area of macrophages in the picture, but there are one or two cells in the picture that are not macrophages, how can I remove these two cells data. Also, some cells stick together and form cell clusters, which is also not what I want, and I want to remove them. How do I achieve this?</p>", "<p>Hi <a class=\"mention\" href=\"/u/fuxiang_li\">@Fuxiang_Li</a></p>\n<p>Welcome to the forum!</p>\n<p>I would advise you to start your own post as it\u2019s usually a better shot of getting seen by the right people.</p>\n<p>Based on your question, I would really advise you to include an example image if you can because it will quite hard to help without that.</p>"], "79238": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<ul>\n<li>Upload an <em>original</em> image file here directly or share via a link to a file-sharing site (such as Dropbox) \u2013 (make sure however that you are allowed to share the image data publicly under the conditions of this forum).</li>\n<li>Share a <a href=\"https://en.wikipedia.org/wiki/Minimal_working_example\" rel=\"noopener nofollow ugc\">minimal working example</a> of your macro code.</li>\n</ul>\n\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<ul>\n<li>What is the image about? Provide some background and/or a description of the image.  Try to avoid field-specific \u201cjargon\u201d.</li>\n</ul>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<ul>\n<li>What information are you interested in getting from this image?</li>\n</ul>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<ul>\n<li>\n<p>What stops you from proceeding?<br>\nthe montage picture should be intact, the reality is after I stitched it, it became one picture containing different squares, and the margin of each small square was bright and would have a side effect on identifying primary objects.</p>\n</li>\n<li>\n<p>What have you tried already?</p>\n</li>\n<li>\n<p>Have you found any related forum topics? If so, cross-link them.</p>\n</li>\n<li>\n<p>What software packages and/or plugins have you tried?<br>\n<a class=\"attachment\" href=\"/uploads/short-url/y7SBYw4n7Sy4yLt66vrmp5a8qC.tif\">DAPI-GFP-PI_B5_1_001.tif</a> (8.4 MB)</p>\n</li>\n</ul>", "<p>Hello <a class=\"mention\" href=\"/u/lanjianqiang\">@lanjianqiang</a>,</p>\n<p>You can try to use rolling ball background subtraction after stitching. This should eliminate the variable illumination. I believe the default Gen5 rolling ball settings aren\u2019t ideal, so you could try setting the rolling ball diameter to ~30 microns to start (depending on the size of your objects) and increase or decrease from there.</p>", "<p>Hi <a class=\"mention\" href=\"/u/mposkus\">@mposkus</a>, really appreciate your help. I adjusted the rolling ball diameter to 90 microns and it seems like the problem had been solved. Now the image is like<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ceecc9a306c14a541972d766840a1a8dd6d9c5b7.jpeg\" data-download-href=\"/uploads/short-url/twxFNePNNF0Epq6gXWxTb9OkPpZ.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ceecc9a306c14a541972d766840a1a8dd6d9c5b7.jpeg\" alt=\"image\" data-base62-sha1=\"twxFNePNNF0Epq6gXWxTb9OkPpZ\" width=\"494\" height=\"500\" data-dominant-color=\"010205\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">863\u00d7872 34.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nThank you again for your valuable advice. <img src=\"https://emoji.discourse-cdn.com/twitter/rose.png?v=12\" title=\":rose:\" class=\"emoji\" alt=\":rose:\" loading=\"lazy\" width=\"20\" height=\"20\"> <img src=\"https://emoji.discourse-cdn.com/twitter/rose.png?v=12\" title=\":rose:\" class=\"emoji\" alt=\":rose:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p><a class=\"mention\" href=\"/u/lanjianqiang\">@lanjianqiang</a> I\u2019m glad it worked!</p>"], "76166": ["<p>Hi there,</p>\n<p>We are doing an analysis that relies on an expansion from a single pixel. The problem is that when one object touches other, the expansion stops. You can find an example in the attached image.<br>\nIf we run one object at the time, this does not happen. How can we do it easily on CellProfiller?</p>\n<p>Thank you for your help!</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6dcf943f35700d57beac79e1a64f14e9c6816764.png\" data-download-href=\"/uploads/short-url/fFqMnXCGRc8ybjZfiTNZG5epU4k.png?dl=1\" title=\"Picture1\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6dcf943f35700d57beac79e1a64f14e9c6816764_2_498x500.png\" alt=\"Picture1\" data-base62-sha1=\"fFqMnXCGRc8ybjZfiTNZG5epU4k\" width=\"498\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/d/6dcf943f35700d57beac79e1a64f14e9c6816764_2_498x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6dcf943f35700d57beac79e1a64f14e9c6816764.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/d/6dcf943f35700d57beac79e1a64f14e9c6816764.png 2x\" data-dominant-color=\"343434\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Picture1</span><span class=\"informations\">599\u00d7601 171 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Beatriz,</p>\n<p>Welcome to the forum! <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Can you say a little more about your intended analysis? In general yes CellProfiler isn\u2019t going to let you have overlapping objects. Pixels either get to belong to object 1 or object 2 so expanding objects that are close to each other will produce a boundary.</p>\n<p>Rebecca</p>"], "78731": ["<p>Hi</p>\n<p>I am using the cellpose 2.2 in cell segmentation.<br>\nThis system was built in WEB platform and the cellpose was realized by using command.<br>\nFirstly, while researching the cellpose, I found that cellpose provide 14 pretrained models.<br>\nBut I am not sure what the differences(and their utilizing examples) are between them because I am not an expert in this field.<br>\nPlease give me good advice about this problem.<br>\nSecondly, I also found that the cellpose provide \u201c*_output.png\u201d file when we get cell segment result by using commands.<br>\nBut sometimes there are some cases that it\u2019s not created(Because I used the same command, I think that it\u2019s related with the kind of image file).<br>\nWhich case is it created or not in?</p>\n<p>Any help would be appreciated.</p>", "<p>Hey!<br>\nRegarding CellPose models, they were trained on different data and perform better/worse depending on your specific situation. We use the base models to train our own domain specific models to improve performance for our use case.</p>\n<p>Regarding files created/missing, an example of your issues might be appropriate.<br>\nIn general, sharing something you tried and the issue you experienced is typically a better approach than soliciting for \u201cgeneral advice\u201d <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Good luck!<br>\nLeo</p>", "<p>Hey,</p>\n<p>Like Leo said, these models were trained on different data.<br>\nHere is a figure to explain them, in a paper: <a href=\"https://www.nature.com/articles/s41592-022-01663-4\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Cellpose 2.0: how to train your own model | Nature Methods</a><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8.jpeg\" data-download-href=\"/uploads/short-url/u0R9Xgsm23jLDChQ2DdYMloKYY0.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_690x451.jpeg\" alt=\"image\" data-base62-sha1=\"u0R9Xgsm23jLDChQ2DdYMloKYY0\" width=\"690\" height=\"451\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_690x451.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_1035x676.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/2/d25a08160edb0fb43183f77491dceb3476ae63e8_2_1380x902.jpeg 2x\" data-dominant-color=\"CCCDCD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1751\u00d71146 228 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best<br>\nYuan</p>", "<p>Hello everyone,</p>\n<p>First of all, really thank you for all of your kind helpness.<br>\nAnd also thank you for the cellpose developers and experts.<br>\nI am a really fresh developer in this field and so curious about the powerful functions of cellpose 2.2.<br>\nNow I just started the cell segment by using cellpose 2.2 and succeed (I am not sure how you think about this but it\u2019s certainly big success for me.) to get  \u201c<em>_seg.npy\" and \"</em>_cp_output.jpg\u201d files from origine images by using windows command.<br>\nThis is the result.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d.jpeg\" data-download-href=\"/uploads/short-url/vPEVnpF0ctIpkD4pADsSgPXzX8V.jpeg?dl=1\" title=\"029_img.ome_cp_output\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg\" alt=\"029_img.ome_cp_output\" data-base62-sha1=\"vPEVnpF0ctIpkD4pADsSgPXzX8V\" width=\"690\" height=\"172\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1380x344.jpeg 2x\" data-dominant-color=\"A5A7A3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">029_img.ome_cp_output</span><span class=\"informations\">3600\u00d7900 239 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And now I want to get more detailed images from \u201c<em>_seg.npy\" file.<br>\nI mean that I want to get these 4 individual images from the \"</em>_seg.npy\u201d file and also to get the correct position of ROIS.<br>\nI really want to do this but I am not sure how to do this because I just have some knowledge about Python and image processing.<br>\nIf possible, please share with me the python source code to get these individual images and correct position of ROIS from the \u201c*_seg.npy\u201d file.</p>\n<p>Thank all of you again.</p>", "<p>Since QuPath is linked in this, if you run CellPose through QuPath, you will get the positions of all of the cells, along with measurements, as ROI/detection objects. I am not as sure about the outlines and cell poses though, maybe <a class=\"mention\" href=\"/u/oburri\">@oburri</a> knows more.  I kind of doubt it since that would be a huge amount of data by default if run across whole slide images.</p>"], "78735": ["<p>Hi all,</p>\n<p>I used to use CellProfiler years ago adn recently got excited when I saw the implementation of Stardist and Cellpose.  So I thought I\u2019d give it a go\u2026</p>\n<p>I tried a few approaches to install CellProfiler from source.<br>\nThis one on the <a href=\"https://github.com/CellProfiler/CellProfiler/wiki/Source-installation-%28Windows%29\" rel=\"noopener nofollow ugc\">github</a> repo did work.<br>\nI failed trying to do it using anaconda.</p>\n<p>Having navigated through this I discovered you need to also clone the <a href=\"https://github.com/CellProfiler/CellProfiler-plugins\" rel=\"noopener nofollow ugc\">plugins</a>.</p>\n<p>I followed the Use in the README.md.</p>\n<p>when I got to step 2 (b).</p>\n<pre><code class=\"lang-auto\">pip install -r requirements-windows.txt\n</code></pre>\n<p>cellh5 and keras installed<br>\nbut cntk did not</p>\n<pre><code class=\"lang-auto\">ERROR: Could not find a version that satisfies the requirement cntk (from versions: none)\nERROR: No matching distribution found for cntk\n</code></pre>\n<p><strong>Is there a solution for this ?</strong></p>\n<hr>\n<p>I also then found the <a href=\"https://github.com/CellProfiler/CellProfiler-plugins/blob/0c2e7cab5c13d17fa376b2bd42ca6a7e5db04960/Instructions/Install_environment_instructions_windows.md\" rel=\"noopener nofollow ugc\">Beginner Guide</a><br>\nASIDE:This suggests using anaconda which is at odds with the cellprofiler github page that recommends PIP.</p>\n<p>This works well.The resulting install has the runstardist and runcellpose modules.  Havent tested they work though. Only obvious issues</p>\n<pre><code class=\"lang-auto\">Could not load variancetransform\nTraceback (most recent call last):\n  File \"C:\\Users\\Admin\\anaconda3\\envs\\CP_plugins\\lib\\site-packages\\cellprofiler_core\\utilities\\core\\modules\\__init__.py\", line 71, in add_module\n    m = __import__(mod, globals(), locals(), [\"__all__\"], 0)\n  File \"C:\\Users\\Admin\\OneDrive - The University of Sydney (Students)\\Documents\\GitHub\\CP2\\CellProfiler-plugins\\variancetransform.py\", line 20, in &lt;module&gt;\n    import pandas\nModuleNotFoundError: No module named 'pandas'\ncould not load these modules: variancetransform\n</code></pre>\n<p>NB. The anaconda based approach \u2192 CellProfiler 4.2.1<br>\nThe PIP approach \u2192 CellProfiler 5.0.0b1 but no runStarDist</p>\n<p>Tried to  solve with</p>\n<pre><code class=\"lang-auto\">pip install stardist csbdeep --no-deps\npip install omnipose\n</code></pre>\n<p>did not work ;(</p>\n<p>Cheers.</p>\n<p>James</p>", "<p>If anyone can advise how to get GPU working for stardist and cellpose it would be much appreciated.<br>\nI cannot work it out</p>"], "80277": ["<p>Hi<br>\nI am running RunImageJMacro in one of my pipelines - it works as expected using all IJ macro commands i tested except \u201cProcess&gt;Filter&gt;Convolve\u201d. Running the macro in Fiji works fine, but when i run it via Cellprofiler my output image is identical to my input image. I have no idea what maybe going on, and as i mentioned, any other IJ command i tried (e.g rotate image, median filter etc) works.<br>\nAny insight would be approciated.<br>\nK</p>\n<p>This is the offending line in the macro:<br>\nrun(\u201cConvolve\u2026\u201d, \u201ctext1=[0.0 0.0 0.0 0.0 0.0 -0.04166667 -0.04166667 -0.04166667 0.0 0.0 0.0 0.0 0.0\\n0.0 0.0 0.0 -0.04166667 -0.04166667 -0.08333334 -0.08333334 -0.08333334 -0.04166667 -0.04166667 0.0 0.0 0.0\\n0.0 0.0 -0.08333334 -0.08333334 -0.125 -0.125 -0.1666667 -0.125 -0.125 -0.08333334 -0.08333334 0.0 0.0\\n0.0 -0.04166667 -0.08333334 -0.125 -0.125 -0.125 -0.08333334 -0.125 -0.125 -0.125 -0.08333334 -0.04166667 0.0\\n0.0 -0.04166667 -0.125 -0.125 -0.04166667 0.1666667 0.25 0.1666667 -0.04166667 -0.125 -0.125 -0.04166667 0.0\\n-0.04166667 -0.08333334 -0.125 -0.125 0.1666667 0.5833333 0.7916667 0.5833333 0.1666667 -0.125 -0.125 -0.08333334 -0.04166667\\n-0.04166667 -0.08333334 -0.1666667 -0.08333334 0.25 0.7916667 1.0 0.7916667 0.25 -0.08333334 -0.1666667 -0.08333334 -0.04166667\\n-0.04166667 -0.08333334 -0.125 -0.125 0.1666667 0.5833333 0.7916667 0.5833333 0.1666667 -0.125 -0.125 -0.08333334 -0.04166667\\n0.0 -0.04166667 -0.125 -0.125 -0.04166667 0.1666667 0.25 0.1666667 -0.04166667 -0.125 -0.125 -0.04166667 0.0\\n0.0 -0.04166667 -0.08333334 -0.125 -0.125 -0.125 -0.08333334 -0.125 -0.125 -0.125 -0.08333334 -0.04166667 0.0\\n0.0 0.0 -0.08333334 -0.08333334 -0.125 -0.125 -0.1666667 -0.125 -0.125 -0.08333334 -0.08333334 0.0 0.0\\n0.0 0.0 0.0 -0.04166667 -0.04166667 -0.08333334 -0.08333334 -0.08333334 -0.04166667 -0.04166667 0.0 0.0 0.0\\n0.0 0.0 0.0 0.0 0.0 -0.04166667 -0.04166667 -0.04166667 0.0 0.0 0.0 0.0 0.0\\n]\u201d);</p>"], "76693": ["<p>I\u2019m working on a pipeline where we start with primary objects, grow our secondary objects, and then detect whether those secondary objects are touching. If they are we then filter them out. We then want to filter out any primary objects that correspond to the filtered out secondary objects. I would think that the option in the FilterObjects module to \u201cRelabel additional objects to match the filtered objects\u201d would do the trick, but it doesn\u2019t.</p>\n<p>What I came up with was to measure the overlap between the secondary objects and the primary objects, and then to filter on those that didn\u2019t have any overlap. Following that, I related the two object sets as parents/children based on overlap.</p>\n<p>While this seems to work, what I\u2019m asking is, is there a more elegant way to do this? It seems very clunky to filter out the primary objects corresponding to the filtered secondary.  We\u2019re doing all this because in the end we want to compare primary to teritary.</p>\n<p>I\u2019m uploading a copy of the pipeline.  If anyone feels like a head scratcher, feel free to take a look!<br>\n<a class=\"attachment\" href=\"/uploads/short-url/7yyiTCqlkmi6pbuIMj9YqBfkmK5.cpproj\">Filter_secondary_Feb_1_2023.cpproj</a> (563.4 KB)</p>", "<p>Unfortunately, the image files are too big to post (80mb each).</p>", "<p>Hi,</p>\n<aside class=\"quote no-group\" data-username=\"tfalle34\" data-post=\"1\" data-topic=\"76693\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/t/848f3c/40.png\" class=\"avatar\"> tfalle34:</div>\n<blockquote>\n<p>What I came up with was to measure the overlap between the secondary objects and the primary objects</p>\n</blockquote>\n</aside>\n<p>I\u2019m a little confused by this. In CellProfiler, if you make secondary objects, they are grown from primary objects. Therefore they should always have some overlap with their primary object (equal to the size of the primary object they start from). This also doesn\u2019t really relate to the idea of deleting secondary objects that touch other secondary objects. I took a look at your pipeline though and I think your general approach makes sense: measureobjectneighbors of secondary with other secondary then filter.  A few comments:</p>\n<p>Not sure if this is intended, but you throw out secondary objects that touch the image edge but you keep the nuclei of these cells. So you might end up with some funky counts for nuclei. Also, you could end up in a situation where a secondary object that itself does not touch the image border <em>does touch</em> a secondary object that does touch the border. Your current analysis will not find this since that object on the border is thrown out.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/e/9edb7287eaa423d6843fe4478e8a0b4c25ebbbb7.png\" data-download-href=\"/uploads/short-url/mFjBsuUI51VGaX0ATpoIWtld03R.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/e/9edb7287eaa423d6843fe4478e8a0b4c25ebbbb7_2_690x116.png\" alt=\"image\" data-base62-sha1=\"mFjBsuUI51VGaX0ATpoIWtld03R\" width=\"690\" height=\"116\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/e/9edb7287eaa423d6843fe4478e8a0b4c25ebbbb7_2_690x116.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/e/9edb7287eaa423d6843fe4478e8a0b4c25ebbbb7.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/e/9edb7287eaa423d6843fe4478e8a0b4c25ebbbb7.png 2x\" data-dominant-color=\"DEDFDF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">936\u00d7158 43.7 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Then I see that in FilterObjects, you don\u2019t choose to relabel additional objects to match the filtered set. In general, it should work to relabel the Nuclei. You might have had some issues because you throw out cells that touch the edge but not their nuclei beforehand which might cause some problems with relabeling. I would look at this option again after you fix the above issue. Does that make sense?</p>", "<p>Hi Rebecca,<br>\nThanks for looking into this! I was actually sure you were right, that it would be just the border touching issue that caused it, but I changed that up, and I still have the same issue.  I had tried before to relabel the primary objects in the filtering step, but the issue there is that the newly relabelled objects don\u2019t show an object number in the pipeline. Under category, nothing comes up.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/d/ada4f3adca93465d42ff2ae48446ac0d5761de32.jpeg\" data-download-href=\"/uploads/short-url/oM80CYLYDgVZymaenqHDnues3f4.jpeg?dl=1\" title=\"No_Object_Number\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ada4f3adca93465d42ff2ae48446ac0d5761de32_2_690x254.jpeg\" alt=\"No_Object_Number\" data-base62-sha1=\"oM80CYLYDgVZymaenqHDnues3f4\" width=\"690\" height=\"254\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ada4f3adca93465d42ff2ae48446ac0d5761de32_2_690x254.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ada4f3adca93465d42ff2ae48446ac0d5761de32_2_1035x381.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/d/ada4f3adca93465d42ff2ae48446ac0d5761de32_2_1380x508.jpeg 2x\" data-dominant-color=\"EEE7E7\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">No_Object_Number</span><span class=\"informations\">1967\u00d7725 71 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I\u2019m uploading a new version of the same pipeline, streamlined down to just show that issue, and a couple sample image sets.  I think I have a very tedious workaround for this, but it seems like something that shouldn\u2019t be so complicated.  To me, it looks like when I do the filtering step, it\u2019s properly throwing out the primary objects that aren\u2019t associated with secondary objects, but it\u2019s not relabelling them properly or giving them an object number\u2026which is strange.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/h1QDdsdWW5pfA3a6PhnFjSoauaJ.tif\">C1_Position_00.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/gkGaEs6SYr4phlAI1IOKroW7OeS.tif\">C2_Position-00.tif</a> (8.0 MB)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/mrg8EYxGfQ4bC7P58nUkFP3EDWT.tif\">C1_Position_01.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/pNT9Yn5dXv9WduEzcApdgyySCqs.tif\">C2_Position-01.tif</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/4K9BOc4AC44MlEIfzBTaEB1ZgT0.cpproj\">Filter_secondary_Feb_8_2023.cpproj</a> (534.9 KB)</p>", "<p>Thanks for posting some images, this is super helpful in diagnosing the problem. Looking at the workspace, it does look like the filtered cytoplasms and nuclei are being created properly\u2026<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/c/4c765ab173d763d8c02c1aa1219c8d096e3ac1da.jpeg\" data-download-href=\"/uploads/short-url/aUpRmb7FbeDwe9M4cFlXwkiCVRM.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c765ab173d763d8c02c1aa1219c8d096e3ac1da_2_690x351.jpeg\" alt=\"image\" data-base62-sha1=\"aUpRmb7FbeDwe9M4cFlXwkiCVRM\" width=\"690\" height=\"351\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c765ab173d763d8c02c1aa1219c8d096e3ac1da_2_690x351.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c765ab173d763d8c02c1aa1219c8d096e3ac1da_2_1035x526.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/c/4c765ab173d763d8c02c1aa1219c8d096e3ac1da_2_1380x702.jpeg 2x\" data-dominant-color=\"A3A3A9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1832\u00d7934 80.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Your two overlays also look correct to me.</p>\n<p>But yes, there is clearly an issue with the DisplayDataOnImage! Very strange. There\u2019s no measurement available for showing object number for the relabeled nuclei. However, I do think these objects have a number. If you look at them with the workspace, you can display it. Right now I\u2019m thinking this is a bug with DisplayDataOnImage. Have you also seen this issue with other modules?</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/3/13564ace980bc306106feee8a92ce870191f9b80.jpeg\" data-download-href=\"/uploads/short-url/2L3X25uw6SYSRE7CDdKFTznb16g.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13564ace980bc306106feee8a92ce870191f9b80_2_690x346.jpeg\" alt=\"image\" data-base62-sha1=\"2L3X25uw6SYSRE7CDdKFTznb16g\" width=\"690\" height=\"346\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13564ace980bc306106feee8a92ce870191f9b80_2_690x346.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13564ace980bc306106feee8a92ce870191f9b80_2_1035x519.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/3/13564ace980bc306106feee8a92ce870191f9b80_2_1380x692.jpeg 2x\" data-dominant-color=\"A8A8AC\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1834\u00d7920 81 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Rebecca,<br>\nI haven\u2019t seen it in other modules, we were trying to make this work so that we could then make tertiary objects from the primary and secondary, and compare the signal in the tertiary and the primary. When we got to this issue, we were trying to solve it before proceeding, so I haven\u2019t tested a lot of modules with this in the meantime.  Glad that I\u2019m not completely losing it though, and there is a small bug!</p>", "<p>Are you having difficulty making tertiary objects? That should work fine (it works if I add in a IdentifyTertiaryObjects to your pipeline on my machine). My guess is it\u2019s just an issue in DisplayDataOnImage. I made an issue to track this <a href=\"https://github.com/CellProfiler/CellProfiler/issues/4730\">here</a>.</p>", "<p>Hi, sorry, I should have been more clear, we weren\u2019t having problems with Tertiary objects.  We were having an issue just making sure that the object number on the primary objects and the tertiary objects were the same, and then we got sidetracked onto this!</p>\n<p>I think it\u2019s all set now, thanks for your help.  I\u2019m uploading a pipeline here, in case anyone comes across the same problem and wants to filter out touching objects.  It\u2019ll work with the images already uploaded. Thanks for your help!<br>\n<a class=\"attachment\" href=\"/uploads/short-url/sAEo9se5TKPD0br0ZhK7YnZnF9K.cpproj\">Filter_Touching_Secondary_Objects_Feb_2023.cpproj</a> (540.9 KB)</p>"], "80792": ["<p>Any way to get a subset of cell painting data like JUMP without all the AWS shenanigans?<br>\nI am trying to test the analysis pipeline to implement the technique in my organization, but I haven\u2019t been able to find a bunch of images + metadata to download outside of AWS (which requires credit card data, to my understanding, and that\u2019s never going to happen).</p>\n<p>Any help would be much appreciated.</p>"], "57752": ["<p>Hi there,</p>\n<p>I am trying to build a pipeline for automatic segmentation and quantification of differentiated myotube.</p>\n<p><strong>Background:</strong></p>\n<p>C2C12 cells are mouse myoblast cells which undergo differentiation under certain culture condition and fuse together to form multinucleated myotube. These myotube express myosin heavy chain (MYH2) whereas the reserved myoblast cells do not. In this way the myotubes can be detected in the mixed population of myotube and myoblast (In the attached image C00 is DAPI and C01 is MYH2. In muscle atrophy research there are certain agents used to induce atrophy which translates to decrease in myotube diameter (thickness in 2D images). There is another effect of atrophy which cause mitochondrial degradation within the myotube as compared to the normal myotube. The mitochondria have been detected by MitoTracker Red in this case image C02.</p>\n<p>My intensions in this experiment are to</p>\n<ul>\n<li>Detect and count total number of nuclei per image</li>\n<li>Automatically segment and count myotubes per image</li>\n<li>Count number of nuclei present within the myotube (may be per object basis)</li>\n<li>Measure mean/median diameter (thickness) of myotube per image (per object basis)</li>\n<li>Measure mitochondrial presence within the myotube in terms of the mean/median intensity</li>\n</ul>\n<p>To achieve this, I have tried to build a pipeline which gives above results closely what I intend to though I am not completely satisfied. I was wondering if any other better way to do it or any improvement I need to adopt.</p>\n<p>My brief workflow goes like this:</p>\n<p><strong>For Nuclei identification and segmentation</strong></p>\n<ol>\n<li>RescaleIntensity</li>\n<li>HistogramEqualization</li>\n<li>MedianFilter</li>\n<li>IdentifyPrimaryObjects</li>\n</ol>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a94f7c1a20605e4f26bf76f7a0e37d80e488ceaf.png\" alt=\"image\" data-base62-sha1=\"o9MZfhQQ1ebcxgqoushgXBLJejR\" width=\"415\" height=\"413\"></p>\n<p>This gives me pretty descent segmentation of nuclei though the clumped nuclei separation is not very good. Is there any improvement I can adopt?</p>\n<p><strong>For identification and segmentation of myotube</strong></p>\n<p>If I understand correct, IdentifySecondaryObject module create one-to-one relationship which uses nuclei as guiding object. But in this case each myotube has multiple nuclei. So, the identifying myotube as secondary object did not work out and for some reason, I could not use the IdentifyPrimaryObject module alone for proper segmentation of myotube. So, used standalone Threshold and Watershed module instead and IdentifyPrimaryObject module to discard small objects and label the myotube fall under the criteria.</p>\n<ol>\n<li>RescaleIntensity</li>\n<li>HistogramEqualization</li>\n<li>MedianFilter</li>\n<li>Threshold</li>\n<li>Watershed (by distance)</li>\n<li>ConvertObjectsToImage</li>\n<li>IdentifyPrimaryObjects</li>\n<li>OverlayOutlines</li>\n</ol>\n<p>This workflow provides good segmentation of myotubes if they are well separated from each other but when they are overlapped or present very close to each other in some images (can be seen as two separate myotube) they are clubbed together by Watershed module. I believe the watershed can be improved in some way but don\u2019t know how.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/d/7d4e3f5f38ad89afa79cc3be7d3059cf2230b767.jpeg\" data-download-href=\"/uploads/short-url/hSvkmnlhvCwgkny1wC7RofC02Xl.jpeg?dl=1\" title=\"Original_\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d4e3f5f38ad89afa79cc3be7d3059cf2230b767_2_334x250.jpeg\" alt=\"Original_\" data-base62-sha1=\"hSvkmnlhvCwgkny1wC7RofC02Xl\" width=\"334\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d4e3f5f38ad89afa79cc3be7d3059cf2230b767_2_334x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d4e3f5f38ad89afa79cc3be7d3059cf2230b767_2_501x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d4e3f5f38ad89afa79cc3be7d3059cf2230b767_2_668x500.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/d/7d4e3f5f38ad89afa79cc3be7d3059cf2230b767_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Original_</span><span class=\"informations\">1197\u00d7894 116 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/4/e4d71162bf1f6aa0f00976c78c51232bb4a4dd7d.png\" data-download-href=\"/uploads/short-url/wEpDW4Oi1ycRkR4qL6kD0Thhl49.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/4/e4d71162bf1f6aa0f00976c78c51232bb4a4dd7d.png\" alt=\"image\" data-base62-sha1=\"wEpDW4Oi1ycRkR4qL6kD0Thhl49\" width=\"332\" height=\"250\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/4/e4d71162bf1f6aa0f00976c78c51232bb4a4dd7d_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">895\u00d7672 14.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><strong>For counting number of nuclei present within the myotube</strong></p>\n<ol>\n<li>RelateObjects (myotube to nuclei)</li>\n<li>FilterObjects (from total number of nuclei)</li>\n<li>OverlayOutlines</li>\n</ol>\n<p>This provide the nuclei within the myotubes and the nuclei even touching the slightest border of the myotube. Here I intend to filter the nuclei which are within the border of myotube only. How do I filter that?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/b/6b1db8ee59ded0c111f4541fc82ee6f0c2058dd0.jpeg\" data-download-href=\"/uploads/short-url/fhALH1OxVc5d91VLdzak9w8O0j6.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1db8ee59ded0c111f4541fc82ee6f0c2058dd0_2_332x250.jpeg\" alt=\"image\" data-base62-sha1=\"fhALH1OxVc5d91VLdzak9w8O0j6\" width=\"332\" height=\"250\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1db8ee59ded0c111f4541fc82ee6f0c2058dd0_2_332x250.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1db8ee59ded0c111f4541fc82ee6f0c2058dd0_2_498x375.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1db8ee59ded0c111f4541fc82ee6f0c2058dd0_2_664x500.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/b/6b1db8ee59ded0c111f4541fc82ee6f0c2058dd0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">895\u00d7672 251 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nRed lines are the myotube border and green lines are filtered nuclei border.</p>\n<p><strong>For measuring thickness of myotube</strong></p>\n<p>I save the images after IdentifyPrimaryObjects from myotube part as binary images and process them with Fiji for local thickness map and export the mean/median thickness per object and export the result from there. Is there any way to do within the CellProfiler? Or else this works just fine for me.</p>\n<p><strong>For measuring the mitochondrial intensity</strong></p>\n<ol>\n<li>RescaleIntensity (MitoTracker channel)</li>\n<li>MeasureObjectIntensity (of myotube objects)</li>\n</ol>\n<p>This I believe works fine.</p>\n<p>Have attached my pipeline and sample image for your reference. Any suggestion or idea would greatly be helpful.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/fcv3gAEAUQsUPI1PeMGo8Fbp9RO.cppipe\">MyotubeQuant.cppipe</a> (17.9 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/8AIsDY9IZL7LOjTXff0iVmUxPio.zip\">ImageSet-1.zip</a> (8.0 MB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/1wp5bSz8jGQGbHebvEXWzz3eluD.zip\">ImageSet-2.zip</a> (8.2 MB)</p>\n<p>Thank you very much in advance for your time and help!</p>\n<p>Hope everyone doing well in these times.</p>\n<p>Be well !! <img src=\"https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Shubhranshu</p>", "<p>Hi <a class=\"mention\" href=\"/u/shubhranshu\">@shubhranshu</a> !<br>\nI hope you are fine. Any news or update with your quantification?</p>\n<p>Alessio</p>"], "76186": ["<p>I finished Oil red O staining and taking pictures (.tif), and the last step using CellProfiler software and Worm Toolbox (pipeline1) was remained. At this point, I would like to ask your kind assistant.</p>\n<p>First problem is the module \u201cUntangleWorms\u201d</p>\n<p>The error message popped up is \u201cError while processing UntangleWorms: Unknown mat file type, version 104, 116. Do you want to stop processing?\u201d as shown below.</p>\n<p>(Both my windows computer and mac have the same problem, but the version number was different.)</p>\n<p>How can I solve this problem?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c8dc26173971f9167af9eaccdc9f4dac4b643327.jpeg\" data-download-href=\"/uploads/short-url/sET9XJztP4GklWwG6PEDE62s2FN.jpeg?dl=1\" title=\"\uc2a4\ud06c\ub9b0\uc0f7 2023-01-19 \uc624\ud6c4 11.25.39\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8dc26173971f9167af9eaccdc9f4dac4b643327_2_690x352.jpeg\" alt=\"\uc2a4\ud06c\ub9b0\uc0f7 2023-01-19 \uc624\ud6c4 11.25.39\" data-base62-sha1=\"sET9XJztP4GklWwG6PEDE62s2FN\" width=\"690\" height=\"352\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8dc26173971f9167af9eaccdc9f4dac4b643327_2_690x352.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8dc26173971f9167af9eaccdc9f4dac4b643327_2_1035x528.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c8dc26173971f9167af9eaccdc9f4dac4b643327_2_1380x704.jpeg 2x\" data-dominant-color=\"434545\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">\uc2a4\ud06c\ub9b0\uc0f7 2023-01-19 \uc624\ud6c4 11.25.39</span><span class=\"informations\">1920\u00d7980 156 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Second issue is the module \u201cExportToDatabase\u201d.<br>\nWhy the checkbox is lighting with the red cross?..<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/9/a92a8e31d3336f6816b146093c98fc1c40cfe493.jpeg\" data-download-href=\"/uploads/short-url/o8vRLfTn9rRLYSXV6m5FxWZWi3h.jpeg?dl=1\" title=\"\uc2a4\ud06c\ub9b0\uc0f7 2023-01-20 \uc624\ud6c4 1.27.29\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a92a8e31d3336f6816b146093c98fc1c40cfe493_2_690x431.jpeg\" alt=\"\uc2a4\ud06c\ub9b0\uc0f7 2023-01-20 \uc624\ud6c4 1.27.29\" data-base62-sha1=\"o8vRLfTn9rRLYSXV6m5FxWZWi3h\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a92a8e31d3336f6816b146093c98fc1c40cfe493_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a92a8e31d3336f6816b146093c98fc1c40cfe493_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/9/a92a8e31d3336f6816b146093c98fc1c40cfe493_2_1380x862.jpeg 2x\" data-dominant-color=\"363636\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">\uc2a4\ud06c\ub9b0\uc0f7 2023-01-20 \uc624\ud6c4 1.27.29</span><span class=\"informations\">1920\u00d71200 122 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Thank you for your kind consideration.</p>", "<p>Hi,</p>\n<p>Can you attach here the files you\u2019re using, or at least link to where you got them from? Possibly you have old versions, but in any case it would make it much easier to debug.</p>\n<p>With regards to the X in ExportToDatabase, if you hover over the module name, it should give you a more specific error.  Hope that helps!</p>", "<p>Hi!</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://accounts.google.com/v3/signin/identifier?dsh=S1543641750%3A1674632303797464&amp;continue=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1AaHLlNRybdAOoBUHRra5u2lZrjLA3TCb%3Fusp%3Dsharing&amp;followup=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1AaHLlNRybdAOoBUHRra5u2lZrjLA3TCb%3Fusp%3Dsharing&amp;osid=1&amp;passive=1209600&amp;service=wise&amp;flowName=WebLiteSignIn&amp;flowEntry=ServiceLogin&amp;ifkv=AWnogHerx5rR4T5vimQA0Fm4s4VB9sa1WIi52G7WZDe36XXfaGFTLx424lbs52tkNectl9RVgbOfIg\">\n  <header class=\"source\">\n\n      <a href=\"https://accounts.google.com/v3/signin/identifier?dsh=S1543641750%3A1674632303797464&amp;continue=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1AaHLlNRybdAOoBUHRra5u2lZrjLA3TCb%3Fusp%3Dsharing&amp;followup=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1AaHLlNRybdAOoBUHRra5u2lZrjLA3TCb%3Fusp%3Dsharing&amp;osid=1&amp;passive=1209600&amp;service=wise&amp;flowName=WebLiteSignIn&amp;flowEntry=ServiceLogin&amp;ifkv=AWnogHerx5rR4T5vimQA0Fm4s4VB9sa1WIi52G7WZDe36XXfaGFTLx424lbs52tkNectl9RVgbOfIg\" target=\"_blank\" rel=\"noopener nofollow ugc\">accounts.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://accounts.google.com/v3/signin/identifier?dsh=S1543641750%3A1674632303797464&amp;continue=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1AaHLlNRybdAOoBUHRra5u2lZrjLA3TCb%3Fusp%3Dsharing&amp;followup=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1AaHLlNRybdAOoBUHRra5u2lZrjLA3TCb%3Fusp%3Dsharing&amp;osid=1&amp;passive=1209600&amp;service=wise&amp;flowName=WebLiteSignIn&amp;flowEntry=ServiceLogin&amp;ifkv=AWnogHerx5rR4T5vimQA0Fm4s4VB9sa1WIi52G7WZDe36XXfaGFTLx424lbs52tkNectl9RVgbOfIg\" target=\"_blank\" rel=\"noopener nofollow ugc\">Google Drive: Sign-in</a></h3>\n\n  <p>Access Google Drive with a Google account (for personal use) or Google Workspace account (for business use).</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>That is our files using the analysis test. Please request access to my google drive.</p>\n<p>I downloaded program in official homepage (<a href=\"https://cellprofiler.org\" rel=\"noopener nofollow ugc\">https://cellprofiler.org</a>) and I used CellProfiler ver 4.2.5 in Mac (intel Mac), and ver 4.2.5, 2.1.0 in windows.</p>\n<p>Thank you for your kind.</p>", "<p>hi!</p>\n<p>I allowed access!</p>\n<p>have a good day!</p>", "<p></p><div class=\"video-container\">\n    <video width=\"100%\" height=\"100%\" preload=\"metadata\" controls=\"\">\n      <source src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc2955615d6fd5434377eef1b9706a70221631ac.mp4\">\n      <a href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc2955615d6fd5434377eef1b9706a70221631ac.mp4\" rel=\"noopener nofollow ugc\">https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/c/cc2955615d6fd5434377eef1b9706a70221631ac.mp4</a>\n    </source></video>\n  </div><p></p>", "<p>Hi,</p>\n<p>Your worm model file is indeed incorrect - on the left is the file you sent me, and on the right is the one we provide in one of the examples on the CellProfiler website example page. At some point along the way, likely before it made its way to you, it was opened in Microsoft Word, seemingly, and corrupted.  Try this file and see if it works better for you - if not, let us know!</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/c/ac5d44d621e57d1890c4defbfb55646d19ddf46f.jpeg\" data-download-href=\"/uploads/short-url/oANX3MGntxudbgDGeDvFm75HLn9.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/c/ac5d44d621e57d1890c4defbfb55646d19ddf46f_2_663x500.jpeg\" alt=\"image\" data-base62-sha1=\"oANX3MGntxudbgDGeDvFm75HLn9\" width=\"663\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/c/ac5d44d621e57d1890c4defbfb55646d19ddf46f_2_663x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/c/ac5d44d621e57d1890c4defbfb55646d19ddf46f_2_994x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/c/ac5d44d621e57d1890c4defbfb55646d19ddf46f_2_1326x1000.jpeg 2x\" data-dominant-color=\"31343D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d71447 176 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/9jlO02DY5DWNc9ACDq9eMtAtkSp.xml\">MyWormModel_B01_B24.xml</a> (25.1 KB)</p>"], "79772": ["<p>Hello! Does anyone have any insight into what happens when Stardist is trained with labels that may contain a few polygons that are not star convex? I am working with wood cells and some of our cells are collapsed and I am trying to figure out how to handle this. If I mark them as they are will Stardist try to mark similar features as the closest fitting star convex polygon? Or will it ignore those shapes? Should I be annotating my masks by drawing the closest posible star convex shape instead of tracing them as they are?</p>\n<p>Stardist marking the closest star convex shape would be the ideal solution, I would rather not mark my training data with something that is not the actual shape, because we hope to eventually use this masks to train an AI system that can handle slight deviations from a star convex shape that we sometimes see. For our application having high resolution on exactly where the boundaries are is important.</p>"], "79776": ["<p>We\u2019ve recorded several time lapse series in elements, and would now like to use cell profilers tracking feature to track cells in these images. Cell profiler gives me an error message when I attempt to import the .nd2 file from the experiment, but if I convert a single series (the same field imaged every 10 minutes for 48 hour) into a .tiff on FIJI, then import to cell profiler, I no longer get the error message. It took over an hour to convert one series from .nd2 to .tiff in FIJI, so I figured Id try to convert the rest of the file on elements. I was hoping to end up with images from each series clearly grouped together, however, I ended up with thousands of separate files- one for each timepoint, field and channel, with little organization. It would take me days to make sense of what I now have, and reorganize them.</p>\n<p>My questions are:</p>\n<ol>\n<li>\n<p>For my purposes (analyzing time lapse series) can I import the ND2 file for the entire experiment directly into cell profiler? The file is very large, and contains multiple fields imaged on 2 channels every 10 mins over a 48 hour period.</p>\n</li>\n<li>\n<p>If I have to convert the files to .tiff, how can I do this in a way that keeps the series together as appropriate?</p>\n</li>\n</ol>\n<p>Thank you!</p>", "<p><a class=\"mention\" href=\"/u/cellprofilerteam\">@CellProfilerTeam</a></p>", "<p>What i usually do when i have multichannel time lapse images is - Convert the timelapse into an image sequence in fiji. This keeps the channel information in the output metadata. You should then import all the .tiff to cellprofiler and analyse after grouping them together with image meatadata.</p>\n<p>From my experience i can tell you that it takes the most amount of time to organize the files (a few hours) then analysis takes maybe 1-2 hours including optimization. The run will potentially take 3-4 hours on a regular PC.</p>"], "31650": ["<p>Hello,<br>\nI am trying to figure out how to use the calculated flat field correction (illumination correction) that a operetta microscope produces in my own cellprofiler pipeline. I was hoping there was a way to possibly build a illumination correction \u2018image\u2019 in imagej/Fiji that I could then use in cellprofiler for correction my images.<br>\nI believe the file is giving me all the information I need to produce an image that looks like:<br>\n<a href=\"http://nic.ucsf.edu/blog/wp-content/uploads/2014/04/FITC_10x.png\" rel=\"nofollow noopener\">http://nic.ucsf.edu/blog/wp-content/uploads/2014/04/FITC_10x.png</a> (not my image)<br>\nAn example of an operetta FFC_Profile.xml (flat field correction) file looks like this:</p>\n<pre><code class=\"lang-auto\">&lt;Results xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.perkinelmer.com/PEHH/HarmonyV5\"&gt;\n&lt;Version&gt;2&lt;/Version&gt;\n&lt;ProfilesComplete&gt;1&lt;/ProfilesComplete&gt;\n&lt;AggregatedQuality Scope=\"Background\"&gt;1&lt;/AggregatedQuality&gt;\n&lt;AggregatedQuality Scope=\"Foreground\"&gt;1&lt;/AggregatedQuality&gt;\n&lt;Quality ChannelID=\"1\" Scope=\"Background\"&gt;1&lt;/Quality&gt;\n&lt;Quality ChannelID=\"1\" Scope=\"Foreground\"&gt;1&lt;/Quality&gt;\n&lt;Count ChannelID=\"1\" Scope=\"Background\"&gt;128&lt;/Count&gt;\n&lt;Count ChannelID=\"1\" Scope=\"Foreground\"&gt;285&lt;/Count&gt;\n&lt;Map&gt;\n&lt;Entry ChannelID=\"1\"&gt;\n&lt;FlatfieldProfile&gt;\n{Background: {Character: NonFlat, Mean: 3747.8228, NoiseConst: 4.0573891, NonFlatness: {Corrected: 0.049106896, Original: 0.44735402, Random: 0.027165147}, Profile: {Coefficients: [[1.1523], [0.0635, 0.0631], [-0.9058, 0.0382, -0.7466], [0.002, -0.1096, -0.0296, -0.253], [-0.3628, -0.1935, 0.2453, -0.0644, -0.9442]], Dims: [2160, 2160], Origin: [1079.5, 1079.5], Scale: [0.00046296296, 0.00046296296], Type: Polynomial}, Quality: 1.0}, Channel: 1, ChannelName: \"*Alexa 647\", Foreground: {Character: NonFlat, NonFlatness: {Original: 0.48933411, Random: 0.061149477}, Profile: {Coefficients: [[1.1134], [0.0265, -0.017], [-0.4092, 0.0168, -0.5735], [0.1694, -0.2256, 0.4858, 0.0598], [-0.9208, 0.2278, -1.8684, -0.3317, -0.5632]], Dims: [2160, 2160], Origin: [1079.5, 1079.5], Scale: [0.00046296296, 0.00046296296], Type: Polynomial}, Quality: 1.0}, Version: Acapella:2013}\n&lt;/FlatfieldProfile&gt;\n&lt;/Entry&gt;\n</code></pre>\n<p>Thank you.</p>", "<p>I hope I understood your question correctly. For ImageJ, this might be useful:<br>\n<a href=\"https://imagejdocu.list.lu/howto/working/how_to_correct_background_illumination_in_brightfield_microscopy\" class=\"onebox\" target=\"_blank\" rel=\"nofollow noopener\">https://imagejdocu.list.lu/howto/working/how_to_correct_background_illumination_in_brightfield_microscopy</a></p>", "<p>Once you produce the right image (like the PNG you point to), it\u2019s trivial to apply it in a CellProfiler pipeline using CorrectIlluminationApply.</p>\n<p>However, the way Perkin Elmer is storing the information in that file is pretty funky. If no one is aware of a converter from PE format to an actual image, I\u2019d suggest contacting PE support to ask them. I wonder if the software itself allows exporting as an image?</p>", "<p>Hi <a class=\"mention\" href=\"/u/moiraine\">@Moiraine</a> ,</p>\n<p>a bit time passed and I have actually the same question. Did you somehow manage to load the Operetta flatfield-correction into cellprofiler?</p>\n<p>Regards, Anna</p>", "<p>Hi <a class=\"mention\" href=\"/u/ahamacher\">@ahamacher</a>,</p>\n<p>I think you could use Harmony software (PE software) to do the same.</p>\n<p>Regards,<br>\nLakshmi<br>\n<a href=\"http://www.wakoautomation.com/\" rel=\"noopener nofollow ugc\">www.wakoautomation.com</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/ahamacher\">@ahamacher</a> ,<br>\nSorry for the late reply. No, I was not able to decipher the FF data from the operetta files. However, like Lakshimi mentioned Perkin Elmer offered to write a script for us to do this but it was going to cost us 1-2k if I recall correctly.<br>\nBest, - M</p>", "<p><a class=\"mention\" href=\"/u/moiraine\">@Moiraine</a> and <a class=\"mention\" href=\"/u/ahamacher\">@ahamacher</a><br>\nI have had this issue for a long time as well. I think I may have figured out a way to do this in python, here is the notebook if you can find it useful. I am not sure what would need to be adapted to use the FFC in cellprofiler, but you could just apply FFC in python then run CP as usual.</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://github.com/arronsullivan/Operetta_FFC\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/bad3e5f9ad67c1ddf145107ce7032ac1d7b22563.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https://github.com/arronsullivan/Operetta_FFC\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690/345;\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/1501bf8554752ca1302f464df54d267f0034e951_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/1501bf8554752ca1302f464df54d267f0034e951_2_690x345.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/5/1501bf8554752ca1302f464df54d267f0034e951_2_1035x517.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/5/1501bf8554752ca1302f464df54d267f0034e951.png 2x\" data-dominant-color=\"F2EEEB\"></div>\n\n<h3><a href=\"https://github.com/arronsullivan/Operetta_FFC\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - arronsullivan/Operetta_FFC</a></h3>\n\n  <p>Contribute to arronsullivan/Operetta_FFC development by creating an account on GitHub.</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nGood Luck,<br>\nArron</p>", "<p><a class=\"mention\" href=\"/u/arronsullivan\">@ArronSullivan</a> pretty cool!</p>\n<p>We also had a script, but it\u2019s only for the case of \u2018complex\u2019 polynomial, and we do not know how it behaves with axis flip, binning, and how the correction is done (dividing?). Sometimes, the flat field is just not there and Harmony has to perform the computation again.</p>\n<p>Here it is in case it\u2019s useful:</p>\n<pre><code class=\"lang-auto\">/*\n* NonFlatness: {Corrected: 0.081553765, Original: 0.71487409, Random: 0.029910839},\n       Profile: {Coefficients: [[1.1291], [0.0849, -0.0372], [-0.1231, -0.2535, -0.2475], [-0.4926, 0.8183, 0.0962, 1.4352], [-3.5431, 0.1713, -0.8743, 1.0486, -3.8259]],\n       Dims: [2160, 2160],\n       Origin: [1079.5, 1079.5],\n       Scale: [0.00046296296, 0.00046296296], Type: Polynomial},\n       Quality: 1.0},\n       Channel: 1,\n*/\ndef dims = [2160, 2160]\ndef origin = [1079.5, 1079.5]\ndef scale = [0.00046296296, 0.00046296296] // = 1/2160\n\ndef w = dims[0]\ndef h = dims[1]\ndef ox = origin[0]\ndef oy = origin[1]\ndef sx = scale[0]\ndef sy = scale[1]\n\ndef c0 = 1.1291\n\ndef c1x = 0.0849\ndef c1y = -0.0372\n\ndef c2xx = -0.1231\ndef c2xy = -0.2535\ndef c2yy = -0.2475\n\ndef c3xxx = -0.4926\ndef c3xxy = 0.8183\ndef c3xyy = 0.0962\ndef c3yyy = 1.4352\n\ndef c4xxxx = -3.5431\ndef c4xxxy = 0.1713\ndef c4xxyy = -0.8743\ndef c4xyyy = 1.0486\ndef c4yyyy = -3.8259\n\n\ndef processor = new FloatProcessor(w,h)\nfloat[] pixels = processor.getPixels()\n\nfor (int x = 0; x&lt;w; x++) {\n   for (int y = 0; y&lt;h; y++) {\n       def dx = (x-ox)*sx\n       def dy = (y-oy)*sy\n       def dx2 = dx*dx\n      \n       def dy2 = dy*dy\n       def dxdy = dx*dy\n      \n       def dx3 = dx2*dx\n       def dx2dy = dx2*dy\n       def dxdy2 = dx*dy2\n       def dy3 = dy2*dy\n      \n       def dx4 = dx3*dx\n       def dx3dy = dx3*dy\n       def dx2dy2 = dx2*dy2\n       def dxdy3 = dx*dy3\n       def dy4 = dy3*dy\n      \n       pixels[y*w+x] = c0 +\n            dx  * c1x   + dy    * c1y +\n            dx2 * c2xx  + dxdy  * c2xy  + dy2   * c2yy +\n            dx3 * c3xxx + dx2dy * c3xxy + dxdy2 * c3xyy + dy3  * c3yyy  +\n            dx4 * c4xxxx+ dx3dy * c4xxxy+ dx2dy2* c4xxyy+ dxdy3* c4xyyy + dy4 * c4yyyy;\n   }\n}\n\ndef imp = new ImagePlus(\"FlatField\", processor)\nimp.show()\n\n\n\nimport ij.ImagePlus\nimport ij.process.FloatProcessor\n</code></pre>\n<p>We decided not to implement it in the BIOP Operetta importer - code maintenance burden. If <a class=\"mention\" href=\"/u/arronsullivan\">@ArronSullivan</a> have something reliable, that\u2019s great. Currently we use another option to export \u201cflatfielded\u201d images as described here:</p>\n<aside class=\"onebox googledocs\" data-onebox-src=\"https://docs.google.com/presentation/d/1jfdDF_viFlqg3St1r2829cHj5HYXFfUfRe7wRWt3410/edit#slide=id.g19b6fbe44fa_0_0\">\n  <header class=\"source\">\n\n      <a href=\"https://docs.google.com/presentation/d/1jfdDF_viFlqg3St1r2829cHj5HYXFfUfRe7wRWt3410/edit#slide=id.g19b6fbe44fa_0_0\" target=\"_blank\" rel=\"noopener\">docs.google.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <a href=\"https://docs.google.com/presentation/d/1jfdDF_viFlqg3St1r2829cHj5HYXFfUfRe7wRWt3410/edit#slide=id.g19b6fbe44fa_0_0\" target=\"_blank\" rel=\"noopener\"><span class=\"googledocs-onebox-logo g-slides-logo\"></span></a>\n\n<h3><a href=\"https://docs.google.com/presentation/d/1jfdDF_viFlqg3St1r2829cHj5HYXFfUfRe7wRWt3410/edit#slide=id.g19b6fbe44fa_0_0\" target=\"_blank\" rel=\"noopener\">Tutorial - Operetta CLS (Workflows)</a></h3>\n\n<p>PerkinElmer Operetta CLS Workflow and Data Management Olivier Burri Romain Guiet Nicolas Chiaruttini November 2022 Operetta CLS Workflow \u25a0</p>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>ping <a class=\"mention\" href=\"/u/oburri\">@oburri</a>, <a class=\"mention\" href=\"/u/romainguiet\">@romainGuiet</a></p>", "<p>Hi Nicolas, this could not be better timed! Have spent the best part of a week figuring this out on my own. I have run into a problem with my images with the ImageJ importer that did not seem to present as an issue last week. This is the error from the console.</p>\n<pre><code class=\"lang-auto\">Well at R1-C1 is Well:0:0\nWell: ome.xml.model.Well@7081144f\nCould not find top left coordinates for fields\nCould not find top left coordinates for fields\n[ERROR] Could not find coordinates for well ome.xml.model.Well@7081144f\n</code></pre>\n<p>Apologies if wrong place for this message. Thanks also for making this code available to the wider user base!</p>", "<p>Hello <a class=\"mention\" href=\"/u/maxewg\">@MaxEWG</a> and welcome to the forum!</p>\n<p>Do you have missing data in the end? Is the export resuming after the error? Are you using the BIOP Operetta Importer ? If yes, with which options ?</p>\n<p>It could be an issue with the data or with the bio-formats reader. I know the dataset is usually big, but you need to share it if you want others to potentially fix it.</p>\n<p>Best,</p>\n<p>Nicolas</p>", "<p>Following your post yesterday I ran my exported files through the flatfield export (via the .bat file).<br>\nI have then been using the BIOP Operetta Importer with files locally on my pc to fuse the image fields. The current data set is only a single well, single image plane, 4x4 square of fields of view (plus the center well image) and 8 channels. (Starting on the small scale to work out all this stuff now!)<br>\nThe importer will load the data files fine, can process the fields individually into channel stacks but refuses to fuse the fields, giving the error mentioned above.</p>\n<p>This happens for both the raw and Flatfield corrected images, measurement sets from the operetta have acted the same.  It could potentially be a bioformats issue, after the error appears when attempting to process it generates a .Index.idx.xml.bfmemo file in the selected image folder.</p>\n<p>The BDV playground had no issue loading in the data and displaying it as a whole image so all the information is clearly there, the importer is just not wanting to read the positions of the well and fields.</p>\n<p><a href=\"https://unsw-my.sharepoint.com/:f:/g/personal/z5448476_ad_unsw_edu_au/EjSvn-IR0ddAswo8Ppq6UO4BpX87E95ykaWwywkclCWLZw?e=kttXXh\" rel=\"noopener nofollow ugc\">Here is the folder containing the data.</a> Again the I have succeeded previously in getting a fused image from the files in the images folder. However now neither those nor the flatfield images could be fused.</p>\n<p>Thanks,</p>\n<p>Max</p>"], "80375": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<p><a class=\"attachment\" href=\"/uploads/short-url/6PoqRPgEJ0dbr3q9mrII4U5Ab4G.tif\">TB + CONTROL C10 - 2022-07-27 10.05.26 (1, x=75103, y=40903, w=1532, h=1129).tif</a> (4.9 MB)</p>\n<h3>\n<a name=\"background-2\" class=\"anchor\" href=\"#background-2\"></a>Background</h3>\n<p>Greeting<br>\nAttached above is section of WSI stained for RNAScope using Mtb Probe. The slide was scanned using Hammamatsu at 11 layers Z-stack.</p>\n<h3>\n<a name=\"analysis-goals-3\" class=\"anchor\" href=\"#analysis-goals-3\"></a>Analysis goals</h3>\n<p>I followed the technical note from ACD on using QuPath to analyze RNAscope\u2026</p>\n<h3>\n<a name=\"challenges-4\" class=\"anchor\" href=\"#challenges-4\"></a>Challenges</h3>\n<p>I was able to observe that QuPath can only detect RNA signal within a cell as you can see from the attached image. There are many more signals outside of cells that can\u2019t be detected.</p>\n<p>Is there a script or method to detect every single signal and quantify them?</p>\n<p>Also attached is a fresh section of the WSI for your convenience. In case you wish to test the function on your side.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/n8JuqUKJc8IgufFFlwBXwxuV2gG.tif\">TB + CONTROL C10 - 2022-07-27 10.05.26.tif</a> (10.2 MB)</p>\n<p>Thanking you in anticipation</p>", "<p>Two things to think about:</p>\n<ol>\n<li>What\u2019s the final goal of the analysis? What number/graph do you want at the end?</li>\n<li>Why do you think there are dots so far from any nuclei?</li>\n</ol>", "<p>It looks like there are some nuclei not being detected by QuPath in your image - you may want to lower the detection threshold\u2026</p>\n<p>To include more dots you can simply increase the expansion. This would include more spots within the cell ROI but you should first be sure that the amount you are expanding by is accurate. The cytoplasm is simply a mathematical expansion and not directly related to any biology on the slide. Some spots will belong to cells whose nuclei are above or below the plane of the section and will not belong to any cell in your image.</p>", "<p>Hi<br>\nThe end goal is to quantify the RNAscope signal. I am aware that not all the dots will be detected but too much is left undetected by QuPath.</p>\n<p>I am not a Pathologist but can speculate that infected cells died and exploded releasing the bacteria\u2019s antigen or RNA. I hope this provides more clarity.</p>\n<p>Is it possible to detected and quantify the RNAscope signals without having to go through cell detection first. I think this approach might yield better result as the bacteria\u2019s RNA does not always remain within the infected cell. After the cells are died and \u201cdisappear\u201d, the signal remains.</p>\n<p>Thanks</p>", "<p>HI<br>\nThis slide was scanned at 11 layers z-stack. I think the undetected cells are on deeper slide as i am not yet familiar with z-stack analysis in QuPath. I try your suggestion and see what i get.</p>\n<p>Thanks a lot.</p>", "<aside class=\"quote no-group\" data-username=\"Delice_Lumamba\" data-post=\"4\" data-topic=\"80375\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/delice_lumamba/40/30105_2.png\" class=\"avatar\"> Delice_Lumamba:</div>\n<blockquote>\n<p>The end goal is to quantify the RNAscope signal.</p>\n</blockquote>\n</aside>\n<p>Sorry, my question wasn\u2019t clear. Do you want Num spots / area? If so, which areas? Do you care about the size of the spots? Alternatively, do you want fraction of cells positive for Mtb signal?</p>", "<aside class=\"quote no-group\" data-username=\"Delice_Lumamba\" data-post=\"4\" data-topic=\"80375\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/delice_lumamba/40/30105_2.png\" class=\"avatar\"> Delice_Lumamba:</div>\n<blockquote>\n<p>Is it possible to detected and quantify the RNAscope signals without having to go through cell detection first.</p>\n</blockquote>\n</aside>\n<p>Not exactly, but you can either use a pixel classifier to detect regions of staining, which will then have an area (which could be divided by something to make an estimated spot count, like the subcellular detection method does).</p>\n<p>Or create tiles, turn those tiles into \u201ccell objects\u201d then run subcellular detection on the cell-tiles. If the image is small enough, you could probably do that with a whole image annotation-cell, but I ran into issues making them too large.</p>", "<p>HI Sara,<br>\nThanks for expanding the question.</p>\n<ol>\n<li>Yes, I want the Num spots within the ROI. The image I shared is a very small portion of the WSI.</li>\n<li>Yes, I do want to know the size of each spot or cluster of spots.</li>\n<li>about the fraction of cells positive for Mtb signal, not necessarily.</li>\n</ol>\n<p>Thanks again</p>", "<p>Thanks a lot <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a></p>\n<p>I am trying the second option you suggested but could only find the this script <a href=\"https://gist.github.com/Svidro/5829ba53f927e79bb6e370a6a6747cfd\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">!Manipulating objects in QuPath \u00b7 GitHub</a> online. I was wondering if this function is integrated on QuPath and where can I find it.</p>\n<p>Thanks again</p>", "<p>I doubt it, but maybe. Mostly just scripting: <a href=\"https://forum.image.sc/t/counting-puncta-in-qupath/72967/6\" class=\"inline-onebox\">Counting puncta in QuPath - #6 by Research_Associate</a> to create the cell objects of some sort, then select them and run subcellular detection.</p>", "<p>I recommend you try a <a href=\"https://qupath.readthedocs.io/en/0.4/docs/tutorials/pixel_classification.html\">pixel classifier</a>. Train the classifier, create the detection objects, and then measure the number of each object and it\u2019s area. Divide by total tissue area.</p>", "<p>I explored various options for chrmogenic RNAscope staining in brightfiled images and in the end settled on</p>\n<ol>\n<li>Make a custom cellpose cell segmentation model in QuPath to segment nuclei (the RNAscope spot pattern makes it tricky to distinguish spot filled nuclei with most publicly availble models) <a href=\"https://github.com/BIOP/qupath-extension-cellpose\" rel=\"noopener nofollow ugc\">GitHub - BIOP/qupath-extension-cellpose: an extension that wraps a Cellpose environment such that WSI can be analyzed using Cellpose through QuPath.</a>\n</li>\n<li>As Sara suggests, train a pixel classifier to detect and segment RNAscope signal</li>\n<li>Measure RNAscope area per cell (you can then divide this by the average spot size to interpret the number of spots per cell, as for RNAscope analysis, what you care about is signal area, not signal intensity). <a href=\"https://forum.image.sc/t/qupath-measure-pixel-classifier-area-per-cell-detection-for-wsis/72701/9\">Qupath measure pixel classifier area per cell detection for WSIs - Image Analysis - Image.sc Forum</a>\n</li>\n</ol>", "<p>Hi <a class=\"mention\" href=\"/u/merrick\">@merrick</a><br>\nThank you for taking the time to assist. I will try your options 1 and 3 next week and see what I get. My main concern though remains that the bacterial RNAscope signal are not always within the cell or nuclei. It would help a lot more if I could detect and quantify the signal without having to first detect cell.</p>\n<p>I have been trying the approach suggested by <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>, but after converting the tiles into cell objects, the subcellular function does not detect anything. I am not sure whether it has to do with the format or quality of the WSI. However, I plan on trying it again on a single layer scanned image.</p>\n<p>Regarding the pixel classification suggested by <a class=\"mention\" href=\"/u/smcardle\">@smcardle</a>, I followed this tutorial (<a href=\"https://qupath.readthedocs.io/en/0.4/docs/tutorials/pixel_classification.html\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Pixel classification \u2014 QuPath 0.4.3 documentation</a>). However, I am still struggling to get a good outcome.</p>\n<p>Again, thank you</p>", "<p>I agree with the pixel classifier approach. I am using the same method to quantify the number of RNAscope spots per mm^2 tissue.</p>\n<p>You definitely want to use as high a resolution as possible, ideally at native scale, since the spots are tiny in size.</p>\n<p>However, from your image, you might not be able to distinguish densely clumped spots.</p>"], "73634": ["<p>Hello,<br>\nI have the output of ClassifyObjects as an image.<br>\nHow can I replace specific \u201cclass\u201d image values like 1, 2 with a value 0 ?<br>\nI tried ConvertImageToObjects, but that only works with 2 classes.<br>\nI have 4 classes.<br>\nI also tried various Image Processing modules with no success.<br>\nThanks in advance for any ideas</p>", "<aside class=\"quote no-group\" data-username=\"keesh\" data-post=\"1\" data-topic=\"73634\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/keesh/40/35630_2.png\" class=\"avatar\"> Eric Kischell:</div>\n<blockquote>\n<p>How can I replace specific \u201cclass\u201d image values like 1, 2 with a value 0 ?</p>\n</blockquote>\n</aside>\n<p>Could you mention a bit more about why you\u2019d like to do this? When you ClassifyObjects, the output is objects so I\u2019m confused why you\u2019re converting to an image and then back into objects.</p>\n<p>EDIT: for clarification, the output of ClassifyObjects <em>can be</em> objects, but only if using model mode (see help for more details). Otherwise, ClassifyObjects acts more like a measurement model, giving a label to object sets.</p>", "<p>Rebecca,<br>\nThanks.<br>\nThe ColorToGray module seemed to help me on my way:<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/d/edfaf393ee1ef63b6e2de93b52e3ba6db504a4d7.png\" alt=\"image\" data-base62-sha1=\"xXgOW3kNh7OUL6M03jfCbMXgvCT\" width=\"293\" height=\"59\"><br>\nthx e.-</p>", "<p>thx.  I will try simplifying my pipeline.<br>\nThe following confused me:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/d/1db732b185be5d14e08e06ec8e7af29df93b0324.png\" data-download-href=\"/uploads/short-url/4eSluiUOwAXQeHFIWXl4yoGNlC4.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/d/1db732b185be5d14e08e06ec8e7af29df93b0324.png\" alt=\"image\" data-base62-sha1=\"4eSluiUOwAXQeHFIWXl4yoGNlC4\" width=\"690\" height=\"106\" data-dominant-color=\"F0F0F0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1035\u00d7160 2.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>FilterObjects does not recognize an input based on ClassifyObjects (objects) output?<br>\nThat is why I needed an image machination.</p>\n<p>Classify Objects has other issues.<br>\nWhen I use the output of IdentifyPrimaryObjects as input to DisplayDataOnImage all is well.<br>\nWhen I use a derived output of ClassifyObjects (ColorToGray | ConvertImageToObjects | FilterObjects - removed objects )<br>\nas input to DisplayDataOnImage the attached error occurs.<br>\n<img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24460d0c8451b1909a2cc51b0945651291a01b3b.png\" alt=\"image\" data-base62-sha1=\"5aTgTiWybnIt25nNhRiZHSTMHFN\" width=\"510\" height=\"255\"></p>", "<aside class=\"quote no-group\" data-username=\"keesh\" data-post=\"4\" data-topic=\"73634\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/keesh/40/35630_2.png\" class=\"avatar\"> Eric Kischell:</div>\n<blockquote>\n<p>The following confused me:</p>\n</blockquote>\n</aside>\n<p>Ah, \u2018retain an image\u2019 is for display purposes only, not for further measurement.</p>\n<aside class=\"quote no-group\" data-username=\"keesh\" data-post=\"5\" data-topic=\"73634\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/keesh/40/35630_2.png\" class=\"avatar\"> Eric Kischell:</div>\n<blockquote>\n<p>FilterObjects does not recognize an input based on ClassifyObjects (objects) output?</p>\n</blockquote>\n</aside>\n<p>Can you say a little more about what your ultimate goal is with this analysis? It might be that the order of operations here isn\u2019t ideal. Also, if you want to attach your pipeline and an image set, I\u2019m happy to look into the details.</p>", "<p>This project is for a commercial company.<br>\nSo I can generally say it is about automatically segmenting and measuring desired cells.<br>\nI can send you the .cppipe file to your private email address.<br>\nYou can try Brightfield images with some sort of particles for testing.<br>\nThanks for your help.<br>\nkeesh</p>", "<p>Hi Keesh,</p>\n<p>Ok, so generally speaking, ClassifyObjects should be used if you have a model or set of measurements you want to use to derive a label for each object. If you want to remove objects based on measurements, use FilterObjects. Think of ClassifyObjects as more of a measurement module (unless using in model mode). Does that make things more clear?</p>\n<p>Rebecca</p>", "<p>Yes, understood.<br>\nMy problem is that IdentifyPrimaryObjects and others are producing<br>\ntoo many false positives based on my datasets.<br>\nI am waiting for \u201cidentifyyeastcells\u201d to be ported to CellProfiler 4.<br>\nThis may yet save my day.</p>", "<aside class=\"quote no-group\" data-username=\"keesh\" data-post=\"9\" data-topic=\"73634\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/keesh/40/35630_2.png\" class=\"avatar\"> Eric Kischell:</div>\n<blockquote>\n<p>too many false positives based on my datasets</p>\n</blockquote>\n</aside>\n<p>Sure! So the solution I would recommend might be tweak the settings of IdentifyPrimaryObjects (for instance, try adjusting the Threshold Correction Factor or the minimum intensity for thresholding if the issue is the objects. you don\u2019t want to include are too bright or too dim. You can also adjust minimum and maximum size). Alternatively, you can add a MeasureObjectIntensity or MeasureObjectSizeShape after IdentifyPrimaryObjects, then insert a FilterObjects to filter the objects based on measurements. If you want to mention a little more about what type of false positives you\u2019re experiencing I can make more specific recommendations.</p>", "<p>Thanks, I have tried all of your suggestions.<br>\nI asked my company if I could share our small test dataset with you.<br>\n\u2026<br>\nAlso, in trying to run my pipeline via the following example:<br>\n<a href=\"https://github.com/CellProfiler/notebooks\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - CellProfiler/notebooks: Jupyter notebooks for CellProfiler and friends</a> (cell_profiler_notebooks\\environment.yml)<br>\nI am running into the dreaded Java-bridge error:<br>\nextractor = self.build_extractor()<br>\n<code><br>\nextractor = javabridge.make_instance(<br>\n1016     \u201corg/cellprofiler/imageset/ImagePlaneMetadataExtractor\u201d, \u201c()V\u201d<br>\n1017 )<br>\n1018 javabridge.call(<br>\n1019     extractor,<br>\n1020     \u201caddImagePlaneExtractor\u201d,<br>\n(\u2026)<br>\n1024     ),<br>\n1025 )<br>\n1026 if any(<br>\n1027     [<br>\n1028         group.extraction_method == X_AUTOMATIC_EXTRACTION<br>\n1029         for group in self.extraction_methods<br>\n1030     ]<br>\n1031 ):</code></p><code>\n<p>File ~\\AppData\\Roaming\\Python\\Python38\\site-packages\\javabridge\\jutil.py:1717, in make_instance(class_name, sig, *args)<br>\n1706 \u2018\u2019\u2018Create an instance of a class<br>\n1707<br>\n1708 :param class_name: name of class in foo/bar/Baz form (not foo.bar.Baz)<br>\n(\u2026)<br>\n1714<br>\n1715 \u2018\u2019\u2019<br>\n1716 args_sig = split_sig(sig[1:sig.find(\u2018)\u2019)])<br>\n \u2192 1717 klass = get_env().find_class(class_name)<br>\n1718 jexception = get_env().exception_occurred()<br>\n1719 if jexception is not None:</p>\n</code><p><code>AttributeError: \u2018NoneType\u2019 object has no attribute \u2018find_class\u2019<br>\n</code></p>", "<p>Rebecca,<br>\nMy company has allowed me to share data.<br>\nDo you have a private FTP or other locale for uploads?<br>\nthx e.-</p>"], "22438": ["<p>Hi,</p>\n<p>I\u2019ve been trying to establish a connection between our Omero server at out institution with Cellprofiler(3.1.5) for high-throughput image analysis.</p>\n<p>I\u2019ve followed the guide at (<a href=\"https://github.com/CellProfiler/CellProfiler/wiki/OMERO:-Accessing-images-from-CellProfiler\" rel=\"nofollow noopener\">https://github.com/CellProfiler/CellProfiler/wiki/OMERO:-Accessing-images-from-CellProfiler</a>) without success, and from my understanding that guide is quite outdated and is not the way to do it. Whatever I try to read into Cellprofiler, it wont understand it as login-credentials to Omero (tried Csv, ice.config, .py and URL)<br>\nI\u2019ve also looked into this post (<a href=\"https://www.openmicroscopy.org/community/viewtopic.php?f=4&amp;t=7821\" rel=\"nofollow noopener\">https://www.openmicroscopy.org/community/viewtopic.php?f=4&amp;t=7821</a>) where they clearly get a connection to omero through a simple csv file. When I attempt this, it only loads the file as omero.csv without actually reading it.</p>\n<p>Im on a mac running the latest OS, with the latest update of java (8). The Omero server is up and running as I can access it through alternative methods (Matlab &amp; Fiji) so there is no problem on that side.</p>\n<p>Can anyone help me/direct me to a solution? <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>\n<p>Thanks in advance,<br>\nOscar</p>", "<p>Hi,</p>\n<p>Unfortunately my understanding is the OMERO bridge is currently broken in the CP3+ builds; we are working with some of the folks there now to try to rectify this!</p>\n<p>In the meantime, I\u2019m afraid CP 2.2 is your only option to do this directly.  Sorry about this!</p>", "<p>Hi <a class=\"mention\" href=\"/u/oscarandre1\">@OscarAndre1</a></p>\n<p>to add some more details on what <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> mentioned, with CP 2.2 you are able to establish a connection to an OMERO server that is running any patch-version of the <strong>5.3 series</strong> only. Otherwise you have to update the OMERO-related JAR files shipped with CP as explained at the bottom of the wiki page linked by you, but I don\u2019t have any predictions how successful that might be with later OMERO-versions.</p>\n<p>Best,<br>\nNiko</p>", "<p>I updated the Github page to make it known it\u2019s a bit outdated and point here. Sorry for the inconvenience of outdated documentation!</p>", "<p>Hi Oscar</p>\n<p>We have built a notebook example on how to use CellProfiler 3.1.3 and IDR (<a href=\"https://idr.openmicroscopy.org/\" rel=\"nofollow noopener\">https://idr.openmicroscopy.org/</a>). IDR is using the OMERO 5.4 series. The notebook uses the Python API of CellProfiler.<br>\nI have not tested CellProfiler 3.1.5.</p>\n<p>Below are links showing how we install CellProfiler 3.1.3  in Docker<br>\n(we have other things that you can ignore like r-gateway)</p>\n<ul>\n<li><a href=\"https://github.com/IDR/idr-notebooks/blob/itr/Dockerfile\" rel=\"nofollow noopener\">https://github.com/IDR/idr-notebooks/blob/itr/Dockerfile</a></li>\n<li><a href=\"https://github.com/IDR/idr-notebooks/blob/itr/docker/environment-python2-cellprofiler.yml\" rel=\"nofollow noopener\">https://github.com/IDR/idr-notebooks/blob/itr/docker/environment-python2-cellprofiler.yml</a></li>\n</ul>\n<p>The notebook itself is <a href=\"https://github.com/IDR/idr-notebooks/blob/itr/idr0002.ipynb\" rel=\"nofollow noopener\">https://github.com/IDR/idr-notebooks/blob/itr/idr0002.ipynb</a></p>\n<p>If you wish to try it out \u201clive\u201d against IDR data</p>\n<ul>\n<li>go to <a href=\"https://idr.openmicroscopy.org/about/itr.html\" rel=\"nofollow noopener\">https://idr.openmicroscopy.org/about/itr.html</a>\n</li>\n<li>click on the N (last column) in the CellProfiler row. A dialog will pop up</li>\n<li>Click on the Notebook link to start an instance.</li>\n</ul>\n<p>This might give you an idea on how to interface CellProfiler and OMERO.</p>\n<p>Cheers</p>\n<p>Jmarie</p>", "<p>I have tested the notebooks with CellProfiler 3.1.8 and it works!</p>\n<p>Cheers</p>\n<p>Jmarie</p>", "<p>Hello All,</p>\n<p>Is there a way I can connect to Omero from my CellProfiler running on Windows (AWS EC2 machine) or local Mac ? I still cannot get the source as Omero to work within the app.</p>\n<p>Any help would be appreciated.</p>\n<p>Regards,<br>\nSarthak</p>", "<p><a class=\"mention\" href=\"/u/spatel\">@spatel</a> Hello,</p>\n<p>Do you mean connect from the CP User Interface ? If yes, it does not seem to me that this is possible unless using a very specific CellProfiler version, but the CellProfiler Team might know better. <a href=\"https://github.com/CellProfiler/CellProfiler/wiki/OMERO:-Accessing-images-from-CellProfiler\" class=\"inline-onebox\">OMERO: Accessing images from CellProfiler \u00b7 CellProfiler/CellProfiler Wiki \u00b7 GitHub</a> seems to be linking back to this very thread.</p>\n<p>There are some examples of connecting CellProfiler and OMERO using their respective APIs, if you want to discuss this, let us know please.</p>\n<p>Hope this helps</p>\n<p>Best</p>\n<p>Petr</p>"], "78759": ["<p>Hi everyone, I am very new to CellProfiler but I have read a couple of recent papers using this software to calculate nuclear envelope invaginations in cells. I would like to do something similar, but am not sure where to start. I have attached the images I want to use this technique for. I have stained for DAPI, Lamin B1, and Actin. A lot of the papers specified they used the amount of Lamin B1 fluorescence in the nucleus to calculate this occurrence. However, I know I am asking a lot with little insight so I do understand if no one is willing to help.<br>\nThank you in advanced. <img src=\"https://emoji.discourse-cdn.com/twitter/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/tIUmm4aZIFXYdOvyJotMnxYosGr.pdf\">Screen Shot 2023-03-17 at 10.57.23 AM.pdf</a> (3.2 MB)</p>"], "75176": ["<p>Hello,</p>\n<p>We\u2019re coming across this error running the attached pipeline after using ilastik for pixel classification. Other posts on the forum have suggested to look into the grayscale vs. color image issue, so we are thinking that ilastik might be changing the image because the pipeline works for non-pixel classified images (without ilastik).</p>\n<p>Attached please find both the ilastik and cellprofiler settings/error screenshots. We would greatly appreciate any input!</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/ayLLSMrp9MECN8yWqEaSUe2s3as.cpproj\">Test_CP (2) - Copy.cpproj</a> (757.6 KB)</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b7bb6dc804f908fcd0d1865f5b251416173668e8.png\" data-download-href=\"/uploads/short-url/qdmWnbvU68zyM7M1xWScMODfqnS.png?dl=1\" title=\"cellprofiler names and types error\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7bb6dc804f908fcd0d1865f5b251416173668e8_2_690x385.png\" alt=\"cellprofiler names and types error\" data-base62-sha1=\"qdmWnbvU68zyM7M1xWScMODfqnS\" width=\"690\" height=\"385\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7bb6dc804f908fcd0d1865f5b251416173668e8_2_690x385.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/7/b7bb6dc804f908fcd0d1865f5b251416173668e8_2_1035x577.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/7/b7bb6dc804f908fcd0d1865f5b251416173668e8.png 2x\" data-dominant-color=\"F3F3F3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">cellprofiler names and types error</span><span class=\"informations\">1259\u00d7704 52.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/ba1260c298cf7f27b3a42d48b7f054f67592f38a.png\" data-download-href=\"/uploads/short-url/qy4bpLetwILKWdniSelmr3UgJiq.png?dl=1\" title=\"ilastik\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/a/ba1260c298cf7f27b3a42d48b7f054f67592f38a_2_462x500.png\" alt=\"ilastik\" data-base62-sha1=\"qy4bpLetwILKWdniSelmr3UgJiq\" width=\"462\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/a/ba1260c298cf7f27b3a42d48b7f054f67592f38a_2_462x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/a/ba1260c298cf7f27b3a42d48b7f054f67592f38a_2_693x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/a/ba1260c298cf7f27b3a42d48b7f054f67592f38a.png 2x\" data-dominant-color=\"EAEAEB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">ilastik</span><span class=\"informations\">698\u00d7754 35.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Alyssia,</p>\n<p>Welcome to the forum! <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Have you taken a look at the after pixel classification.tiff image produced by ilastik? Sometimes you need to select a particular file type (e.g. multipage tiff instead of just tiff). If you can attach your after pixel classification image here I\u2019m happy to try it out.</p>\n<p>Rebecca</p>", "<p>Hello Rebecca,</p>\n<p>Thank you so much for your advice. Yes, our after pixel classification .tiff image is produced by ilastik. Attached here is one of our images.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/7bA82qmMnYw2dCzQwjMgFjw0OER.tiff\">after pixel classification - DNA 191lr (12.08).tiff</a> (249.9 KB)</p>\n<p>Thank you,<br>\nAlyssia</p>", "<p>Hi,<br>\nI have two potential options for you. Have you tried using \u201ctiff sequence\u201d for export? This will produce 1 image per class in your classifier and you can use rules in NamesAndTypes to import them in CellProfiler. Alternatively, you can change the data type to \u201cunsigned 32 bit\u201d as below and this worked for me:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/7/7797558ca16d7d461706f0b4db0a66f145400abc.png\" data-download-href=\"/uploads/short-url/h3X39TNztW7tdhAUvJ3X9qRbZOQ.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/7797558ca16d7d461706f0b4db0a66f145400abc_2_472x500.png\" alt=\"image\" data-base62-sha1=\"h3X39TNztW7tdhAUvJ3X9qRbZOQ\" width=\"472\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/7797558ca16d7d461706f0b4db0a66f145400abc_2_472x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/7797558ca16d7d461706f0b4db0a66f145400abc_2_708x750.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/7/7797558ca16d7d461706f0b4db0a66f145400abc_2_944x1000.png 2x\" data-dominant-color=\"E7E7E8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1370\u00d71450 95.9 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Thank you, Rebecca!</p>"], "73128": ["<p>Hi, I am relatively new to using cell profiler and have a question about an error have been getting consistently. I am trying to count the signals in a single channel image of a slice of spinal cord tissue. I am using the 4.2.4 version on a windows desktop. I am also using the \u201cCounting and Scoring\u201d example package, which I have modified slightly to use for the cy5 images I am trying to analyze. However, despite changing the absolute minimum, I am still getting an error message that I cannot seem to figure out.</p>\n<p>The message states, \u201cError while processing NamesAndTypes: (Worker) ValueError: the input array must have size 3 along 'channel_axis\u201d, got (1110, 1641, 4) Do you want to stop processing?\"</p>\n<p>I chose to continue processing, and the results from this analysis do not provide any concrete results. For example, I am not getting the images with the pixel highlighting and signal outlines/annotations despite that being in the pipeline.</p>\n<p>Does anyone have any suggestions or know what I can do to fix this? I am confused also because I used the same exact pipeline on a mac laptop a couple months ago (which I very carefully documented) and I was able to get some great results from that.</p>\n<p>I sincerely appreciate any advice!</p>", "<p>Hi <a class=\"mention\" href=\"/u/t_mas\">@T_MAS</a></p>\n<p>Sorry to hear that, can you share one of your images? Can you also share your modified pipeline? Apparently your image has 4 channels and not 3 as expected.</p>\n<p>Best,<br>\nMario</p>", "<p><a class=\"attachment\" href=\"/uploads/short-url/1kTWo5qddZt6AgjU4yzL1N3DWXO.cpproj\">working pipeline 102522.cpproj</a> (449.2 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/i3QGnvDgvcdH6tGFUDpMbN6OrkI.tiff\">CellProfilerTestImage.tiff</a> (7.0 MB)</p>\n<p>Hi Mario!</p>\n<p>Here is the image I am trying to count signals for, as well as the modified pipeline I am using. This is odd to me because I thought my image only had 1 channel, but if you can explain it to me I would greatly appreciate it.</p>\n<p>Warmly,<br>\nT</p>", "<p>Your image is RGB and so to CellProfiler has 3 channels, not 1.</p>\n<p>This and the fact that the scaling is in inches (here it is open in FIJI) leads me to believe this is some sort of display type export from another software.</p>\n<p>Could you explain the origin of the image?</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/4/344d3ada190f7153ed19667bd595a4608f35d299.png\" alt=\"image\" data-base62-sha1=\"7sGi9if6PeSJJVw7hd7j8hY6NER\" width=\"596\" height=\"310\"></p>", "<p>I had the same problem, it was solved by converting the images to grayscale<br>\nfor example you can use the following site:</p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://pinetools.com/grayscale-image\">\n  <header class=\"source\">\n      <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/8/68b2a4cf56ee45860f7595024d1ebb91cc372777.png\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https://pinetools.com/grayscale-image\" target=\"_blank\" rel=\"noopener nofollow ugc\">PineTools</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"https://pinetools.com/grayscale-image\" target=\"_blank\" rel=\"noopener nofollow ugc\">Grayscale image online</a></h3>\n\n  <p>Desaturate completely an image</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n", "<p>Hello <a class=\"mention\" href=\"/u/t_mas\">@T_MAS</a>,</p>\n<p>Sorry for the late reply, but it seems a bug in our side, that just affect CP 4.2.4, the practical solution would be use CP4.2.1 in your Windows machine.</p>\n<p>We gonna correct the error for the next release.</p>\n<p>Thank you.<br>\nMario</p>"], "74154": ["<p>Pipeline:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/I6j72kemgUTPVp5ZFftTFr2XWX.cpproj\">test5.cpproj</a> (743.1 KB)</p>\n<p>Original image (from Zen software; two channels)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/r6hBtzhcErqUwmDAT7BANfknO8A.czi\">Snap-2124.czi</a> (18.1 MB)</p>\n<p>PDF with examples:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/2pwNObh8BcQ9AYgVctDzOSzmW30.pdf\">Images.pdf</a> (979.3 KB)</p>\n<h3>\n<a name=\"background-1\" class=\"anchor\" href=\"#background-1\"></a>Background</h3>\n<p>I am trying to quantify the percentage of positive cells that stain for PAX6 (a marker, expressed in the nuclei) on the total number of cells stained with DAPI.<br>\nAs you can see from the image the culture is very dense and CellProfiler has some difficulties to identify each nucleus.</p>\n<p>I used <span class=\"hashtag\">#identifyprimaryobject</span> (IPO) to identify in the first channel the number of cells stained for DAPI (see pipeline) and then again, for the second channel, I used IPO to identify the number of cells stained for PAX (my marker). However, when I run IPO for PAX in the second channel, CellProfiler cannot distiguish very well between DAPI and PAX staining.</p>\n<p>Can somebody help me to improve this analysis?<br>\nIs there a way to better identify the positive cells stained for PAX avoiding DAPI staining?</p>\n<p>I found something similar related to my question, but it did help me a lot <img src=\"https://emoji.discourse-cdn.com/twitter/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"><a href=\"https://forum.image.sc/t/count-and-calculate-positive-stained-cells-immunofluorescence-staining-in-ratio-to-stained-nuclei/45862\" class=\"inline-onebox\">Count and calculate positive stained Cells Immunofluorescence Staining in ratio to stained nuclei</a>)</p>\n<p>Thank you a lot!</p>"], "75691": ["<p>Hi all, first time poster and Cell-profiler user here.<br>\nI have been using the RelateObjects module to identify touching objects between two nuclear foci. The exported excel gave the index number for the \u201cChild objects\u201d - which is super useful. However, I can not so far find an efficient way to identify each child object\u2019s touching parent object, whose indexing information is needed if to examine the relationship between features of the parents vs. child\u2019s. I appreciate any suggestion and inputs!</p>", "<p>Hi Lu! Welcome to the forum! <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>To find each child object\u2019s parent ID, check out the column <code>Parent_Cells</code> (it will be \u201cParent_\u201d then whatever you have called the Parent class. If you haven\u2019t, I would also recommend splitting the spreadsheet export to have one sheet per object type. I\u2019ll include an example of this type of export here.</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/5b1BtMD4fFF5IQ9YbWhXGXajv9U.cppipe\">pipeline.cppipe</a> (17.2 KB)</p>\n<p>Rebecca</p>", "<p>One additional thing to add on top of everything Rebecca said - if you want a table that is just the relationships, you can get one in ExportToSpreadsheet by setting \u201cExport all measurement types\u201d to No and then selecting, in addition to all your regular spreadsheets like Experiment, Image, and your objects, the Object Relationships spreadsheet, which is exactly what it sounds like. If you\u2019re using ExportToDatabase, you can get the same info by setting \u201cExport object relationships?\u201d to \u201cYes\u201d. Especially in ExportToSpreadsheet, it\u2019s not documented as well as it should be, something we hope to fix in CellProfiler 5!</p>", "<p>Hi Beth and Rebecca,</p>\n<p>Thanks for your replies both, which have been helping me move forward with my analyses.</p>\n<p>One thing I got disconnected was the exported Excel spreadsheets, with columns and file names, with the actual measurement.</p>\n<p>I appreciate the incredible power of CellProfiler \u2013 it has fundamentally changed how we do our cell biological research. Looking forward to new features in CP5.</p>\n<p>Best</p>\n<p>Lu</p>"], "75692": ["<p>Hello everyone,</p>\n<p>I am new to CellProfiler and am likely making a newbie mistake. I have been attempting to process a dataset of large images in CellProfiler 4.2.5. I keep getting the error message related to Java heap space, however I already increased my Java heap space manually through windows control panel. I increased it to a rather large size (10GB), but I still keep getting the same error. Is there anything else I can do? Please let me know if there are any other details I can provide.</p>\n<p>Thank you all very much,</p>\n<p>Toni</p>", "<p>Hi Toni,</p>\n<p>Welcome to the forum! <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>How large are your images? Are you able to process just one image or are you still getting the error? If the error is caused by processing many images and your computer can\u2019t handle &gt; 10GB, you might need a more powerful computer. It\u2019s also possible your images are very large and large image support is something we\u2019re working on supporting in the near future.</p>\n<p>Rebecca</p>", "<p>Hi, bumping this thread since I\u2019m having the same issue. I\u2019ve read the various posts on here about this, and have tried increasing the max memory for java but without any change.</p>\n<p>For context, in my case, I\u2019m trying to make a pipeline for ~500 mb *.czi images. (2D, 4 color tile scans) I\u2019m on a workstation with 192 gb of RAM and have allocated a max of 100 gb to java. However, even with only 1 image, as soon as I go into test mode, I get the above error. Any advice? (This workstation is running Windows 7 if that makes a difference!)</p>"], "78764": ["<p>Hello everyone,</p>\n<p>excuse my basic questions but I have so far used QuPath only on fluorescent images and just recently been trying to incorporate it also on Light-Microscopy.</p>\n<p>My samples are mouse-livers that are stained via PAS-staining. My aim is to quantify the amount of glycogen in various degrees of septic disease on my slides. Visually there is a tremendous difference, but I would like to obtain a quantitative measure for my observation to assess my results with statistics (Avoiding a self-made scoring system of whom I am not a fan). Do you have any ideas on a interesting measure (percentage area, positive cells etc.) and how to approach this image analysis?</p>\n<p>Thank you all in advance<br>\nBest and have a nice weekend<br>\nAlex<br>\n(To avoid upcoming questions, there is no difference in staining intensity, both organs were stained on the same glass slide to avoid confusion. The difference is almost exclusively to be attributed to the disease severity)</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca.jpeg\" data-download-href=\"/uploads/short-url/eunvW3UVQGrQRdPOmBCZe1OHzHI.jpeg?dl=1\" title=\"Leber 90p 9.tif - Series 0 (1, x=15116, y=12487, w=3321, h=2421)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca_2_685x500.jpeg\" alt=\"Leber 90p 9.tif - Series 0 (1, x=15116, y=12487, w=3321, h=2421)\" data-base62-sha1=\"eunvW3UVQGrQRdPOmBCZe1OHzHI\" width=\"685\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca_2_685x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca_2_1027x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/6/5/658d8f52a01dca24efda5051be058007a83ea7ca_2_1370x1000.jpeg 2x\" data-dominant-color=\"776494\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Leber 90p 9.tif - Series 0 (1, x=15116, y=12487, w=3321, h=2421)</span><span class=\"informations\">1920\u00d71400 434 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33.jpeg\" data-download-href=\"/uploads/short-url/nF6JpOr3SSIn5RCRWRo4CgmZLXR.jpeg?dl=1\" title=\"Leber Base 1.tif - Series 0 (1, x=4292, y=11320, w=6138, h=5039)\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33_2_609x500.jpeg\" alt=\"Leber Base 1.tif - Series 0 (1, x=4292, y=11320, w=6138, h=5039)\" data-base62-sha1=\"nF6JpOr3SSIn5RCRWRo4CgmZLXR\" width=\"609\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33_2_609x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33_2_913x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/5/a5d79d1187466614bb4600b97101add0ea578a33_2_1218x1000.jpeg 2x\" data-dominant-color=\"452257\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Leber Base 1.tif - Series 0 (1, x=4292, y=11320, w=6138, h=5039)</span><span class=\"informations\">1920\u00d71576 503 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<aside class=\"quote no-group\" data-username=\"Grambuld\" data-post=\"1\" data-topic=\"78764\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/278dde/40.png\" class=\"avatar\"> Alexander:</div>\n<blockquote>\n<p>Visually there is a tremendous difference, but I would like to obtain a quantitative measure for my observation to assess my results with statistics (Avoiding a self-made scoring system of whom I am not a fan). Do you have any ideas on a interesting measure (percentage area, positive cells etc.) and how to approach this image analysis?</p>\n</blockquote>\n</aside>\n<p>Seems most appropriate to use a thresholder for a particular stain deconvolution if you want to assess disease severity by area. If distribution is important, or cell density, you may try cell segmentation with one of the deep learning algorithms, but I have my doubts how well that would work for the darker stain.<br>\nMight be worth checking your reagents, since that is much darker than I usually see for PAS.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc.png\" data-download-href=\"/uploads/short-url/4BOXVxydKQW47UcwmHkioOQvNVO.png?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc_2_690x228.png\" alt=\"image\" data-base62-sha1=\"4BOXVxydKQW47UcwmHkioOQvNVO\" width=\"690\" height=\"228\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc_2_690x228.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc_2_1035x342.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/0/204f3379ddafb06f8da98b6430712b48bacaa1cc_2_1380x456.png 2x\" data-dominant-color=\"EFEFEF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1388\u00d7459 48.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nThe intensity might be due to disease, but the color might be an issue as well, which could cause difficulties with quantification.</p>\n<p>I don\u2019t know that PAS staining is quantitative, and I kind of doubt you can use stain intensity to measure an amount of something in this case. Especially when it is so dark.</p>", "<aside class=\"quote no-group\" data-username=\"Research_Associate\" data-post=\"2\" data-topic=\"78764\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/research_associate/40/17481_2.png\" class=\"avatar\"> MicroscopyRA:</div>\n<blockquote>\n<p>Might be worth checking your reagents, since that is much darker than I usually see for PAS.</p>\n</blockquote>\n</aside>\n<p>Maybe <a class=\"mention\" href=\"/u/smcardle\">@smcardle</a> or <a class=\"mention\" href=\"/u/zbigniew_mikulski\">@Zbigniew_Mikulski</a> or the pathologists have more experience though. Will cross link once I find that.</p>", "<p>Hi <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a> , I agree the PAS stains seems very dark on the posted image. Polysaccharides such as glycogen, glycoproteins, and glycolipids stain bright magenta with PAS without diastase, while normal liver without glycogen, for example, would have only the hematoxylin counter stain. So one would have to design a QuPath experiment in distinguishing cells with the magenta pigment to the ones without it.</p>\n<p>I tried before to establish thresholds to differentiate colors in qupath, but I was unsuccesful. I tried to measure fibrosis using Masons Trichrome in human cardiac tissue.</p>", "<aside class=\"quote no-group\" data-username=\"Fabio_Tavora\" data-post=\"4\" data-topic=\"78764\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/fabio_tavora/40/49123_2.png\" class=\"avatar\"> Fabio Tavora:</div>\n<blockquote>\n<p>I tried before to establish thresholds to differentiate colors in qupath</p>\n</blockquote>\n</aside>\n<p>Hmm, Masson\u2019s should be pretty straightforward since the blue and red are generally well separated (with color decon), but if you have <em>overlap</em> (nuclei are a problem), if the staining is too dark you will be unable to accurately separate the colors. In general, dark staining is a problem because black is all colors, and the closer you get to black, the less information you have.</p>\n<p>Pixel classifier might be a better way to go about it, but last time I did Masson\u2019s QuPath didn\u2019t have a pixel classifier and relied on\u2026 dun dun dunnnnnn superpixels (<a class=\"mention\" href=\"/u/smcardle\">@smcardle</a> ).</p>", "<p>Here are some examples:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4.jpeg\" data-download-href=\"/uploads/short-url/7LHc1HMc3DGBzmMTCbbmKdpSvUE.jpeg?dl=1\" title=\"141 masson pre\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4_2_690x424.jpeg\" alt=\"141 masson pre\" data-base62-sha1=\"7LHc1HMc3DGBzmMTCbbmKdpSvUE\" width=\"690\" height=\"424\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4_2_690x424.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4_2_1035x636.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/6/36737a61b20ef5bedd2db504bde91d80bf9410f4.jpeg 2x\" data-dominant-color=\"AB558F\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">141 masson pre</span><span class=\"informations\">1249\u00d7769 259 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e.jpeg\" data-download-href=\"/uploads/short-url/9zwGgGRuLdTIPxmMGI1BOxIgSlg.jpeg?dl=1\" title=\"whole case\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e_2_690x305.jpeg\" alt=\"whole case\" data-base62-sha1=\"9zwGgGRuLdTIPxmMGI1BOxIgSlg\" width=\"690\" height=\"305\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e_2_690x305.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e_2_1035x457.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/4/3/4317a8f3791527a67f9b82f339ad5986610e2d1e_2_1380x610.jpeg 2x\" data-dominant-color=\"573E49\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">whole case</span><span class=\"informations\">1920\u00d7851 106 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Here am trying to \u201cteach\u201d qupath what is fibrotic tissue:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade.jpeg\" data-download-href=\"/uploads/short-url/zkkSg1Tb9bysMcbM3B2IBaG46hE.jpeg?dl=1\" title=\"fibrosis teachin\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade_2_690x305.jpeg\" alt=\"fibrosis teachin\" data-base62-sha1=\"zkkSg1Tb9bysMcbM3B2IBaG46hE\" width=\"690\" height=\"305\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade_2_690x305.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade_2_1035x457.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/7/f798991d8f9447f614515d95289cc40261a85ade_2_1380x610.jpeg 2x\" data-dominant-color=\"AE5892\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">fibrosis teachin</span><span class=\"informations\">1920\u00d7851 429 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Ah, I was thinking pure collagen detection, which only requires a blue thresholder. If you want context, that will be much harder to impossible for the pixel classifier. Until there is a deep learning option, I don\u2019t think it will work well unless there is a strong per pixel or pixel cluster difference between areas. Context (it\u2019s red, but it\u2019s nearby enough blue stuff) isn\u2019t going to work.</p>", "<aside class=\"quote no-group\" data-username=\"Grambuld\" data-post=\"1\" data-topic=\"78764\">\n<div class=\"title\">\n<div class=\"quote-controls\"></div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://avatars.discourse-cdn.com/v4/letter/g/278dde/40.png\" class=\"avatar\"> Alexander:</div>\n<blockquote>\n<p>PAS-staining</p>\n</blockquote>\n</aside>\n<p>Hi, I strongly suspect that those two slides were stained with PAS  and they also had haematoxylin (?) to identify nuclei.<br>\nPAS looks magenta colour, so in those two slides you can\u2019t really tell what is the contribution of the PAS and which is haematoxylin.<br>\nOne idea would be to stain for PAS but not use any other nuclear stain.<br>\nIf you want to \u201cquantify\u201d glycogen (as in how much there is), please consider that PAS is not specific to glycogen as it also stains <em><strong>many</strong></em> other things like basement membrane components, fungi, cellulose, glycoproteins, etc.<br>\nI would be a bit cautious about attempting to equate stain to quantities just by considering intensity without a proper calibration <strong>and</strong> being confident that the stain is stoichiometric (not sure it is). Be aware that if it is not stoichiometric, the intensity of the stain detected will not  be a reliable measure of the amount of glycogen present.<br>\nHope this helps.</p>", "<p>+1 to what Gabriel wrote.</p>\n<p>A couple of other thoughts - change gamma to better visualize the stain</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18.jpeg\" data-download-href=\"/uploads/short-url/pf6Q9U7tQt7SJMluZsH9acpHcKc.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18_2_690x280.jpeg\" alt=\"image\" data-base62-sha1=\"pf6Q9U7tQt7SJMluZsH9acpHcKc\" width=\"690\" height=\"280\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18_2_690x280.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18_2_1035x420.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/0/b0eb9de125a2e10a7727395ede199e5c395a5c18_2_1380x560.jpeg 2x\" data-dominant-color=\"8F7B9C\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7780 201 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Skipping hematoxylin can help to separate the stains, here is an example of Alcian Blue-PAS in the crypts of the small intestine differentially staining Goblet cells and Paneth cells.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2.jpeg\" data-download-href=\"/uploads/short-url/xeCeAJt5D1wb3FFlMumvce67c2u.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2_2_528x500.jpeg\" alt=\"image\" data-base62-sha1=\"xeCeAJt5D1wb3FFlMumvce67c2u\" width=\"528\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2_2_528x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2_2_792x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/8/e8eeb9275734fba7eefbc7a4cd8daad2046275b2_2_1056x1000.jpeg 2x\" data-dominant-color=\"B4D2DF\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1221\u00d71155 93.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Adding a blue or blackish hematoxylin to it will make it a lot harder. Histology stains are weird, it\u2019s not always additive, and the limitations of RGB imaging mean that less is often more.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813.jpeg\" data-download-href=\"/uploads/short-url/iiy23xqzFPV6MgVYMLzXtEGrWj9.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813_2_378x500.jpeg\" alt=\"image\" data-base62-sha1=\"iiy23xqzFPV6MgVYMLzXtEGrWj9\" width=\"378\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813_2_378x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813_2_567x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/8/0/803fe7d0a70558017768107a75d766ad78c2a813_2_756x1000.jpeg 2x\" data-dominant-color=\"6480BA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">902\u00d71193 60.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>For collagen staining, I highly recommend Picrosirus red - Fast green, it\u2019s much more straightforward to use with a simple thresholder or pixel classifier on color-deconvolved samples and in our hands, there are fewer artifacts like plasma being stained by components of trichrome <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4682672/\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Histochemical Detection of Collagen Fibers by Sirius Red/Fast Green Is More Sensitive than van Gieson or Sirius Red Alone in Normal and Inflamed Rat Colon - PMC</a></p>\n<p>An example file to play with: <a href=\"https://drive.google.com/open?id=1TJBdmc6Mt8CgUBQ90bu39BUs_-s4bV_p&amp;authuser=zmikulski%40lji.org&amp;usp=drive_fs\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">PSR-Fast Green.czi - Google Drive</a></p>\n<p>Best, Z.</p>"], "76209": ["<p>Hello,</p>\n<p>I am trying to count puncta in 3D fluorescent cells with Cellprofiler. I have 4 channels for DAPI, RFP+ cells, YFP+ cells, and RNA FISH.</p>\n<p>I tried generating watershed from distance along with the shape declump method, but I ran into issues segmenting touching objects (as shown in image below).</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/1/9163c7e241a10271f019d0aa773eb34226132179.jpeg\" data-download-href=\"/uploads/short-url/kKb0Mnl0wsP036vOPnbQXMSGNER.jpeg?dl=1\" title=\"RNA_Threshold0.15_watershed_distance:shape\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/1/9163c7e241a10271f019d0aa773eb34226132179_2_690x211.jpeg\" alt=\"RNA_Threshold0.15_watershed_distance:shape\" data-base62-sha1=\"kKb0Mnl0wsP036vOPnbQXMSGNER\" width=\"690\" height=\"211\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/1/9163c7e241a10271f019d0aa773eb34226132179_2_690x211.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/1/9163c7e241a10271f019d0aa773eb34226132179_2_1035x316.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/1/9163c7e241a10271f019d0aa773eb34226132179_2_1380x422.jpeg 2x\" data-dominant-color=\"424242\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">RNA_Threshold0.15_watershed_distance:shape</span><span class=\"informations\">1920\u00d7588 44.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Therefore, I tried segmenting puncta with the \u201cintensity declump method\u201d, but it kept sending me error messages:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/e/c/ec5ef227729f6c6d72129d30029935a06118c3e6.jpeg\" data-download-href=\"/uploads/short-url/xJ26qERUmsSFf03RjWsyOUGAzyu.jpeg?dl=1\" title=\"RNA_intensitywatershed_error\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ec5ef227729f6c6d72129d30029935a06118c3e6_2_690x480.jpeg\" alt=\"RNA_intensitywatershed_error\" data-base62-sha1=\"xJ26qERUmsSFf03RjWsyOUGAzyu\" width=\"690\" height=\"480\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ec5ef227729f6c6d72129d30029935a06118c3e6_2_690x480.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ec5ef227729f6c6d72129d30029935a06118c3e6_2_1035x720.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/e/c/ec5ef227729f6c6d72129d30029935a06118c3e6_2_1380x960.jpeg 2x\" data-dominant-color=\"353435\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">RNA_intensitywatershed_error</span><span class=\"informations\">1920\u00d71337 130 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I also tried \u201cintensity declumping\u201d and \u201cgenerate from markers\u201d with my fluorescent cells, but it is not working either:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/4/24864dffdbf3a671786fbc1b5319227de4ac6836.jpeg\" data-download-href=\"/uploads/short-url/5d6VZFp8VAvJ3jtgLsdXriiQyWy.jpeg?dl=1\" title=\"RFP_watershed error with intensity declumping\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24864dffdbf3a671786fbc1b5319227de4ac6836_2_631x500.jpeg\" alt=\"RFP_watershed error with intensity declumping\" data-base62-sha1=\"5d6VZFp8VAvJ3jtgLsdXriiQyWy\" width=\"631\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24864dffdbf3a671786fbc1b5319227de4ac6836_2_631x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24864dffdbf3a671786fbc1b5319227de4ac6836_2_946x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/4/24864dffdbf3a671786fbc1b5319227de4ac6836_2_1262x1000.jpeg 2x\" data-dominant-color=\"E8E8E9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">RFP_watershed error with intensity declumping</span><span class=\"informations\">1920\u00d71520 171 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/7/1718961101a7cd649f0fdbf7d47ba44d2d90c5ba.jpeg\" data-download-href=\"/uploads/short-url/3ijELrIO37OnYHssQtwPAXFl2jg.jpeg?dl=1\" title=\"RFP_watershed error with generate from marker\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/1718961101a7cd649f0fdbf7d47ba44d2d90c5ba_2_690x487.jpeg\" alt=\"RFP_watershed error with generate from marker\" data-base62-sha1=\"3ijELrIO37OnYHssQtwPAXFl2jg\" width=\"690\" height=\"487\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/1718961101a7cd649f0fdbf7d47ba44d2d90c5ba_2_690x487.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/1718961101a7cd649f0fdbf7d47ba44d2d90c5ba_2_1035x730.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/1/7/1718961101a7cd649f0fdbf7d47ba44d2d90c5ba_2_1380x974.jpeg 2x\" data-dominant-color=\"E8E7E8\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">RFP_watershed error with generate from marker</span><span class=\"informations\">1920\u00d71356 169 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Overall, I\u2019m having trouble segmenting touching objects, and lowering watershed parameters such as \u201cfootprint\u201d and \u201cminimum distance between seeds\u201d leads to oversegmentation issues. Therefore, I\u2019m hoping that someone can teach me how to segment 3D objects based on markers and intensity. I see that there are reference images that I need to add, but the annotation didn\u2019t explain what types of images they are looking for. I added my DAPI threshold image for the marker reference and the original fluorescent cell image for the intensity declumping, but that resulted in error messages.</p>\n<p>Any help would be appreciated!</p>", "<p>I forgot to mention this, but I am using Cellprofiler 4.2.5 on a MacOS system. Thank you!</p>", "<p>Hi Slaine,</p>\n<p>It looks like there is a resize module in your pipeline, and the modules that are complaining are doing so about a size mismatch - is it possible that some of these images in these modules ultimately trace back to the resized image, and some do not? It\u2019s hard to say for sure without the pipeline but that would be my best guess.</p>\n<p>If that doesn\u2019t fix it, please feel free to upload the pipeline and a couple of sample images for us to take a look!</p>", "<p>Hi Beth,</p>\n<p>Thank you so much for the suggestion! Unfortunately, I tried resizing the image and it still doesn\u2019t work. Do you know where I can find examples that use the intensity declumping method, or any documentation on this? I would also really appreciate it if you could take a look at my pipeline. You can access my pipeline and sample images at this site:<br>\n<a href=\"https://drive.google.com/drive/folders/1rYw0hBadfJO1UNAeM78mmd5UQIk1Kkuu?usp=share_link\" rel=\"noopener nofollow ugc\">https://drive.google.com/drive/folders/1rYw0hBadfJO1UNAeM78mmd5UQIk1Kkuu?usp=share_link</a></p>", "<p>Hi <a class=\"mention\" href=\"/u/slaine_troyard\">@Slaine_Troyard</a>,</p>\n<p>if Fiji is an alternative to CellProfile for you in this case the following macro separates the spots relatively well (which might be further improved):</p>\n<pre><code class=\"lang-auto\">run(\"Make 3D Image Isotropic\");\nrun(\"Voronoi Threshold Labler (2D/3D)\", \"filtermethod=Gaussian filterradius=1.0 backgroundsubtractionmethod=TopHat backgroundradius=3.0 histogramusage=full thresholdmethod=Li fillholes=Off separationmethod=Maxima spotsigma=0.0 maximaradius=1.0 volumerange=0-Infinity excludeonedges=false outputtype=Labels stackslice=20 applyoncompleteimage=false processonthefly=true\");\n//run(\"Separate Labels (2D/3D)\"); //optional to see separations better but might remove tiny spots\n</code></pre>\n<p>To be able to run this in a (new) Fiji installation please follow first the <a href=\"https://biovoxxel.github.io/bv3dbox/#installation\">installation instructions</a> for the <a href=\"https://biovoxxel.github.io/bv3dbox/\">BioVoxxel 3D Box (bv3dbox)</a>.</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a3acf54cf3ab034668dc735833ee755c88ac34ab.png\" alt=\"image\" data-base62-sha1=\"nlWo72KFHlbYMuaSQpM2ZFkFuNZ\" width=\"564\" height=\"495\"></p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/4/a42d6fb2910378b0fbe596cc458dda991e49c8e9.png\" alt=\"image\" data-base62-sha1=\"nqnEqlU07uxiMcEZ50w4iIleNSN\" width=\"564\" height=\"495\"></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1f31f7b7894a406fa0217723a5972a45f6d8cd3.jpeg\" data-download-href=\"/uploads/short-url/tXiG9xxEv9QFqvCzJItc0WGMsLx.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1f31f7b7894a406fa0217723a5972a45f6d8cd3_2_569x499.jpeg\" alt=\"image\" data-base62-sha1=\"tXiG9xxEv9QFqvCzJItc0WGMsLx\" width=\"569\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1f31f7b7894a406fa0217723a5972a45f6d8cd3_2_569x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1f31f7b7894a406fa0217723a5972a45f6d8cd3.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1f31f7b7894a406fa0217723a5972a45f6d8cd3.jpeg 2x\" data-dominant-color=\"232022\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">846\u00d7743 206 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a></p>\n<p>Thank you for the suggestion. I tried installing Biovoxxel 3D Box and the other associated plugins, but I still ran into an error message when I tried to run the macro code (as shown below). This is my first time running scripts on imagej, so I\u2019m wondering if I\u2019m missing other important plugins.</p>\n<p>Best,<br>\nSlaine</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ceb5fa94678837ebbadc31440acc95d4a6833977.jpeg\" data-download-href=\"/uploads/short-url/tuEfivHStCTEF6JRvHT6z80i4WH.jpeg?dl=1\" title=\"Screen Shot 2023-01-31 at 5.28.03 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ceb5fa94678837ebbadc31440acc95d4a6833977_2_690x374.jpeg\" alt=\"Screen Shot 2023-01-31 at 5.28.03 PM\" data-base62-sha1=\"tuEfivHStCTEF6JRvHT6z80i4WH\" width=\"690\" height=\"374\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ceb5fa94678837ebbadc31440acc95d4a6833977_2_690x374.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ceb5fa94678837ebbadc31440acc95d4a6833977_2_1035x561.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ceb5fa94678837ebbadc31440acc95d4a6833977_2_1380x748.jpeg 2x\" data-dominant-color=\"B3B2B3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-01-31 at 5.28.03 PM</span><span class=\"informations\">1920\u00d71041 224 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi <a class=\"mention\" href=\"/u/slaine_troyard\">@Slaine_Troyard</a>,</p>\n<p>Hm\u2026 strange. According to your screenshot the macro seems to process the images correctly.<br>\nThe error points out a problem with the image type. Have you potentially tried to run it on another type of image, such as an RGB/color image, or on a 2D image or stack slice instead of a complete stack?<br>\nEspecially because it produces the expected output, I am not sure where the error comes from. You seem to have installed all necessary plugins and update sites.<br>\nIs the error popping up every time you run it on that image?</p>", "<p>Hi <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a></p>\n<p>Thank you for the immediate response! Unfortunately, it seems that this error message shows up every time I try to run the program. It suggests that I contact the CLIJ2 developers about this, so that\u2019s what I\u2019m probably going to do.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/9/c95d649d5fd69cab8edd0b55acc8f2065133cea0.jpeg\" data-download-href=\"/uploads/short-url/sJm42C7yChXNYDIJF5LpfzmdEWs.jpeg?dl=1\" title=\"Screen Shot 2023-02-02 at 8.24.42 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/9/c95d649d5fd69cab8edd0b55acc8f2065133cea0_2_690x428.jpeg\" alt=\"Screen Shot 2023-02-02 at 8.24.42 AM\" data-base62-sha1=\"sJm42C7yChXNYDIJF5LpfzmdEWs\" width=\"690\" height=\"428\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/9/c95d649d5fd69cab8edd0b55acc8f2065133cea0_2_690x428.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/9/c95d649d5fd69cab8edd0b55acc8f2065133cea0_2_1035x642.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/9/c95d649d5fd69cab8edd0b55acc8f2065133cea0_2_1380x856.jpeg 2x\" data-dominant-color=\"BCB9BA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-02 at 8.24.42 AM</span><span class=\"informations\">1920\u00d71192 230 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>We can ask <a class=\"mention\" href=\"/u/haesleinhuepf\">@haesleinhuepf</a> in addition for his opinion in the case</p>", "<p>Hi <a class=\"mention\" href=\"/u/slaine_troyard\">@Slaine_Troyard</a> , hi <a class=\"mention\" href=\"/u/biovoxxel\">@biovoxxel</a> ,</p>\n<p>I think I have seen similar error messages on older Macs where the GPU was not fully supported. Can you run choose the CPU to execute the code on? It should work then.</p>\n<p>Let us know if this helps!</p>\n<p>Best,<br>\nRobert</p>\n<p>P.S.: The discussion continues here: <a href=\"https://forum.image.sc/t/biovoxxel-3d-error-due-to-clij2-exception/76748\" class=\"inline-onebox\">Biovoxxel 3D Error due to CLIJ2 exception</a></p>"], "78771": ["<p>Hello everyone,</p>\n<p>First of all, really thank you for the cellpose developers and experts.<br>\nI am a really fresh developer in this field and so curious about the powerful functions of cellpose 2.2.<br>\nNow I just started the cell segment by using cellpose 2.2 and succeeded to get  \u201c<em>_seg.npy\" and \"</em>_cp_output.jpg\u201d files from origine images by using windows command.<br>\nThis is the result.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d.jpeg\" data-download-href=\"/uploads/short-url/vPEVnpF0ctIpkD4pADsSgPXzX8V.jpeg?dl=1\" title=\"029_img.ome_cp_output\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg\" alt=\"029_img.ome_cp_output\" data-base62-sha1=\"vPEVnpF0ctIpkD4pADsSgPXzX8V\" width=\"690\" height=\"172\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/f/df1a58fe3e205d751493546cb043a2ca23a3204d_2_1380x344.jpeg 2x\" data-dominant-color=\"A5A7A3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">029_img.ome_cp_output</span><span class=\"informations\">3600\u00d7900 239 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>And now I want to get more detailed images from the \u201c<em>_seg.npy\" file.<br>\nI really want to do this but I am not sure how to do this because I just have some knowledge about Python and image processing.<br>\nIf possible, please share with me the python source code to get these individual images and correct position of ROIS from the \"</em>_seg.npy\u201d file.</p>\n<p>Thank all of you again.</p>", "<aside class=\"onebox githubblob\" data-onebox-src=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb\">\n  <header class=\"source\">\n\n      <a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb\" target=\"_blank\" rel=\"noopener\">github.com</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <h4><a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb\" target=\"_blank\" rel=\"noopener\">MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb</a></h4>\n\n\n      <pre><code class=\"lang-ipynb\">{\n  \"nbformat\": 4,\n  \"nbformat_minor\": 0,\n  \"metadata\": {\n    \"accelerator\": \"GPU\",\n    \"colab\": {\n      \"provenance\": [],\n      \"include_colab_link\": true\n    },\n    \"kernelspec\": {\n      \"display_name\": \"Python 3\",\n      \"language\": \"python\",\n      \"name\": \"python3\"\n    },\n    \"language_info\": {\n      \"codemirror_mode\": {\n        \"name\": \"ipython\",\n        \"version\": 3\n      },\n      \"file_extension\": \".py\",\n</code></pre>\n\n\n\n  This file has been truncated. <a href=\"https://github.com/MouseLand/cellpose/blob/e559dae0f9bb055aa69b84310229b6f66ea2492f/notebooks/run_cellpose_2.ipynb\" target=\"_blank\" rel=\"noopener\">show original</a>\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n<p>\nFrom github repository, it looks like the flags are default set to false to output those images individually (see last cell).</p>"], "75700": ["<p>Hi everyone, I\u2019m new to this forum and I\u2019m searching for help about an issue with my CellProfiler pipeline.  Is it possible that the image format changes how the pipeline works and specifically how the threshold method is settled? I\u2019m quantifying the number of foci present in the nuclei of my cells, in the IdentifyPrimaryObjects module, if I have a bmp image I have to apply the Otsu threshold method, while if I have a tif image I apply the Minimum Cross-Entropy method, so it change how the software identifies the objects.</p>", "<p>Hi Michela,</p>\n<p>Welcome to the forum! <img src=\"https://emoji.discourse-cdn.com/twitter/partying_face.png?v=12\" title=\":partying_face:\" class=\"emoji\" alt=\":partying_face:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Yes, the format can definitely matter. There are a few reasons. A major one is compression. If you save your images as JPEG files for instance, they will typically undergo a space-saving process called compression. This makes for a smaller file but changes the values in your image. For bmp vs tif, it\u2019s hard to know exactly what is happening without examples of the images, but it\u2019s possible the program you\u2019re saving the images in is applying some compression or changing the bit depth of the image. In general, we\u2019d recommend you use .tif files as most scientific image analysis software can export this and does so without changing your pixel values. But to check you could open both images in Fiji and then look at the histogram of values for each by pressing <code>h</code> after selecting each image.</p>\n<p>Rebecca</p>", "<p>Just changing the format of the pictures does change a lot of perimeters, such as scale (255 to 1000000\u2019s) An 8bit image changed to 16bit will turn invisible. And then everything Becky says is also correct so just slow down,<br>\nBob</p>"], "74675": ["<p>Hello! I am a new user of CellProfiler and have run into some issues. I want to count the number of dots in the nuclei of cells in an RNAscope assay. I have Opal 570, Opal 520 and DAPI dyes probing 3 different RNAs. I already ran max intensity projection on ImageJ and exported these images, so for each field of view I have 3 images.</p>\n<p>Previously, I optimized the pipeline for my experiment using a set of 2 dummy images (Opal 570 and DAPI); I have attached the pipeline here. I ran into no issues optimizing the modules in the pipeline and I borrowed the Speckles pipeline from CellProfiler. However, the errors arise when I loaded my experimental images.</p>\n<p>The weird thing is, if I open a new project, load in the same images in an empty pipeline, it is actually able to detect that there are 9 image sets in NamesAndTypes, and can even group them into 3s in the Group module. Once I toggle back to NamesAndTypes however, I receive the error message and am unable to get it to work anymore. In short, image set recognition only works in a fresh project, for the first time, for me at least. Could anyone explain why this is happening? Thank you so much!</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/1/c14cbc0847e4456e46f476803dad9fb7b3d30712.jpeg\" data-download-href=\"/uploads/short-url/rA0yoilViPmqIMUWxP337PXt6hQ.jpeg?dl=1\" title=\"Screen Shot 2022-12-05 at 12.02.24 PM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c14cbc0847e4456e46f476803dad9fb7b3d30712_2_690x431.jpeg\" alt=\"Screen Shot 2022-12-05 at 12.02.24 PM\" data-base62-sha1=\"rA0yoilViPmqIMUWxP337PXt6hQ\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c14cbc0847e4456e46f476803dad9fb7b3d30712_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c14cbc0847e4456e46f476803dad9fb7b3d30712_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/1/c14cbc0847e4456e46f476803dad9fb7b3d30712_2_1380x862.jpeg 2x\" data-dominant-color=\"2F2F30\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-05 at 12.02.24 PM</span><span class=\"informations\">1920\u00d71200 109 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7fb288d993f30169533b3bbae143c6165d1f9404.jpeg\" data-download-href=\"/uploads/short-url/idF98duSptRBfmmi9dqDozZVtTm.jpeg?dl=1\" title=\"Screen Shot 2022-12-05 at 11.59.27 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/f/7fb288d993f30169533b3bbae143c6165d1f9404_2_690x431.jpeg\" alt=\"Screen Shot 2022-12-05 at 11.59.27 AM\" data-base62-sha1=\"idF98duSptRBfmmi9dqDozZVtTm\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/f/7fb288d993f30169533b3bbae143c6165d1f9404_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/f/7fb288d993f30169533b3bbae143c6165d1f9404_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/f/7fb288d993f30169533b3bbae143c6165d1f9404_2_1380x862.jpeg 2x\" data-dominant-color=\"303031\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-05 at 11.59.27 AM</span><span class=\"informations\">1920\u00d71200 112 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7b81f7fd1b77ec0a2fd0f1789364aeb2354582d0.jpeg\" data-download-href=\"/uploads/short-url/hCBbvbMyC03In9OjwdCOiWaM6CA.jpeg?dl=1\" title=\"Screen Shot 2022-12-05 at 11.59.04 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b81f7fd1b77ec0a2fd0f1789364aeb2354582d0_2_690x431.jpeg\" alt=\"Screen Shot 2022-12-05 at 11.59.04 AM\" data-base62-sha1=\"hCBbvbMyC03In9OjwdCOiWaM6CA\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b81f7fd1b77ec0a2fd0f1789364aeb2354582d0_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b81f7fd1b77ec0a2fd0f1789364aeb2354582d0_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/b/7b81f7fd1b77ec0a2fd0f1789364aeb2354582d0_2_1380x862.jpeg 2x\" data-dominant-color=\"434345\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-05 at 11.59.04 AM</span><span class=\"informations\">1920\u00d71200 137 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/73666f81ebea7018f5b3473f8298800a92562048.jpeg\" data-download-href=\"/uploads/short-url/gsSnkxyGQHomqftSecQeMDdq3S8.jpeg?dl=1\" title=\"Screen Shot 2022-12-05 at 11.58.56 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73666f81ebea7018f5b3473f8298800a92562048_2_690x431.jpeg\" alt=\"Screen Shot 2022-12-05 at 11.58.56 AM\" data-base62-sha1=\"gsSnkxyGQHomqftSecQeMDdq3S8\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73666f81ebea7018f5b3473f8298800a92562048_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73666f81ebea7018f5b3473f8298800a92562048_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/73666f81ebea7018f5b3473f8298800a92562048_2_1380x862.jpeg 2x\" data-dominant-color=\"434345\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-05 at 11.58.56 AM</span><span class=\"informations\">1920\u00d71200 133 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/5/750c517c4761314dea6a27bc0bee0e0bfa0c118a.jpeg\" data-download-href=\"/uploads/short-url/gHsfRClSwjfk6gBpKXASuhhOMsy.jpeg?dl=1\" title=\"Screen Shot 2022-12-05 at 11.59.36 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/5/750c517c4761314dea6a27bc0bee0e0bfa0c118a_2_690x431.jpeg\" alt=\"Screen Shot 2022-12-05 at 11.59.36 AM\" data-base62-sha1=\"gHsfRClSwjfk6gBpKXASuhhOMsy\" width=\"690\" height=\"431\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/5/750c517c4761314dea6a27bc0bee0e0bfa0c118a_2_690x431.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/5/750c517c4761314dea6a27bc0bee0e0bfa0c118a_2_1035x646.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/5/750c517c4761314dea6a27bc0bee0e0bfa0c118a_2_1380x862.jpeg 2x\" data-dominant-color=\"323234\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-05 at 11.59.36 AM</span><span class=\"informations\">1920\u00d71200 117 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a class=\"attachment\" href=\"/uploads/short-url/18LdvPQkRtE8CSpNroDQdL2fNXy.cpproj\">RNAscope_quant_pipeline_alt.cpproj</a> (198.1 KB)</p>", "<p>Hi <a class=\"mention\" href=\"/u/cindyow6\">@cindyow6</a>,</p>\n<p>This is happening because you set your pipeline to group the images without extracting the metadata. To solve the issue you can set \"Do you want to group your images? as No in the Groups module or you can extract the metadata and assign the category in Groups.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/e/2eec7ed82b3f676c8a186283f6b4ef6ab96af528.jpeg\" data-download-href=\"/uploads/short-url/6H6EtGbHvzkrapFQgMgqHooMBZe.jpeg?dl=1\" title=\"Screen Shot 2022-12-05 at 3.44.01 PM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2eec7ed82b3f676c8a186283f6b4ef6ab96af528_2_690x322.jpeg\" alt=\"Screen Shot 2022-12-05 at 3.44.01 PM\" data-base62-sha1=\"6H6EtGbHvzkrapFQgMgqHooMBZe\" width=\"690\" height=\"322\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2eec7ed82b3f676c8a186283f6b4ef6ab96af528_2_690x322.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2eec7ed82b3f676c8a186283f6b4ef6ab96af528_2_1035x483.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/e/2eec7ed82b3f676c8a186283f6b4ef6ab96af528_2_1380x644.jpeg 2x\" data-dominant-color=\"ECEDED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-05 at 3.44.01 PM</span><span class=\"informations\">1680\u00d7784 73 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best,<br>\nMario</p>", "<p>Thank you so much <a class=\"mention\" href=\"/u/mcruz\">@Mcruz</a>! I think I finally understand what\u2019s happening. To clarify, in order for Groups to work, I must be extracting metadata? So when I turn Groups on after successfully assigning NamesAndTypes, NamesAndTypes shuts down because I did not extract metadata in the first place?</p>\n<p>Additional question: can you analysis batches of images without grouping, will the pipeline run analysis for each set of images I have? Thanks!</p>", "<p>Hi <a class=\"mention\" href=\"/u/cindyow6\">@cindyow6</a>,</p>\n<p>to use groups you need some metadata, so I would say yes!</p>\n<p>And the pipeline should also run the analysis for each set of images you have. For example, all files in row 1 (bellow) will be processed as a set of images.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/8/98c35d3a2a1f90257eef10063f699e62bd84305f.png\" data-download-href=\"/uploads/short-url/lNp8LbZNn8036mbN7sxVAStTeq3.png?dl=1\" title=\"Screen Shot 2022-12-09 at 10.22.48 AM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98c35d3a2a1f90257eef10063f699e62bd84305f_2_690x92.png\" alt=\"Screen Shot 2022-12-09 at 10.22.48 AM\" data-base62-sha1=\"lNp8LbZNn8036mbN7sxVAStTeq3\" width=\"690\" height=\"92\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98c35d3a2a1f90257eef10063f699e62bd84305f_2_690x92.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98c35d3a2a1f90257eef10063f699e62bd84305f_2_1035x138.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/9/8/98c35d3a2a1f90257eef10063f699e62bd84305f_2_1380x184.png 2x\" data-dominant-color=\"A3A3A3\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2022-12-09 at 10.22.48 AM</span><span class=\"informations\">1620\u00d7218 73.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Here we have the detailed information for this module (<a href=\"https://cellprofiler-manual.s3.amazonaws.com/CellProfiler-4.2.1/modules/input.html?highlight=groups#module-cellprofiler_core.modules.groups\" class=\"inline-onebox\">Input \u2014 CellProfiler 4.2.1 documentation</a>)</p>\n<p>Thanks for your question.<br>\nMario</p>"], "80827": ["<p>Hello, I\u2019m trying to use cell profiler for various image sets but I have only completed the analysis of one image. I have to use the same protocol for all the images and the problem is that cell profiler never finish to run the analysis, it seems to be like the time restant is the duplicate of the time that i have been passed running the program. In the test mode i don\u2019t obtain any fails but i cant get the excel data file. I have probed in 3 diferent computers, so i dont know what i\u2019m doing wrong. (i supose that it is not a requirement error of the pc) Could you give some advise or solution?<br>\nthank you very much</p>", "<p>Hey Laura,</p>\n<p>have you checked the command line window from CellProfiler (automatically opened with the UI) for some error or other log information that may answer why your pipeline doesn\u2019t finish or where it hangs? This is typically a very valuable source for information if the UI is not providing any error messages. If you cannot figure out the cause on your own, please post this command log together with your pipeline and some sample images to be able to check it.</p>\n<p>Regards, Anna</p>"], "80830": ["<p>I am running cellprofiler on command line through a compute canada node. My imaging files are large, up to 4GB each (27853 x 29696 pixels), with 3 images in each set. I broke them into tiles (~500kb each) to make things more manageable when setting up and testing my pipeline, and have succesfully run my pipeline in command line on compute canada for these small images. Now I need to analyze my full sized images, but when running the same pipeline i get this error about only 2gb of data can be extracted at a time. <div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/3/a36451067d8c2a6b9807b2fb1bf2c584050c084f.png\" data-download-href=\"/uploads/short-url/njqKNt3sySJCuSPFrAtu1pin3Ij.png?dl=1\" title=\"Screenshot 2023-05-05 at 11.46.06 AM\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36451067d8c2a6b9807b2fb1bf2c584050c084f_2_690x17.png\" alt=\"Screenshot 2023-05-05 at 11.46.06 AM\" data-base62-sha1=\"njqKNt3sySJCuSPFrAtu1pin3Ij\" width=\"690\" height=\"17\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36451067d8c2a6b9807b2fb1bf2c584050c084f_2_690x17.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36451067d8c2a6b9807b2fb1bf2c584050c084f_2_1035x25.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/3/a36451067d8c2a6b9807b2fb1bf2c584050c084f_2_1380x34.png 2x\" data-dominant-color=\"E0E0E1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-05-05 at 11.46.06 AM</span><span class=\"informations\">1624\u00d742 45.8 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nHowever I aloted 15gb of memory to the job through compute canada (and i can alot much more if needed). Does this mean cellprofiler can only ever handle 2gb at a time? Which means it cannot analyze my large images? I really want to avoid the step of breaking these images into tiles at it takes up quite some time as I have over 100 images to analyze.<br>\nOr is there something i can change in my cellprofiler pipeline/script to allow it to handle more GB?<br>\nI am very new to command line/computer work so appreciate any help</p>\n<p>attached is my pipeline<br>\n<a class=\"attachment\" href=\"/uploads/short-url/tcHFa85pjlJfUMB3RC1PwOEWi0j.cppipe\">RS_INSGCGcounting.cppipe</a> (27.1 KB)</p>", "<p>This is an issue with the limit of the size of a byte array. Bioformats will also throw this error if trying to extract more than 2g of bytes. You will have to open such a large file in smaller pieces. <a href=\"https://stackoverflow.com/questions/6616739/why-is-the-max-size-of-byte-2-gb-57-b\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">.net - Why is the max size of byte[] 2 GB - 57 B? - Stack Overflow</a></p>"], "78271": ["<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/739ad55b634da7a6da8a91c0aa36df4b518467e0.jpeg\" alt=\"sytox_ImageJ\" data-base62-sha1=\"guGDzbFzBvKcGo0A1bgA6SWpoc0\" width=\"278\" height=\"254\"></p>\n<p>I have stained my tissue with the nuclei dye Sytox green to image cell nuclei (I have also tried DAPI with the same problems as Sytox green). I am trying to measure cell shape (especially interested in circularity and roundness), and because I need it to be fairly exact, drawing the outline around each cell is not possible. I have tried using the measurements plugin in ImageJ, but because the cells are so close together it does not recognize them as different nuclei. I have also tried CellProfiler, but just the example pipes from their website, which has the same problem as ImageJ. I don\u2019t need to be able to recognize every single cell in the image.</p>\n<p>I have tried to find posts on here already that answer this question, but because the cells are so close together, none of them seem exactly applicable. I would really appreciate suggestions of any ways that I might be able to get it to work.</p>\n<p>I have included an image of what I am trying to analyze. Thank you!</p>", "<p><a class=\"mention\" href=\"/u/meghanrb\">@meghanrb</a><br>\n<a href=\"https://imagej.net/imaging/watershed\" rel=\"noopener nofollow ugc\">Watershed seperation</a> looks like it will help with your issue.</p>\n<p>Christian</p>", "<p>If you still have trouble, you may want to try StarDist, it tends to handle fairly circular objects well. Though it can bias towards circularity. CellPose is another option.</p>\n<p>From a quick test on the cellpose website<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d.jpeg\" data-download-href=\"/uploads/short-url/c8lnd9OiGjODcKmTrgE6kcDBFbT.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_690x160.jpeg\" alt=\"image\" data-base62-sha1=\"c8lnd9OiGjODcKmTrgE6kcDBFbT\" width=\"690\" height=\"160\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_690x160.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_1035x240.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/5/550b9203beb5324596ceea8f4d51ecc1b51def4d_2_1380x320.jpeg 2x\" data-dominant-color=\"504C4B\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1920\u00d7447 107 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nIt would be better if you had a monolayer of cells rather than multiple layers.</p>", "<p>In addition, <a href=\"https://github.com/stardist/stardist\">StarDist</a> should do a pretty good job in such a case.</p>\n<p>EDIT: sorry <a class=\"mention\" href=\"/u/research_associate\">@Research_Associate</a>, didn\u2019t see that you had it included already in your post. I just saw the CellPose examples.</p>", "<p>Works really well for most</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1e89ee6375099000cb5546f8de5715c3a1d50580.png\" alt=\"image\" data-base62-sha1=\"4m9Q6kIxoK5RbrHJE5ihWvmXzCo\" width=\"278\" height=\"254\"></p>\n<p>Here the parameter recording for the screenshot you posted. If the original is different, that needs adaption.</p>\n<pre><code class=\"lang-auto\">run(\"Command From Macro\", \"command=[de.csbdresden.stardist.StarDist2D], args=['input':'Clipboard', 'modelChoice':'Versatile (fluorescent nuclei)', 'normalizeInput':'true', 'percentileBottom':'1.0', 'percentileTop':'99.8', 'probThresh':'0.85', 'nmsThresh':'0.1', 'outputType':'ROI Manager', 'nTiles':'1', 'excludeBoundary':'2', 'roiPosition':'Automatic', 'verbose':'false', 'showCsbdeepProgress':'false', 'showProbAndDist':'false'], process=[false]\");\n</code></pre>", "<p>Both CellPose and and Stardist seem to be working fairly well, thank you! StarDist seems a bit easier for me at least because it can be run directly in ImageJ instead of Python (I don\u2019t have as much experience in Python as ImageJ).</p>", "<p>This is really great, thank you!</p>", "<p>Thank you for the suggestion! Watershed was okay for this image, but did not work as well for some of the other images I analyzed.</p>", "<p>Try using the software MIPAR. I suppose it can give you decent results.<br>\nBut it is a proprietary software.</p>"], "74686": ["<p>Mac user<br>\nCell Profiler 4.1.3</p>\n<p>Hi,</p>\n<p>I\u2019m trying to estimate the GFP intensity in the nuclei.<br>\nI identified the nuclei using the DAPI channel and created a mask and applied it to the GFP channel image. slide21 - is a negative control with minimal GFP signal and slide 22 is a positive control with strong GFP signal. However, when I run analysis using MeasureObjectIntensity, the integrated intensity values for GFP are higher in the negative control image than in the positive control.<br>\nHow do I fix this issue? I have attached the 2 images and my pipeline.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/wUd8Rkl1gxVn3SxZn1SeWViGSHA.cpproj\">Intensity.cpproj</a> (110.0 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/1ldDwBRkgtU9mAemIYuAUyvvCWn.tif\">slide21-01-Orthogonal Projection-04-Image Export-09_c1.tif</a> (716.2 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/sqVfG7Yc1tBNyRQGnLeQiAinJhw.tif\">slide21-01-Orthogonal Projection-04-Image Export-09_c2.tif</a> (488.3 KB)</p>\n<p><a class=\"attachment\" href=\"/uploads/short-url/udr1AlVj2jbTsCbsF3div8cpogG.tif\">slide22-01-Orthogonal Projection-03-Image Export-10_c1.tif</a> (633.1 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/4Y6snWRLuwfjedg233aSXAlbnLQ.tif\">slide22-01-Orthogonal Projection-03-Image Export-10_c2.tif</a> (343.7 KB)</p>", "<p>Hi Divya,</p>\n<p>So it looks to me like the analysis isn\u2019t doing anything wrong - the issue is that the image intensity <em>really is</em> as high, or almost as high, in the negative as the positive controls! It\u2019s a bit of a trick of the eye I think, partly caused by the background being so much lower in the positive control.  e.g. opening the images here in FIJI, and setting the max display values the same, you can measure some objects to check:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d91b012035a29f21c88738187a5df4ef7e47a96e.png\" data-download-href=\"/uploads/short-url/uYBt00bXDVZuWA4lhtFhUXAATT0.png?dl=1\" title=\"Screenshot 2022-12-06 105037\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d91b012035a29f21c88738187a5df4ef7e47a96e_2_517x218.png\" alt=\"Screenshot 2022-12-06 105037\" data-base62-sha1=\"uYBt00bXDVZuWA4lhtFhUXAATT0\" width=\"517\" height=\"218\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d91b012035a29f21c88738187a5df4ef7e47a96e_2_517x218.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d91b012035a29f21c88738187a5df4ef7e47a96e_2_775x327.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d91b012035a29f21c88738187a5df4ef7e47a96e_2_1034x436.png 2x\" data-dominant-color=\"35573E\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2022-12-06 105037</span><span class=\"informations\">1424\u00d7602 239 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>\u2026and see that the dimmer nuclei in the positive control really do have only about half the intensity of the negative controls.<br>\nDid you (or whoever acquired these images) ensure that the same microscope settings were used when acquiring them all?</p>\n<hr>\n<p>Also, a couple of other things that might help you:</p>\n<p>The regex in the Metadata block didn\u2019t seem right, so I used this instead, to extract slide number, the rest of the text as \u2018ImageInfo\u2019, and the channel number from the filename:</p>\n<p>^slide(?P[0-9]{2})-(?P.*)_c(?P[0-9]{1}).tif</p>\n<p>You might want to change that if you need further info extracted from the filenames.</p>\n<p>In the IdentifyPrimaryObjects block your settings seemed unusual, with a very low threshold correction factor - I used something more like this instead:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7b95cca185a8f98f589c1b4821ca2d5ac427abe2.png\" data-download-href=\"/uploads/short-url/hDhFGYtTOiXVMRsD899ob0b1tt0.png?dl=1\" title=\"Screenshot 2022-12-06 102058\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/b/7b95cca185a8f98f589c1b4821ca2d5ac427abe2.png\" alt=\"Screenshot 2022-12-06 102058\" data-base62-sha1=\"hDhFGYtTOiXVMRsD899ob0b1tt0\" width=\"345\" height=\"186\" data-dominant-color=\"EAEAEA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2022-12-06 102058</span><span class=\"informations\">721\u00d7390 8.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>I don\u2019t think you actually need the masks block at all, if I understand correctly what you\u2019re trying to do. You just choose to measure the GFP channel in the nuclei objects, and the results are the same.</p>\n<p>Try this edited version if you like:<br>\n<a class=\"attachment\" href=\"/uploads/short-url/fKnbnJLrk2yRCgfjcle4Y3xKLUn.cpproj\">Intensity_edited.cpproj</a> (632.5 KB)</p>", "<p>\u2026sorry for the mangled regular expression above, I meant this:</p>\n<pre><code class=\"lang-auto\">^slide(?P&lt;SlideID&gt;[0-9]{2})-(?P&lt;ImageInfo&gt;.*)_c(?P&lt;ChannelNumber&gt;[0-9]{1}).tif\n</code></pre>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f7c2b6e8851600f22158f3a2089b49a1bd18beb3.png\" data-download-href=\"/uploads/short-url/zlN6LexybeAn4g5H2RDTVNfT1o7.png?dl=1\" title=\"Screenshot 2022-12-06 110020\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/7/f7c2b6e8851600f22158f3a2089b49a1bd18beb3.png\" alt=\"Screenshot 2022-12-06 110020\" data-base62-sha1=\"zlN6LexybeAn4g5H2RDTVNfT1o7\" width=\"517\" height=\"130\" data-dominant-color=\"ECEEED\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2022-12-06 110020</span><span class=\"informations\">736\u00d7186 6.61 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Huw,</p>\n<p>Sorry I didn\u2019t get a chance to respond sooner. Thank you so much for your input.<br>\nI did use the same setting to acquire all images. I believe in the GFP negative slide there is just noise or auto-fluorescence issues (not sure how best to fix it). I completely agree with you. I don\u2019t need to mask the GFP image. I can just measure GFP intensity in the nuclei region.</p>"], "79299": ["<h3>\n<a name=\"sample-image-andor-code-1\" class=\"anchor\" href=\"#sample-image-andor-code-1\"></a>Sample image and/or code</h3>\n<ul>\n<li><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1e2afee687e67285b917d9f036dd573aa5b974de.jpeg\" data-download-href=\"/uploads/short-url/4iSroZQekQc3MP0nlwARinv4T8y.jpeg?dl=1\" title=\"Example1_EdgeEnhancement\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/1/e/1e2afee687e67285b917d9f036dd573aa5b974de.jpeg\" alt=\"Example1_EdgeEnhancement\" data-base62-sha1=\"4iSroZQekQc3MP0nlwARinv4T8y\" width=\"690\" height=\"357\" data-dominant-color=\"3A3A3A\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Example1_EdgeEnhancement</span><span class=\"informations\">1920\u00d7995 62.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></li>\n<li>\n<a class=\"attachment\" href=\"/uploads/short-url/8OCa4oHTVuLnLjqq8885ocXFcGH.zip\">Sample Images.zip</a> (7.8 MB)</li>\n<li>\n<a class=\"attachment\" href=\"/uploads/short-url/eLf5AOdVoJl7vt5fhW4SiiTUU8I.cpproj\">Plate_1_ImageAnalysisPipeline2.cpproj</a> (110.8 KB)</li>\n</ul>\n\n<h3>\n<a name=\"backgroundanalysis-goals-2\" class=\"anchor\" href=\"#backgroundanalysis-goals-2\"></a>Background/Analysis Goals</h3>\n<ul>\n<li>I would like to design a pipeline to count the number of foci within each cell in an image (they are the faintly bright spots towards the center of the cell in the example image I provided on the right).</li>\n</ul>\n<h3>\n<a name=\"challenges-3\" class=\"anchor\" href=\"#challenges-3\"></a>Challenges</h3>\n<ul>\n<li>I haven\u2019t been able to find a way to effectively identify the individual foci within the cell. At the moment, I am using speckle enhancement on a masked image to facilitate better object identification, but it appears that the edges of my cells are being enhanced as well. Is there any way to avoid this, as I believe it may be impacting the downstream image thresholding?</li>\n</ul>", "<p>Dear <a class=\"mention\" href=\"/u/jsh\">@jsh</a>,</p>\n<p>Looking at the images, it looks like your foci are just one pixel, meaning you might lack the resolution to actually enhance them\u2026 And indeed the edges of the cells also present an intensity gradient, which get enhanced by such filters.</p>\n<p>Barring higher resolution data, you could first use \u201cEnlarge or Shrink objects\u201d by a few pixels to exclude the edge of the cells before trying to find your speckles.</p>\n<p>But again, I don\u2019t see speckles, I see single bright pixels that make me think you might not have enough resolution to really locate them.</p>\n<p>Best</p>\n<p>Oli</p>", "<p>Dear Oli,</p>\n<p>Thank you so much for the guidance! I think you are right and I will need to take higher-resolution images.</p>"], "74179": ["<p>Hi,<br>\nI am doing a tracking analysis on a cluster and am running CP headless. The images are multitif files with 400 frames. When I run the analysis from the GUI all images are analysed. On the cluster, CP only analyses the first frame and none of the others. Looking at the cpppipe file generated by CP, I see that the extraction method from the file headers is set, but this seems then to be ignored. Is there some other flag to set on the command line for multitif images?<br>\nThanks,<br>\nMarc</p>"], "71621": ["<p>Hi <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a>,</p>\n<p>we are installing CellProfiler like this:</p>\n<pre><code class=\"lang-auto\">source /opt/conda/etc/profile.d/conda.sh &amp;&amp; \\\n  conda create --yes -c bioconda -c conda-forge --name cellprofiler-4.2.1 python=3.8 &amp;&amp; \\\n  conda activate cellprofiler-4.2.1 &amp;&amp; \\\n  conda install \"cellprofiler=4.2.1\" -c bioconda -c conda-forge &amp;&amp; \\\n</code></pre>\n<p>And we are getting this error:</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f2c007cb4c5dee70fb53ec96a5fa1337c283d6b.jpeg\" data-download-href=\"/uploads/short-url/6JiIjvHytKtJn6KSwLkmQI36zdV.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f2c007cb4c5dee70fb53ec96a5fa1337c283d6b_2_690x327.jpeg\" alt=\"image\" data-base62-sha1=\"6JiIjvHytKtJn6KSwLkmQI36zdV\" width=\"690\" height=\"327\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f2c007cb4c5dee70fb53ec96a5fa1337c283d6b_2_690x327.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f2c007cb4c5dee70fb53ec96a5fa1337c283d6b_2_1035x490.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/2/f/2f2c007cb4c5dee70fb53ec96a5fa1337c283d6b.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/2/f/2f2c007cb4c5dee70fb53ec96a5fa1337c283d6b_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1332\u00d7632 102 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Do you have an idea how to fix this?</p>\n<p>Are we maybe missing some h5 dependency?</p>", "<p>Hmm, yes, it definitely seems like it\u2019s either missing h5py or its the wrong version. Can you <code>pip freeze</code> inside that environment to see what the package versions are?</p>", "<pre><code class=\"lang-auto\">(cellprofiler-4.2.1) tischer@jupyter-tischer:/home/tischer$ pip freeze\nboto3 @ file:///home/conda/feedstock_root/build_artifacts/boto3_1662601700173/work\nbotocore @ file:///home/conda/feedstock_root/build_artifacts/botocore_1662598303806/work\nbrotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1648854175163/work\ncached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work\nCellProfiler @ file:///opt/conda/conda-bld/cellprofiler_1645568945790/work\ncellprofiler-core @ file:///opt/conda/conda-bld/cellprofiler-core_1626963621474/work\ncentrosome @ file:///opt/conda/conda-bld/centrosome_1659109167139/work\ncertifi==2022.6.15\ncffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1656782821535/work\ncharset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work\ncloudpickle @ file:///home/conda/feedstock_root/build_artifacts/cloudpickle_1662587369221/work\ncryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1657174007680/work\ncycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1635519461629/work\ncytoolz @ file:///home/conda/feedstock_root/build_artifacts/cytoolz_1657553437789/work\ndask @ file:///home/conda/feedstock_root/build_artifacts/dask-core_1662155269419/work\ndeprecation @ file:///home/conda/feedstock_root/build_artifacts/deprecation_1589881437857/work\ndocutils @ file:///home/conda/feedstock_root/build_artifacts/docutils_1657104281902/work\nfonttools @ file:///home/conda/feedstock_root/build_artifacts/fonttools_1661381335192/work\nfsspec @ file:///home/conda/feedstock_root/build_artifacts/fsspec_1662037448631/work\nfuture @ file:///home/conda/feedstock_root/build_artifacts/future_1649010148304/work\nh5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1624405622079/work\nidna @ file:///home/conda/feedstock_root/build_artifacts/idna_1642433548627/work\nimagecodecs @ file:///home/conda/feedstock_root/build_artifacts/imagecodecs_1633273363659/work\nimageio @ file:///home/conda/feedstock_root/build_artifacts/imageio_1661789389824/work\nimportlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1653252793585/work\ninflect @ file:///home/conda/feedstock_root/build_artifacts/inflect_1659257797164/work\nJinja2 @ file:///home/conda/feedstock_root/build_artifacts/jinja2_1654302431367/work\njmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1655568249366/work\njoblib @ file:///home/conda/feedstock_root/build_artifacts/joblib_1633637554808/work\nkiwisolver @ file:///home/conda/feedstock_root/build_artifacts/kiwisolver_1657953078266/work\nlocket @ file:///home/conda/feedstock_root/build_artifacts/locket_1650660393415/work\nmahotas @ file:///home/conda/feedstock_root/build_artifacts/mahotas_1658970320967/work\nMarkupSafe @ file:///home/conda/feedstock_root/build_artifacts/markupsafe_1648737563195/work\nmatplotlib @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-suite_1661439843860/work\nmysqlclient @ file:///home/conda/feedstock_root/build_artifacts/mysqlclient_1655713813220/work\nnetworkx @ file:///home/conda/feedstock_root/build_artifacts/networkx_1661192904448/work\nnumpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1612116908533/work\npackaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1637239678211/work\npartd @ file:///home/conda/feedstock_root/build_artifacts/partd_1660316728562/work\npathlib2 @ file:///home/conda/feedstock_root/build_artifacts/pathlib2_1649684025223/work\nPillow @ file:///home/conda/feedstock_root/build_artifacts/pillow_1653922726761/work\nprokaryote==2.4.4\npsutil @ file:///home/conda/feedstock_root/build_artifacts/psutil_1662356143277/work\npycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work\npydantic @ file:///home/conda/feedstock_root/build_artifacts/pydantic_1662443078596/work\npyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1643496850550/work\npyparsing @ file:///home/conda/feedstock_root/build_artifacts/pyparsing_1652235407899/work\nPypubsub==4.0.3\nPySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1661604839144/work\npython-bioformats @ file:///opt/conda/conda-bld/python-bioformats_1624335447152/work\npython-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work\npython-javabridge @ file:///home/conda/feedstock_root/build_artifacts/python-javabridge_1603497750589/work\nPyWavelets @ file:///home/conda/feedstock_root/build_artifacts/pywavelets_1649616412805/work\nPyYAML @ file:///home/conda/feedstock_root/build_artifacts/pyyaml_1648757091578/work\npyzmq==18.1.1\nrequests @ file:///home/conda/feedstock_root/build_artifacts/requests_1661872987712/work\ns3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1654039987929/work\nscikit-image @ file:///home/conda/feedstock_root/build_artifacts/scikit-image_1660146492852/work\nscikit-learn @ file:///home/conda/feedstock_root/build_artifacts/scikit-learn_1659726091027/work\nscipy==1.9.1\nsentry-sdk @ file:///home/conda/feedstock_root/build_artifacts/sentry-sdk_1662396194958/work\nsix @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\nthreadpoolctl @ file:///home/conda/feedstock_root/build_artifacts/threadpoolctl_1643647933166/work\ntifffile @ file:///home/conda/feedstock_root/build_artifacts/tifffile_1635944860688/work\ntoolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work\ntyping_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1656706066251/work\nunicodedata2 @ file:///home/conda/feedstock_root/build_artifacts/unicodedata2_1649111919534/work\nurllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1658789158161/work\nwxPython==4.1.1\nzipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1659400682470/work\n</code></pre>", "<p>This looks a bit funny\u2026am I maybe missing some parameters to <code>pip freeze</code>?</p>", "<p>Nope, it just means those all came via conda.</p>\n<p>Can you check it instead with the below? But briefly, what I\u2019m nearly certain of is that it is that your h5py version is too new. If you roll it back to something between 3.0.0 and 3.2.1, does it fix the issue?</p>\n<pre><code class=\"lang-python\">import h5py\nh5py.__version__\n</code></pre>", "<p>Thank you! I am getting <code>3.3.0</code> as a version for h5py. Is that too new?</p>\n<p>If so, do we know who maintains?<br>\n<code>conda install \"cellprofiler=4.2.1\" -c bioconda -c conda-forge</code><br>\nBecause then maybe this needs a fix?</p>\n<p>ping <a class=\"mention\" href=\"/u/jkh1\">@jkh1</a></p>", "<p>It might be - if you in that environment <code>pip install h5py==3.2.1</code>, does it fix your issue? (It\u2019s entirely possible we\u2019re on the wrong track here, a lot of things MIGHT lead to an h5py error since it\u2019s a pretty base-level package).</p>\n<p>If that does fix it, we don\u2019t maintain the conda version - it appears to source from <a href=\"https://github.com/bioconda/bioconda-recipes/blob/6d8bb678cea1492ec25148fcbea3bb20c263d44c/recipes/cellprofiler/meta.yaml\">here</a>.</p>", "<p>Hi, yes that seems to work, at least I can now open an image and crop it.<br>\nI will test some more and get back to you\u2026</p>", "<p><a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> meanwhile, the below code is working within a Dockerfile:</p>\n<pre><code class=\"lang-auto\"># install cellprofiler\n# \nRUN DEBIAN_FRONTEND=noninteractive apt-get install -y python3-pip default-libmysqlclient-dev libnotify-dev libsdl2-dev\nENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\nWORKDIR /tmp\nRUN wget https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-20.04/wxPython-4.1.0-cp38-cp38-linux_x86_64.whl\n\nRUN source /opt/conda/etc/profile.d/conda.sh &amp;&amp; \\\n  conda create --yes -c bioconda -c conda-forge --name cellprofiler-4.2.1 python=3.8 &amp;&amp; \\\n  /opt/conda/envs/cellprofiler-4.2.1/bin/pip3 install wxPython-4.1.0-cp38-cp38-linux_x86_64.whl\n\nWORKDIR /opt\nRUN git clone --branch v4.2.1 https://github.com/CellProfiler/CellProfiler.git\nWORKDIR /opt/CellProfiler\nRUN /opt/conda/envs/cellprofiler-4.2.1/bin/pip3 install .\nRUN echo $'#!/usr/bin/env -S bash --noprofile --norc\\nsource /opt/conda/etc/profile.d/conda.sh\\nconda activate cellprofiler-4.2.1\\ncellprofiler $@' &gt; /usr/local/bin/cellprofiler-4.2.1\nRUN chmod +x /usr/local/bin/cellprofiler-4.2.1\n</code></pre>\n<p>I am now wondering how to also add the ilastix plugin to this installation recipe. Can you help there?</p>\n<p>I am thinking maybe simply adding:</p>\n<pre><code class=\"lang-auto\">RUN git clone https://github.com/CellProfiler/CellProfiler-plugins.git\nRUN cd CellProfiler-plugins.git\nRUN /opt/conda/envs/cellprofiler-4.2.1/bin/pip3 install -r requirements.txt\n</code></pre>\n<p>And can I then add to the cellprofiler command line call</p>\n<p><code>cellprofiler --plugins-directory opt/CellProfiler-plugins</code></p>\n<p>And it would open the GUI with the plugins?</p>", "<p>I wouldn\u2019t rely on the requirements.txt files, at least not for right now: they\u2019ve historically been poorly updated, and we\u2019re working now actually moving to per-plugin requirements files. A couple of postdocs on my team have on their plate for the next couple of months to get the plugin repo in shape for the first time in a while (sadly), so stay tuned!</p>\n<p>The ilastik plugin shouldn\u2019t require any external python dependencies, so as long as your container has an ilastik executable (which presumably you can just <code>wget</code>), you should be good to just clone the plugins repo and then run. If you\u2019re doing this in a container, adding the plugins directory to the start call isn\u2019t a bad idea, but the plugins directory can also be set in the preferences if the idea is that wherever you\u2019re running will persist for a bit.</p>\n<p>(For future reference/future readers, we do have an official CellProfiler <a href=\"https://github.com/CellProfiler/distribution/blob/b1b653ae85c0fb5b04030627614e2228c8152936/docker/Dockerfile\">dockerfile</a> and all builds since 2.3 (which is just 2.2 with a couple bugfixes) <a href=\"https://hub.docker.com/r/cellprofiler/cellprofiler\">are on Dockerhub</a> if you wan to <code>from</code> them. 2.3-4.2.1 are on Ubuntu 16, 4.2.4 is Ubuntu 18. Absolutely nothing wrong with rolling your own if you want to, but we hope these help!)</p>", "<p><a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> thank you very much for all the useful information!</p>\n<p>I can now confirm that this works:</p>\n<pre><code class=\"lang-auto\">WORKDIR /opt\nRUN source /opt/conda/etc/profile.d/conda.sh &amp;&amp; \\\n  conda create --yes -c bioconda -c conda-forge --name cellprofiler-4.2.1 python=3.8 &amp;&amp; \\\n  conda activate cellprofiler-4.2.1 &amp;&amp; \\\n  conda install \"cellprofiler=4.2.1\" -c bioconda -c conda-forge &amp;&amp; \\\n  conda run --name cellprofiler-4.2.1 python -m pip install h5py==3.2.1\n</code></pre>\n<p>, where the last line down-grades the h5 dependency.</p>", "<p><a class=\"mention\" href=\"/u/bcimini\">@bcimini</a></p>\n<p>We have now two working recipes for installing Cellprofiler (See below).</p>\n<p>I would like to settle on one of them.</p>\n<p>Could you give some advice on which option you\u2019d recommend?</p>\n<h3>\n<a name=\"option-1-1\" class=\"anchor\" href=\"#option-1-1\"></a>Option 1</h3>\n<pre><code class=\"lang-auto\">ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n\nWORKDIR /tmp\nRUN wget https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ubuntu-20.04/wxPython-4.1.0-cp38-cp38-linux_x86_64.whl\n\nRUN source /opt/conda/etc/profile.d/conda.sh &amp;&amp; \\\n  conda create --yes -c bioconda -c conda-forge --name cellprofiler-4.2.1 python=3.8 &amp;&amp; \\\n  /opt/conda/envs/cellprofiler-4.2.1/bin/pip3 install wxPython-4.1.0-cp38-cp38-linux_x86_64.whl\n\nWORKDIR /opt\nRUN git clone --branch v4.2.1 https://github.com/CellProfiler/CellProfiler.git\nWORKDIR /opt/CellProfiler\nRUN /opt/conda/envs/cellprofiler-4.2.1/bin/pip3 install .\nRUN echo $'#!/usr/bin/env -S bash --noprofile --norc\\nsource /opt/conda/etc/profile.d/conda.sh\\nconda activate cellprofiler-4.2.1\\ncellprofiler $@' &gt; /usr/local/bin/cellprofiler-4.2.1\nRUN chmod +x /usr/local/bin/cellprofiler-4.2.1\n</code></pre>\n<h3>\n<a name=\"option-2-2\" class=\"anchor\" href=\"#option-2-2\"></a>Option 2</h3>\n<pre><code class=\"lang-auto\">WORKDIR /opt\nRUN source /opt/conda/etc/profile.d/conda.sh &amp;&amp; \\\n  conda create --yes -c bioconda -c conda-forge --name cellprofiler-4.2.1 python=3.8 &amp;&amp; \\\n  conda activate cellprofiler-4.2.1 &amp;&amp; \\\n  conda install \"cellprofiler=4.2.1\" -c bioconda -c conda-forge &amp;&amp; \\\n  conda run --name cellprofiler-4.2.1 python -m pip install h5py==3.2.1 &amp;&amp; \\\n  conda clean -tipy &amp;&amp; \\\n  echo $'#!/usr/bin/env -S bash --noprofile --norc\\nsource /opt/conda/etc/profile.d/conda.sh\\nconda activate cellprofiler-4.2.1\\ncellprofiler $@' &gt; /usr/local/bin/cellprofiler-4.2.1 &amp;&amp; \\\n  chmod +x /usr/local/bin/cellprofiler-4.2.1\n</code></pre>", "<p>The second seems simpler to maintain to me, but if they both work and have comparable creation times, really it\u2019s a matter of personal preference at this point. Whichever is good!</p>", "<p><a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> Could you please do us a favour and test <code>Option 2</code> and measure how long it takes? For us it is more than 1 hour\u2026</p>", "<p>Hi <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a>,</p>\n<p><a class=\"mention\" href=\"/u/yi_sun\">@Yi_Sun</a> and I did some more tests and it looks like as if option 2 is very slow (almost unusable). Can you confirm this?</p>\n<p>Regarding Java:</p>\n<ul>\n<li>Do you know whether option 2 ships its own java? If so, which version?</li>\n<li>I guess option 1 does not ship java?</li>\n<li>Do you know which version of Java we need? Does it have to be 8 or could it be something newer?</li>\n</ul>", "<p><a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> <a class=\"mention\" href=\"/u/yi_sun\">@Yi_Sun</a> another question. Option 1 now seems to result in such errors:</p>\n<pre><code class=\"lang-auto\">hayashi@jupyter-hayashi:/home/hayashi$ cellprofiler-4.2.1\n07:50:25 AM: Debug: Adding duplicate image handler for 'Windows bitmap file'\n07:50:25 AM: Debug: Adding duplicate animation handler for '1' type\n07:50:25 AM: Debug: Adding duplicate animation handler for '2' type\n/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/centrosome/filter.py:15: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n  from . import _filter\nCould not load cellprofiler_core.modules.align\nTraceback (most recent call last):\n  File \"/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/cellprofiler_core/utilities/core/modules/__init__.py\", line 71, in add_module\n    m = __import__(mod, globals(), locals(), [\"__all__\"], 0)\n  File \"/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/cellprofiler_core/modules/align.py\", line 54, in &lt;module&gt;\n    from centrosome.filter import stretch\n  File \"/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/centrosome/filter.py\", line 15, in &lt;module&gt;\n    from . import _filter\n  File \"centrosome/_filter.pyx\", line 45, in init centrosome._filter\n  File \"/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/numpy/__init__.py\", line 284, in __getattr__\n    raise AttributeError(\"module {!r} has no attribute \"\nAttributeError: module 'numpy' has no attribute 'bool'\n/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/centrosome/filter.py:15: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\n  from . import _filter\nCould not load cellprofiler.modules.correctilluminationcalculate\nTraceback (most recent call last):\n  File \"/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/cellprofiler_core/utilities/core/modules/__init__.py\", line 71, in add_module\n    m = __import__(mod, globals(), locals(), [\"__all__\"], 0)\n  File \"/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/cellprofiler/modules/correctilluminationcalculate.py\", line 49, in &lt;module&gt;\n    import centrosome.filter\n  File \"/opt/conda/envs/cellprofiler-4.2.1/lib/python3.8/site-packages/centrosome/filter.py\", line 15, in &lt;module&gt;\n</code></pre>\n<p>Any idea where that could come from?</p>", "<p>I think this may be because the numpy version is too new\u2026<br>\ncellprofiler needs numpy&gt;=1.20.1, but maybe need to restrict it to &gt;=1.20.1 and &lt; 1.24 \u2026<br>\nI will test it</p>", "<p>an upgrade to cellprofiler 4.2.5 solved the problem.</p>\n<p>however, I still don\u2019t know why it didn\u2019t work for v4.2.1. Downgrade numpy did not make v4.2.1 work. The app starts, but some modules don\u2019t load.</p>", "<p>I unfortunately am traveling and likely couldn\u2019t debug these directly, but without knowing what source Docker you\u2019re starting from, I can\u2019t know whether or not it has Java pre-installed, and it will be hard to test without knowing which image to start from. Sorry!</p>\n<p>I still would recommend our Dockers (either the Dockers themselves or the recipes), rather than trying to make new ones from scratch- you can always <code>FROM</code> our Dockers and add any extra packages you want.</p>\n<p>And yes, the numpy issue sounds likely to just need a pin to earlier, we don\u2019t generally provide a \u201cmax version\u201d in our pins for most of our packages.</p>", "<p>Thanks a lot <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> .</p>\n<p>For reference I am posting again the \u201cofficial\u201d <a href=\"https://github.com/CellProfiler/distribution/blob/b1b653ae85c0fb5b04030627614e2228c8152936/docker/Dockerfile\">CellProfiler Dockerfile here</a>.</p>"], "74182": ["<p><strong>TL;DR:</strong> We are looking for a Bioimage Analyst and/or RSE to join our growing image analysis facility here at <a href=\"https://humantechnopole.it\">Human Technopole</a> in the beautiful city of Milan, Italy. Make yourself a nice Christmas present and apply by December 16. All the glorious details can be found below, but we\u2019re happy if you <a href=\"mailto:iaf@fht.org\">drop us a line</a> with your questions. <a href=\"https://careers.humantechnopole.it/o/bioimage-analyst-and-research-software-engineer-computational-biology-research-center\">CLICK HERE TO APPLY</a></p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/f/afef5a90966fc3ea3f494f1f64ea149ce5561395.jpeg\" data-download-href=\"/uploads/short-url/p6omXRNjmfRkcYSfu57lKNbXeUl.jpeg?dl=1\" title=\"IMG_4653\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/afef5a90966fc3ea3f494f1f64ea149ce5561395_2_499x375.jpeg\" alt=\"IMG_4653\" data-base62-sha1=\"p6omXRNjmfRkcYSfu57lKNbXeUl\" width=\"499\" height=\"375\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/afef5a90966fc3ea3f494f1f64ea149ce5561395_2_499x375.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/afef5a90966fc3ea3f494f1f64ea149ce5561395_2_748x562.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/f/afef5a90966fc3ea3f494f1f64ea149ce5561395_2_998x750.jpeg 2x\" data-dominant-color=\"9C8F7D\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">IMG_4653</span><span class=\"informations\">1920\u00d71440 238 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<em><strong>Figure 1:</strong></em> The current staff of our facility. But three is at least one too little. Hence, the facility needs you!</p>\n<h1>\n<a name=\"job-description-1\" class=\"anchor\" href=\"#job-description-1\"></a>Job description</h1>\n<p><strong>APPLICATION CLOSING DATE: December 16th</strong> <strong>, 2022</strong><br>\n<a href=\"https://careers.humantechnopole.it/o/bioimage-analyst-and-research-software-engineer-computational-biology-research-center\">CLICK HERE TO APPLY</a></p>\n<p>Human Technopole (HT) is a new interdisciplinary life science research institute, created and supported by the Italian Government, with the aim of developing innovative strategies to improve human health. HT is composed of five Centers: Computational Biology, Structural Biology, Genomics, Neurogenomics and Health Data Science. The Centers work together to enable interdisciplinary research and to create an open, collaborative environment that will help promote life science research both nationally and internationally.</p>\n<p>The Image Analysis Facility at Human Technopole is recruiting a <strong>Bioimage Analyst and Research Software Engineer</strong> to add to our creative and interdisciplinary research team at the Human Technopole (HT) in Milan, Italy.</p>\n<p>The facility will be offering bleeding-edge image analysis services to researchers within HT but is equally dedicated to supporting the wider science landscape in Italy and beyond. Our service portfolio includes technical consultation with users, building and deploying suitable analysis pipelines, quality control, publishing support, visualization and rendering services, as well as the development of new open-source tools. As well as image analysis and software development, we also envision a strong focus on teaching: we aim to enable users to perform an increasing amount of their required analyses by themselves.</p>\n<p>The work in the image analysis facility will be tightly coupled to the research conducted in the <strong>Jug Group</strong> at HT. The Jug Lab is developing novel computer vision and machine learning methods and algorithms for a broad array of image-based biomedical research. We seek an open-minded and motivated personality to help us further our service portfolio and software landscape for bio-image analysis. The offered job naturally comes with the opportunity to visit other members of the community of Bioimage Analysts around the world and will bring you in touch with the latest algorithms, software tools, but also microscopy hardware and technology. The position is full time and based in Milan, Italy. HT offers attractive conditions and benefits appropriate to a leading, internationally competitive, research organization that promotes a collegial and open atmosphere.</p>\n<p><strong>Key tasks and responsibilities</strong></p>\n<ul>\n<li>Consult with internal and external researchers to provide advice on bioimage analysis.</li>\n<li>Design and execute bioimage analysis projects in collaboration with internal and and external researchers.</li>\n<li>Contribute to planning and teaching of courses run by the Image Analysis Facility.</li>\n<li>Stay up to date on the latest bioimage analysis tools and methodologies.</li>\n<li>Participate in the scientific life of the Jug lab.</li>\n</ul>\n<h3>\n<a name=\"job-requirements-2\" class=\"anchor\" href=\"#job-requirements-2\"></a>Job requirements</h3>\n<ul>\n<li>MSc or higher in Biology, Bioengineering, Computer Science, Bioinformatics, or a related field.</li>\n<li>A strong background in image processing and image analysis.</li>\n<li>English proficiency.</li>\n</ul>\n<p><strong>Advantageous qualities</strong></p>\n<ul>\n<li>PhD in Biology, Bioengineering, Computer Science, Bioinformatics, or a related field, or equivalent industry experience (3+ years).</li>\n<li>Background in microscopy or optics.</li>\n<li>Experience with the application of image processing methods on biological datasets, with an emphasis on microscopy.</li>\n<li>Proficiency in at least one programming language (Python, Java, Javascript, ImageJ macro, R, Matlab).</li>\n<li>Familiarity with at least one established image analysis software (eg. Fiji, ilastik, Cell Profiler, napari, Imaris, Arivis, or a similar tool).</li>\n<li>Knowledge of deep-learning methods.</li>\n<li>Keen interest in finding suitable solutions for our users while working in a young and friendly team.</li>\n</ul>\n<p>Special consideration will be given to candidates who are part of the protected categories list, according to L. 68/99</p>\n<p><strong>Application</strong> <strong>instructions</strong><br>\nPlease apply by sending a CV and a up to 2 pages of detailed motivation letter in English only through the dedicated area below (Apply now) and the name and contact details of at least 2 potential referees. <strong>Importantly, international candidates coming to Italy for the first time, or Italian nationals returning after residing abroad, benefit from very attractive income tax benefits.</strong></p>\n<p><strong>Additional information:</strong><br>\nHT offers a highly collaborative, international culture to foster top quality, interdisciplinary research by promoting a vibrant environment consisting of independent research groups with access to outstanding graduate students, postdoctoral fellows and core facilities.</p>\n<p>HT is an inclusive employer that fosters diversity and engages systematically to ensure that equal employment opportunities are provided without regard to age, race, creed, religion, sex, disability, medical condition, sexual orientation, gender identity or expression, national or ethnic origin or any other legally recognized status entitled to protection under applicable laws. HT offers attractive conditions and benefits appropriate to a leading, internationally competitive, research organization that promotes a collegial and open atmosphere.</p>\n<p><a href=\"https://careers.humantechnopole.it/o/bioimage-analyst-and-research-software-engineer-computational-biology-research-center\">CLICK HERE TO APPLY</a></p>"], "75208": ["<p>Hello, I have a large number of greyscale microscopic image sets (two channels). The images stem from different regions of a slide and each region consists of multiple tiles. At each tile position, z-stacks with 64 images were recorded. I generated image groups for the regions / tiles showing up the correct number of images per z-stack. Now, I would like to save Tiff files with z-stacks per tile position and region (16-bit greyscale) of unprocessed images. Thus, all images of a group need to be added to a stack and this is then saved; GrayToColor does not work in this case. Is there a way to perform this action from within CellProfiler / perhaps with an ImageJ script ran from within CellProfiler? Thank you very much for your help! Best, Mathias</p>", "<p>Hi Mathias - What you\u2019re describing is exactly what\u2019s done in Example 1 in <a href=\"https://forum.image.sc/t/input-modules-tutorial/63911#h-3d-processing-4814-6\">this section of this blog post</a> - it also links out to a video version. I hope that helps!</p>"], "78795": ["<p>Hi, I am new to Cell Profiler and am hoping to analyze confocal zstacks.</p>\n<p>I exported my LEICA image as individual .tifs, but when I double click any image, I get a pipeline error (see below). Tutorial module images work fine and I added imagecodecs to PATH, but this image is still showing up. A similar error happens if I try with jpgs.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c64cabf9ab92a718242452748c13bba8cbacafb2.png\" data-download-href=\"/uploads/short-url/sieO4HWlSchcMrzmIvCD3Tlu9MK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/6/c64cabf9ab92a718242452748c13bba8cbacafb2.png\" alt=\"image\" data-base62-sha1=\"sieO4HWlSchcMrzmIvCD3Tlu9MK\" width=\"690\" height=\"362\" data-dominant-color=\"DBDBDB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1332\u00d7700 69.6 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Marie,</p>\n<p>Could you upload one image, please?  It\u2019s difficult to help you without.</p>\n<p>M.</p>", "<p>Here are two images! I am hoping to segment the plaques from channels 00 and 01.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/bDndgZLusikoW0DSqdQZzZkPcvS.tif\">cell profiler test_2021111 CC_AD_089 PBB5 Iba1 a _z06_ch00.tif</a> (92.9 KB)<br>\n<a class=\"attachment\" href=\"/uploads/short-url/gLWUJq0BbfGEaWqkKnugmDlxtwP.tif\">cell profiler test_2021111 CC_AD_089 PBB5 Iba1 a _z06_ch01.tif</a> (81.2 KB)</p>"], "80844": ["<p>I have installed the plugin. However, the windows exhibited like that:<br>\n<strong>Starting CellProfiler 4.2.5</strong><br>\n<strong>Could not load runcellpose</strong><br>\n<strong>Traceback (most recent call last):</strong><br>\n**  File \u201ccellprofiler_core\\utilities\\core\\modules_<em>init</em>_.py\u201d, line 71, in add_module**<br>\n**  File \u201cD:\\google download\\Install_CellProfiler_with_Plugins\\Install_CellProfiler_with_Plugins\\CellProfiler-plugins\\runcellpose.py\u201d, line 3, in **<br>\n**    from cellpose import models, io, core, utils**<br>\n<strong>ModuleNotFoundError: No module named \u2018cellpose\u2019</strong><br>\n<strong>Could not load runimagejscript</strong><br>\n<strong>Traceback (most recent call last):</strong><br>\n**  File \u201ccellprofiler_core\\utilities\\core\\modules_<em>init</em>_.py\u201d, line 71, in add_module**<br>\n**  File \u201cD:\\google download\\Install_CellProfiler_with_Plugins\\Install_CellProfiler_with_Plugins\\CellProfiler-plugins\\runimagejscript.py\u201d, line 25, in **<br>\n**    import cpij.bridge as ijbridge, cpij.server as ijserver**<br>\n**  File \u201cD:\\google download\\Install_CellProfiler_with_Plugins\\Install_CellProfiler_with_Plugins\\CellProfiler-plugins\\cpij\\bridge.py\u201d, line 3, in **<br>\n**    import atexit, cpij.server as ijserver**<br>\n**  File \u201cD:\\google download\\Install_CellProfiler_with_Plugins\\Install_CellProfiler_with_Plugins\\CellProfiler-plugins\\cpij\\server.py\u201d, line 9, in **<br>\n**    import jpype, imagej, multiprocessing, socket, threading, time**<br>\n<strong>ModuleNotFoundError: No module named \u2018jpype\u2019</strong><br>\n<strong>Could not load runomnipose</strong><br>\n<strong>Traceback (most recent call last):</strong><br>\n**  File \u201ccellprofiler_core\\utilities\\core\\modules_<em>init</em>_.py\u201d, line 71, in add_module**<br>\n**  File \u201cD:\\google download\\Install_CellProfiler_with_Plugins\\Install_CellProfiler_with_Plugins\\CellProfiler-plugins\\runomnipose.py\u201d, line 3, in **<br>\n**    from cellpose_omni import models, io, core, plot**<br>\n<strong>ModuleNotFoundError: No module named \u2018cellpose_omni\u2019</strong><br>\n<strong>Could not load runstardist</strong><br>\n<strong>Traceback (most recent call last):</strong><br>\n**  File \u201ccellprofiler_core\\utilities\\core\\modules_<em>init</em>_.py\u201d, line 71, in add_module**<br>\n**  File \u201cD:\\google download\\Install_CellProfiler_with_Plugins\\Install_CellProfiler_with_Plugins\\CellProfiler-plugins\\runstardist.py\u201d, line 5, in **<br>\n**    from stardist.models import StarDist2D, StarDist3D**<br>\n<strong>ModuleNotFoundError: No module named \u2018stardist\u2019</strong><br>\n<strong>could not load these modules: runcellpose,runimagejscript,runomnipose,runstardist</strong></p>\n<p>I did have installed the module. But I don\u2019t know why they can not find them.</p>"], "80848": ["<p>Hello my name is Julian, Im doing a university project in Spain, and I need your help. I have to do a brief comparative of 8 pages about Fiji and CellProfiler for example, reviewing its interface as they both analyze the same image and some similar processes. Someone could help me? I\u2019m a newbie and I don\u2019t know how to use both programs very well. Or provide me some help with a paper about it or a manual\u2026</p>", "<p>Hello <a class=\"mention\" href=\"/u/julianrc17\">@julianrc17</a> !</p>\n<p>In this <a href=\"https://forum.image.sc/t/bioimage-analysis-recommended-reading-and-viewing/28051\">post</a> you can find a plethora of resources related with image analysis and softwares. Actually them all are really good and have many applications. For sure you will find some info for your project. <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>\n<p>Suerte con el trabajo,</p>\n<p>Isaac</p>", "<p>Hi,</p>\n<p>maybe <a href=\"https://febs.onlinelibrary.wiley.com/doi/full/10.1002/1873-3468.14451\">this publication</a> could also be interesting.</p>", "<p>Hello <a class=\"mention\" href=\"/u/julianrc17\">@julianrc17</a>,</p>\n<p>While other replies have focussed on resources to tell you the difference, i would suggest that the best way would be to install both software and pick a simple problem to act as a case study.</p>\n<p>Perhaps counting cell nuclei would be a good place to start, especially as there are many datasets available (see for example the Broad Bioimage Benchmark Collection: <a href=\"https://bbbc.broadinstitute.org/\">https://bbbc.broadinstitute.org/</a>) and this is a common and well documented workflow in both software.</p>\n<p>Hope that helps!</p>"], "78805": ["<p>Hello Everybody,</p>\n<p>First of all, thank you for giving me really good advice.<br>\nNowadays, I am trying to get 4 individual images from the \u201c_seg.npy\u201d file in cellpose 2.2.<br>\nAs you can see on the screen, the result shows individual 4 images but actually it is one image(I mean that those 4 images are inside one image).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e.jpeg\" data-download-href=\"/uploads/short-url/zQo3vr4uDifjbN1KadaLNNXMHN4.jpeg?dl=1\" title=\"028_img.ome_cp_output\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_690x172.jpeg\" alt=\"028_img.ome_cp_output\" data-base62-sha1=\"zQo3vr4uDifjbN1KadaLNNXMHN4\" width=\"690\" height=\"172\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_690x172.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_1035x258.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/b/fb381bb9ad0458238de794a89a4dbcdcab4b065e_2_1380x344.jpeg 2x\" data-dominant-color=\"A1B0A0\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">028_img.ome_cp_output</span><span class=\"informations\">3600\u00d7900 207 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\nBut I need the individual 4 images for the next step of processing.<br>\nIf you know the python source code to separate those 4 images from the \u201c*_seg.npy\u201d file, please share with me.<br>\nThis task is really important for me but I am not an expert in this field.<br>\nSo I wish for your kind helpness.</p>", "<p>Hi <a class=\"mention\" href=\"/u/jameswang80428\">@JamesWang80428</a>! I\u2019m not familiar with cellpose, but it sounds like youre trying to split out a multichannel image.</p>\n<p>If that\u2019s a simple numpy save file (as it appears), yous hould be able to do the following:</p>\n<pre data-code-wrap=\"pyhton\"><code class=\"lang-plaintext\">import numpy as np\n\ndata = np.load(image_filepath)\n</code></pre>\n<p><code>data</code> should now have a specific <code>shape</code> where one of the dimensions is the channel axis. This is usually the first dimension:</p>\n<pre><code class=\"lang-python\">data.shape  # something like (4, X, Y)\n\nfirst_channel = data[0]\nsecond_channel = data[1]\n</code></pre>\n<p>I recomment reading the <a href=\"https://numpy.org/doc/stable/user/absolute_beginners.html\" rel=\"noopener nofollow ugc\">introducting docs of numpy</a>, you need very little to get a long way <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"></p>", "<p>I think it\u2019s a bit more complicated than that <a class=\"mention\" href=\"/u/brisvag\">@brisvag</a><br>\nSee my post in the other thread about reading cellpose .npy:</p><aside class=\"quote quote-modified\" data-post=\"8\" data-topic=\"76319\">\n  <div class=\"title\">\n    <div class=\"quote-controls\"></div>\n    <img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https://sea1.discourse-cdn.com/business4/user_avatar/forum.image.sc/psobolewskiphd/40/44573_2.png\" class=\"avatar\">\n    <a href=\"https://forum.image.sc/t/importing-seg-npy-files-into-napari/76319/8\">Importing seg.npy files into napari?</a> <a class=\"badge-wrapper  bullet\" href=\"/c/usage-issues/7\"><span class=\"badge-category-bg\" style=\"background-color: #BF1E2E;\"></span><span style=\"\" data-drop-close=\"true\" class=\"badge-category clear-badge\" title=\"This category is for discussing technical questions and problems with scientific image software.\">Usage &amp; Issues</span></a>\n  </div>\n  <blockquote>\n    Looking more closely at the cellpose save code: \n\nThe .npy file contains actually a dictionary with more than just the labels, which is what you\u2019d visualize with napari and further process with region props, etc. There\u2019s no way I don\u2019t think for napari builtin reader to a priori know that it needs to do: \ndata_to_load = np.load('_seg.npy', allow_pickle=True).item()\nlabels = data_to_load['masks']\n\nSeems like this requires a cellpose-specific npy reader plugin. \nNow cellpose offers a png output, w\u2026\n  </blockquote>\n</aside>\n\n<p>So what you need to do to load them is to mimic what cellpose does. For example to get the masks:</p>\n<pre><code class=\"lang-auto\">data_to_load = np.load('_seg.npy', allow_pickle=True).item()\nlabels = data_to_load['masks']\n</code></pre>"], "80859": ["<p>Hello, I have been using MeasureObjectSizeShape to measure the volume of 3D objects where the voxel size of the input image is (0.1205 X 0.1205 X 0.9756 micron^3). The output volume from the module is in voxels, but am not sure how to convert it back into micron^3 (can I directly multiply the voxel volume). Thanks!</p>"], "80348": ["<p>Hello,</p>\n<p>I\u2019m trying to load a .nd2 file using Cellprofiler. I\u2019m running the 5.0.0b1 from source. I can\u2019t view, extract metadata from, or process in any way .nd2 files. Since BioFormats is the only reader in \u201cConfigure Readers\u201d that can handle .nd2 files, presumably it\u2019s an issue with this. I hadn\u2019t had issues in prior releases so I\u2019m not sure what changed. The error I\u2019m getting seems to be related to the maven package (see below); however, I\u2019m not using, to my knowledge, the ImageJ plugin that requires maven. It seems there\u2019s an pyimagej wrapper for bio formats (I\u2019m not familiar with this terminology so hopefully I am saying this correctly), so I\u2019m wondering if it\u2019s trying to use that rather than the python-bioformats package that comes with cellprofiler? The error occurs when I try to view the .nd2 file by clicking on it (presumably this prompts Cellprofiler to read the image, which then throws the error. This is the error I\u2019m receiving when I double-click the image. Am I missing a package? I\u2019ve already tried reinstalling cellprofiler. I\u2019ve not found any documentation that states cellprofiler needs maven (it seems this is only necessary for the imageJ plugin), so I\u2019m not sure why this error is occurring.</p>\n<p>Thank you for your help</p>\n<p>Traceback (most recent call last):<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\cellprofiler\\cellprofiler\\gui\\module_view_module_view.py\u201d, line 1212, in callback<br>\nself.__on_do_something(event, setting)<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\cellprofiler\\cellprofiler\\gui\\module_view_module_view.py\u201d, line 2279, in __on_do_something<br>\nsetting.on_event_fired()<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\core\\cellprofiler_core\\setting\\do_something_do_something.py\u201d, line 27, in on_event_fired<br>\nself.__callback(*self.__args)<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\core\\cellprofiler_core\\setting\\do_something_path_list_extract_button.py\u201d, line 14, in fn_callback<br>\nself.callback(*args, **kwargs)<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\cellprofiler\\cellprofiler\\gui\\pipelinecontroller.py\u201d, line 1873, in on_extract_metadata<br>\nfile_object.extract_planes(self.__workspace)<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\core\\cellprofiler_core\\pipeline_image_file.py\u201d, line 91, in extract_planes<br>\nmeta_dict = reader.get_series_metadata()<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\core\\cellprofiler_core\\readers\\bioformats_reader.py\u201d, line 310, in get_series_metadata<br>\nself._ensure_file_open()<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\core\\cellprofiler_core\\readers\\bioformats_reader.py\u201d, line 54, in _ensure_file_open<br>\nself.get_reader().setId(self.file.path)<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\core\\cellprofiler_core\\readers\\bioformats_reader.py\u201d, line 45, in get_reader<br>\nImageReader = jimport(\u201cloci.formats.ImageReader\u201d)<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\core\\cellprofiler_core\\utilities\\java.py\u201d, line 63, in jimport<br>\nstart_java()<br>\nFile \u201cc:\\users\\tme lab user\\documents\\github\\core\\cellprofiler_core\\utilities\\java.py\u201d, line 49, in start_java<br>\nscyjava.start_jvm(options=args)<br>\nFile \u201cc:\\users\\tme lab user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scyjava_java.py\u201d, line 190, in start_jvm<br>\n_, workspace = jgo.resolve_dependencies(<br>\nFile \u201cc:\\users\\tme lab user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\jgo\\jgo.py\u201d, line 640, in resolve_dependencies<br>\nmvn = executable_path_or_raise(\u201cmvn\u201d)<br>\nFile \u201cc:\\users\\tme lab user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\jgo\\jgo.py\u201d, line 199, in executable_path_or_raise<br>\nraise ExecutableNotFound(tool, os.getenv(\u201cPATH\u201d))<br>\njgo.jgo.ExecutableNotFound: mvn not found on path C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.19.7-hotspot\\bin;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\dotnet;C:\\Program Files\\PuTTY;C:\\Program Files\\Microsoft SQL Server\\120\\Tools\\Binn;C:\\Program Files\\MATLAB\\R2023a\\runtime\\win64;C:\\Program Files\\MATLAB\\R2023a\\bin;C:\\Program Files\\MATLAB\\R2022b\\runtime\\win64;C:\\Program Files\\MATLAB\\R2022b\\bin;C:\\Users\\TME Lab User\\AppData\\Local\\Programs\\Python\\Python38\\Scripts;C:\\Users\\TME Lab User\\AppData\\Local\\Programs\\Python\\Python38;C:\\Users\\TME Lab User\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\TME Lab User\\AppData\\Local\\GitHubDesktop\\bin</p>"], "57821": ["<p>Hello,<br>\nI have noticed the paper called \u201cGPU-accelerated CellProfiler\u201d from Imen Chakroun, Nick Michiels and Roel Wuyts, 2018. The authors made the \u201cMeasureTexture\u201d module and \u201cMeasureObjectSizeShape\u201d module run on GPU and thus improved the performance 7.5 times.</p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https://ieeexplore.ieee.org/document/8621271\">\n  <header class=\"source\">\n\n      <a href=\"https://ieeexplore.ieee.org/document/8621271\" target=\"_blank\" rel=\"noopener nofollow ugc\">ieeexplore.ieee.org</a>\n  </header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/9/7/97b982ab95d326d8b25ac873d7262ba98123166d.png\" class=\"thumbnail onebox-avatar\" width=\"200\" height=\"200\">\n\n<h3><a href=\"https://ieeexplore.ieee.org/document/8621271\" target=\"_blank\" rel=\"noopener nofollow ugc\">GPU-accelerated CellProfiler</a></h3>\n\n  <p>CellProfiler excels at bridging the gap between advanced image analysis algorithms and scientists who lack computational expertise. It lacks however high performance capabilities needed for High Throughput Imaging experiments where workloads reach...</p>\n\n\n  </article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  </div>\n\n  <div style=\"clear: both\"></div>\n</aside>\n\n<p>I have seen the threads from 2017 saying that GPUs are not employed. Is it still the case ?<br>\nAre any solutions of GPU-computing out there? And how hard would it be to implement it?</p>\n<p>Thanks a lot,<br>\nViktor</p>", "<p>I can not believe that a paper was published on this but there was no effort to push the changes upstream into the official CP code?</p>", "<p>The primary problem here is that, while every user will have a CPU, not every user will have a GPU. Even then only specific GPU types are supported for some workflows. Python dependencies specific to the hardware setup often need to be installed. This makes it difficult to produce a CellProfiler build which will work across a broad variety of system configurations</p>\n<p>Furthermore, CellProfiler\u2019s current approach to multiprocessing involves running multiple instances of the program concurrently, which doesn\u2019t always work when sharing a single GPU between them.</p>\n<p>To my knowledge we\u2019re yet to recieve a PR which would integrate GPU processing, but if the above challenges can be handled it\u2019d be more than welcome. As it stands I included some support for GPU processing in the new RunCellpose and RunStarDist plugins, since these run on CellProfiler built from source (enabling the user to configure GPU-specific dependencies for their machine).</p>\n<p>With regards to MeasureTexture and MeasureObjectSizeShape, substantial revisions were made to these modules in CellProfiler 4 in order to resolve performance issues. We managed to make some of the texture measurements 100-fold faster by optimising the existing code (instead of running the older, inefficient code on a GPU). Hopefully this relieves some of the issues for the time being.</p>", "<p>I see, thank you for the info. I am glad to hear that the pipelines I am using now are already optimized even without the use of GPU.</p>", "<p>How can I download this particular version of cellprofiler</p>"], "76765": ["<p>Hello,<br>\nI am getting an error message during the analysis part of my pipeline. The pipeline processes most of the images fine, but on some this error message pops up. It does not seem consistent on the type of image (I am using both CZI and TIFF files and there is no correlation), and what\u2019s even weirder is I\u2019ve run the analysis twice after changing some settings and different images get the error message. Ones that got the error message the second time didn\u2019t get it the first and vice versa. These images load fine during test mode.</p>\n<p>Please let me know if you have any suggestions or need more information!</p>\n<pre><code class=\"lang-auto\">Error while processing NamesAndTypes:\n(Worker) JavaException: ome.jxrlib.JXRJNI.new_DecodeContext()J\nDo you want to stop processing?\n</code></pre>"], "78819": ["<p>Hi there,<br>\nI\u2019m trying to combine Live/Dead images for analysis. Files are named like<br>\nRun-10_23d802-01_B3_Treat5_Eth_1.tif</p>\n<p>and the metadata regex<br>\n^(?P.<em>)_(?P.</em>)<em>(?P[A-P][0-9])</em>(?P.<em>)_(?P.</em>)_(?P[0-9]{1}).tif<br>\ncreates the expected table of metadata correctly.</p>\n<p>When I then try to assign Live and Dead channels using the metadata, there is the dreaded \u201cSorry, \u2026 no valid image sets\u2026\u201d error. Images shall be matched by Plate, Well and Nr (=repeat). I cut it down to two images to no avail. I also tried to assign channels based on the filename (\u201ccontains: Eth\u201d) but that didn\u2019t help - no sets detected (it used to work with a few files with the \u201corder\u201d detection but we need the metadata).</p>\n<p>Can anyone point my mistake out? The path contains a space, but I used CP before like that without issues.</p>\n<p>Any help is greatly appreciated,<br>\nKonstantin</p>\n<p><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/6/76903dda9a68a70886058df3e19e86986b91bca1.png\" alt=\"cp-1\" data-base62-sha1=\"gURnq4QzDYn09T8Y9bpxMZ3LV8R\" width=\"234\" height=\"33\"><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7f306d25890600738d74f5876d98029528d83c78.png\" data-download-href=\"/uploads/short-url/i9aohOGZbTv93ZvGs1y2YfJdxe8.png?dl=1\" title=\"cp-2\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/f/7f306d25890600738d74f5876d98029528d83c78.png\" alt=\"cp-2\" data-base62-sha1=\"i9aohOGZbTv93ZvGs1y2YfJdxe8\" width=\"690\" height=\"255\" data-dominant-color=\"E9E9E9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">cp-2</span><span class=\"informations\">851\u00d7315 8.42 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a1a515ef432a735be6f8134893dfc21fc6bc8ddf.png\" data-download-href=\"/uploads/short-url/n3Yzag3TGnm8C1zV2AlKsmOBWft.png?dl=1\" title=\"cp-3\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/1/a1a515ef432a735be6f8134893dfc21fc6bc8ddf.png\" alt=\"cp-3\" data-base62-sha1=\"n3Yzag3TGnm8C1zV2AlKsmOBWft\" width=\"690\" height=\"458\" data-dominant-color=\"EAEAEA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">cp-3</span><span class=\"informations\">902\u00d7600 20.4 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hi Konstantin,<br>\nI do have the same problem using CP4.2.5 (and earlier versions) with images acquired from spheroids through Opera Phenix and uploaded as *.tiff in CellProfiler.<br>\n1 - Trying to update the NamesAndTypes module, CellProfiler is giving me an identical error message (see Screenshot#1).<br>\n2 - Then when trying to visualize the acquired images through CellProfiler, it giving a second error message claiming that it failed to load the images (see Screenshot#2).<br>\n3 - Also tried to re-import the acquired images but the error message persisted!<br>\n<a>Uploading: Screenshot#1.png\u2026</a> i<br>\n<a>Uploading: Screenshot#2.png\u2026</a>s<br>\nAny help would be appreciated<br>\nThank you</p>"], "78823": ["<p>Hi,<br>\nI generated a classifier model using CPA 3.0.4 based on features from the actin cytoskeleton of cells. When I plug this model into the ClassifyObjects module in CP 4.2.4 and run the pipeline on new images, I get an error message indicating that some measurements don\u2019t exist, but the appropriate modules are present and running. I would appreciate any help or insight I can get for this issue. I am attaching here all the required information for others to look into (error message, CP pipeline, CPA model, sample images).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png\" data-download-href=\"/uploads/short-url/sAwr8M64Nvc1FyOISchV1Hie36Y.png?dl=1\" title=\"errorCellProfiler\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png\" alt=\"errorCellProfiler\" data-base62-sha1=\"sAwr8M64Nvc1FyOISchV1Hie36Y\" width=\"421\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc_2_421x500.png, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/8/c85dca2f06b0bc4729e9d42ca05972fad2a994cc.png 2x\" data-dominant-color=\"CBD2DA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">errorCellProfiler</span><span class=\"informations\">430\u00d7510 73.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a class=\"attachment\" href=\"/uploads/short-url/5v48dYmZdYzze8TvbWMxMGMPWX.zip\">CPAmodel_CPpipeline.zip</a> (9.8 MB)</p>\n<p>Thank you!</p>\n<p>Victoria</p>"], "74217": ["<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/a/0af4bb859764be6cad9431b6d38ff1834218690a.jpeg\" data-download-href=\"/uploads/short-url/1yV6VjazfM07fRklGN4WKkatCEa.jpeg?dl=1\" title=\"example_brain\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0af4bb859764be6cad9431b6d38ff1834218690a_2_690x471.jpeg\" alt=\"example_brain\" data-base62-sha1=\"1yV6VjazfM07fRklGN4WKkatCEa\" width=\"690\" height=\"471\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0af4bb859764be6cad9431b6d38ff1834218690a_2_690x471.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0af4bb859764be6cad9431b6d38ff1834218690a_2_1035x706.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/a/0af4bb859764be6cad9431b6d38ff1834218690a_2_1380x942.jpeg 2x\" data-dominant-color=\"240224\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">example_brain</span><span class=\"informations\">1754\u00d71199 164 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/5/8/5873672dca8401a1fdbe130e6e8bea20528b0c93.jpeg\" data-download-href=\"/uploads/short-url/cCtgzpL3bmdooZSQqKvcr2ZQkr9.jpeg?dl=1\" title=\"example_brain_fixed\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/5873672dca8401a1fdbe130e6e8bea20528b0c93_2_690x471.jpeg\" alt=\"example_brain_fixed\" data-base62-sha1=\"cCtgzpL3bmdooZSQqKvcr2ZQkr9\" width=\"690\" height=\"471\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/5873672dca8401a1fdbe130e6e8bea20528b0c93_2_690x471.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/5873672dca8401a1fdbe130e6e8bea20528b0c93_2_1035x706.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/5/8/5873672dca8401a1fdbe130e6e8bea20528b0c93_2_1380x942.jpeg 2x\" data-dominant-color=\"240224\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">example_brain_fixed</span><span class=\"informations\">1754\u00d71199 159 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<h3>\n<a name=\"goal-1\" class=\"anchor\" href=\"#goal-1\"></a>Goal</h3>\n<p>We are hoping to clean up some images where parts of single (but sometimes torn) brain tissue slices have drifted apart on the microscope slide. We need to stitch humpty dumpty back together again, so that these slices are more readily mapped onto reference brain atlases by semi-automated tools like DeepSlice and ABBA.</p>\n<h3>\n<a name=\"example-images-2\" class=\"anchor\" href=\"#example-images-2\"></a>Example images</h3>\n<p>You can see the parts of the image at the right have drifted out of place. We have stitched them back together to our liking in a compressed and flattened jpg version, but we are hoping to do this type o thing in full resolution TIFFs (see file sizes below).</p>\n<h3>\n<a name=\"our-image-and-hardware-specs-3\" class=\"anchor\" href=\"#our-image-and-hardware-specs-3\"></a>Our Image and Hardware Specs</h3>\n<p>-Specifically, we are working with 3-channel, 16-bit TIFF images of fluorescent brain slices that average 20 000 x 20 000 px in size.</p>\n<p>-We have the hardware to handle the images (128 GB RAM, Intel Core i9, NVIDIA GeForce GTX 1080 Ti).</p>\n<h3>\n<a name=\"barries-weve-run-into-in-trying-to-address-this-4\" class=\"anchor\" href=\"#barries-weve-run-into-in-trying-to-address-this-4\"></a>Barries we\u2019ve run into in trying to address this:</h3>\n<p>-ImageJ doesn\u2019t allow copying subsection of more than one channel at once or rotating subsections. Duplication of a subsection of multiple channels into a new image is possible and this also allows for rotation. However, the merging process is not very flexible (i.e. you cannot click and drag or rotate the separate images easily to put them exactly where you want them).</p>\n<p>-GIMP can manipulate all three channels at once if they are Composed into the three color channels of a layer. But it isn\u2019t designed for images of this size and so can take minutes to select, move, and rotate subsections. Also, the values of the pixels in the images are not high enough to be visible without increasing their brightness, but I don\u2019t know how to increase the brightness of an image in GIMP without actually changing the pixels\u2019 values.</p>\n<h3>\n<a name=\"request-5\" class=\"anchor\" href=\"#request-5\"></a>Request</h3>\n<p>If you happen to know of a software and/or protocol for doing this type of image cleanup, we would greatly appreciate it if you could share it with us.</p>\n<p>Thank you very much!!</p>"], "69103": ["<p>Hello,</p>\n<p>I am new to both the field and to Cellprofiler. I am studying how neurite outgrowth of primary rat cells change when treated with certain chemical compounds. My goal is to count colocalized pre and post synaptic spots in close proximity to the neurite network. I have stained the cells for MAP2, PSD95 and Synapsin1.</p>\n<p>I have obtained my images on a Celldiscoverer 7, Zeiss instrument, 140 wells, two field of views per well, 7 stacks per field of view with 4 channels (DAPI, 488, 568 and 647nm WF).</p>\n<p>My general issue is that I don\u2019t really know where to start to perform this task. I have analyzed the images in the more simple (in my opinion) Harmony software from Perkin Elmer but want to bridge the results with analysis conducted in Cellprofiler.</p>\n<p>In snipped image below, Synapsin (purple) and PSD95 (green) can be seen separate and colocalized. I want to capture the total number of both populations as well as the colocalized.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/f/3/f3600501b2eab66feda74ee2e0cffaa16035f3dd.jpeg\" data-download-href=\"/uploads/short-url/yIZKgFpJCbuFb1MO6vb6nYwnj09.jpeg?dl=1\" title=\"Example_spots_colocalized.PNG\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f3600501b2eab66feda74ee2e0cffaa16035f3dd_2_690x417.jpeg\" alt=\"Example_spots_colocalized.PNG\" data-base62-sha1=\"yIZKgFpJCbuFb1MO6vb6nYwnj09\" width=\"690\" height=\"417\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f3600501b2eab66feda74ee2e0cffaa16035f3dd_2_690x417.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f3600501b2eab66feda74ee2e0cffaa16035f3dd_2_1035x625.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f3600501b2eab66feda74ee2e0cffaa16035f3dd_2_1380x834.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/f/3/f3600501b2eab66feda74ee2e0cffaa16035f3dd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Example_spots_colocalized.PNG</span><span class=\"informations\">1914\u00d71157 126 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>In this zoomed in cropped image we can see that they are colocalizing but also stand alone.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/a/7/a7c97b5a4dcdac3ef4cc9b162a1512d7951026eb.jpeg\" data-download-href=\"/uploads/short-url/nWjprH78yejsGXYVK9C1Z3lrsHV.jpeg?dl=1\" title=\"Zoomed_in_example_coloc.PNG\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7c97b5a4dcdac3ef4cc9b162a1512d7951026eb_2_690x336.jpeg\" alt=\"Zoomed_in_example_coloc.PNG\" data-base62-sha1=\"nWjprH78yejsGXYVK9C1Z3lrsHV\" width=\"690\" height=\"336\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7c97b5a4dcdac3ef4cc9b162a1512d7951026eb_2_690x336.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7c97b5a4dcdac3ef4cc9b162a1512d7951026eb_2_1035x504.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7c97b5a4dcdac3ef4cc9b162a1512d7951026eb_2_1380x672.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/a/7/a7c97b5a4dcdac3ef4cc9b162a1512d7951026eb_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Zoomed_in_example_coloc.PNG</span><span class=\"informations\">1920\u00d7937 46.2 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>What I\u2019m asking is if someone could please either help me build a pipeline or point me to a good start.</p>\n<p>With kind regards,<br>\nAlex</p>", "<p>Hi Alex,</p>\n<p>You could try using IdentifyPrimaryObjects to identify your green and purple spots, then MeasureObjectOverlap to measure the overlap between them and decide on some cutoff for what you want to consider colocalized. An alternative approach would be to use MaskObjects, which allows you to pick a fraction of object overlap to consider (e.g., only keep green spots that are 80% colocalized with purple).<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/3/e/3e555944944455f6738a54f810515f1be091d738.jpeg\" data-download-href=\"/uploads/short-url/8TqsDCtfxifVzEaf8y81YdMyaOA.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3e555944944455f6738a54f810515f1be091d738_2_690x225.jpeg\" alt=\"image\" data-base62-sha1=\"8TqsDCtfxifVzEaf8y81YdMyaOA\" width=\"690\" height=\"225\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3e555944944455f6738a54f810515f1be091d738_2_690x225.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3e555944944455f6738a54f810515f1be091d738_2_1035x337.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3e555944944455f6738a54f810515f1be091d738_2_1380x450.jpeg 2x\" data-small-upload=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/3/e/3e555944944455f6738a54f810515f1be091d738_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1772\u00d7580 110 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>One other thing I\u2019d recommend is looking into your imaging settings and seeing if there is any way to minimize XY shift between your channels. It looks like there\u2019s a fair amount of shift between the purple and green spots which could cause you to miss real colocalization events. We have the Align module which could help with this, but it\u2019s going to try to find the highest correlation between your images and you might just want to shift them a certain # of pixels relative to each other based on shift measured with tetraspeck (or other multicolor) beads.</p>", "<p>Hello Rebecca,</p>\n<p>Thank you for your informative and swift reply! The MeasureObjectOverlap worked like a charm, and with the aid of other posts here I managed to create a mask out of the neurite network to only capture spots found within the network.</p>\n<p>// Alex</p>"], "80368": ["<p>Hi <a class=\"mention\" href=\"/u/bcimini\">@bcimini</a> and <a class=\"mention\" href=\"/u/aklemm\">@aklemm</a>,</p>\n<p>Do you know whether there is a function in Cellprofiler to replace objects by their convex hull?<br>\nEssentially make them \u201cmore round\u201d?</p>\n<p>Thanks!</p>", "<p>We just found that the \u201cFillObjects\u201d Method has this option!</p>"], "75767": ["<p>All,</p>\n<p>I\u2019m trying to follow the Pixel-based Classification Tutorial using my images that I took through Ilastik and got the segmentation.</p>\n<p>I\u2019m on the final part of the tutorial-III. Segmenting probabilities with CellProfiler.</p>\n<p>I loaded my images and the probability images.  In the names and Types module I kept the jpg images has phase and the .tiff as cho.  In the ColortoGray I converted just the green to gray and named it choSegmented as the background was my first class in Ilastik.</p>\n<p>But I get an error when I try to run the program\u2026\u2026 I\u2019ve attached screen shots<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg\" data-download-href=\"/uploads/short-url/Ef8Wlhap0RPA0mhugiuF1GmMQt.jpeg?dl=1\" title=\"1_2_L_x5\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285_2_666x499.jpeg\" alt=\"1_2_L_x5\" data-base62-sha1=\"Ef8Wlhap0RPA0mhugiuF1GmMQt\" width=\"666\" height=\"499\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285_2_666x499.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/0/4/048c99a0407f167421f150d40de17b801d2cd285.jpeg 2x\" data-dominant-color=\"ADADAD\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">1_2_L_x5</span><span class=\"informations\">990\u00d7743 227 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<a class=\"attachment\" href=\"/uploads/short-url/kyWEJsFl9DDLgXWOMx4DOLetcog.tiff\">1_2_L_x5_Probabilities.tiff</a> (1.5 MB)<br>\nof the error message.  I was hoping that I could modify this program to fit my images.  I have also added an original image and the segmentation if it is easier to go that route.</p>\n<p>Analysis Goals-<br>\nI\u2019m trying the quantitate the amount of deposits in the image.  I am happy with the segmentation from ilastik, but now I need to quantify using CellProfiler.</p>\n<p>Challenges<br>\nThe error message I\u2019m getting, what have I done wrong??</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/6/d607d5ecba71c48fc8359d348663dc33aa859dc5.png\" data-download-href=\"/uploads/short-url/uxoXV5BGxiI1xoGPafLLCaNP4mF.png?dl=1\" title=\"errormessage\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d607d5ecba71c48fc8359d348663dc33aa859dc5_2_690x399.png\" alt=\"errormessage\" data-base62-sha1=\"uxoXV5BGxiI1xoGPafLLCaNP4mF\" width=\"690\" height=\"399\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d607d5ecba71c48fc8359d348663dc33aa859dc5_2_690x399.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/6/d607d5ecba71c48fc8359d348663dc33aa859dc5_2_1035x598.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/6/d607d5ecba71c48fc8359d348663dc33aa859dc5.png 2x\" data-dominant-color=\"F2F2F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">errormessage</span><span class=\"informations\">1320\u00d7764 29.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/9/d9b48369d3cc14bfd9f57e5571fc1df097621ca2.png\" data-download-href=\"/uploads/short-url/v3UmgszaEY4MWnGXacfU5wJYmS6.png?dl=1\" title=\"images\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9b48369d3cc14bfd9f57e5571fc1df097621ca2_2_690x380.png\" alt=\"images\" data-base62-sha1=\"v3UmgszaEY4MWnGXacfU5wJYmS6\" width=\"690\" height=\"380\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9b48369d3cc14bfd9f57e5571fc1df097621ca2_2_690x380.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9b48369d3cc14bfd9f57e5571fc1df097621ca2_2_1035x570.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/9/d9b48369d3cc14bfd9f57e5571fc1df097621ca2_2_1380x760.png 2x\" data-dominant-color=\"F5F5F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">images</span><span class=\"informations\">1485\u00d7819 25.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div><br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/7/3/7360013837950d3b472035d6c4e4aec41010a4d3.png\" data-download-href=\"/uploads/short-url/gsEB65WpRnIfeIPETvUhxrDFNar.png?dl=1\" title=\"namesandtypes\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/7360013837950d3b472035d6c4e4aec41010a4d3_2_690x370.png\" alt=\"namesandtypes\" data-base62-sha1=\"gsEB65WpRnIfeIPETvUhxrDFNar\" width=\"690\" height=\"370\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/7360013837950d3b472035d6c4e4aec41010a4d3_2_690x370.png, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/7360013837950d3b472035d6c4e4aec41010a4d3_2_1035x555.png 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/7/3/7360013837950d3b472035d6c4e4aec41010a4d3_2_1380x740.png 2x\" data-dominant-color=\"F2F2F2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">namesandtypes</span><span class=\"informations\">1695\u00d7910 31 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>", "<p>Hello <a class=\"mention\" href=\"/u/specialk\">@SpecialK</a>,</p>\n<p>I noticed in your NamesAndTypes module you\u2019re selecting the phase images based on the criteria extension \u201cpng\u201d, but your images have a \u201cjpg\u201d extension, can you change it and try again?</p>\n<p>Best,<br>\nMario</p>", "<p>Yes, it\u2019s the simple things. Thank you!</p>\n<p>BTW, since I did the segmentation in Ilastik and am happy with it, is there a way to just use the segmentation as it is? In seems like instead, we need to do the thresholding and find parameters that match what I already had, which is difficult to do. Is there an easier way?</p>\n<p>Thanks,</p>\n<p>Kristin</p>", "<p>Also, when running the pipeline the Identify objects shows the % coverage value which is what I\u2019m after. However, when I look at the output, that value isn\u2019t coming up?</p>\n<p>Thanks,<br>\nKristin</p>", "<p><a class=\"mention\" href=\"/u/mcruz\">@Mcruz</a> Thanks for your help, that fixed my issue.  But I did have another thought. Can I use the segmentation from Ilastik as a binary image and then just quantitate from it. So it\u2019s basically a black and white image and I determine the %white of the total.  That\u2019s not what the tutorial is having me do. Instead, even though I have to probability map, I still have to threshold to try to find parameters that work? Can\u2019t I just use the segmentation map? What if I have 3 labeled features and I only want to count 1 of them and exclude the other two? Or I want to count them all separately and know the percent of total for each? There has to be an easy way to use the segmentation map?</p>", "<p>Hi Kristin,</p>\n<p>That tutorial is just one way of interacting with ilastik - you\u2019re absolutely right that you could bring segmentations into CellProfiler as well (I can\u2019t be CERTAIN that this works in ilastik without trying it, but if it works the way nearly all other software packages do with regards to segmentation, you can import them into CellProfiler as type \u201cObject\u201d). ilastik does also produce a number of measurements, so if you\u2019re happy with the segmentation you\u2019re getting there AND the measurements available to you, you could probably stick to an \u201call-ilastik\u201d solution! Switching across tools is typically only for 1) cases where you want more fine-grained control of the segmentation or 2) cases where CellProfiler has a measurement you want and ilastik does not.</p>", "<p>Hi Beth!</p>\n<p>Yes, I was able to get the segmentation in Ilastik and the do the object classification with the segmentation and get some values, but it\u2019s not as straight forward as Cellprofilier since I\u2019m a bit more familiar with easily getting a size, count, and % of area.</p>\n<p>So, if I can\u2019t get that all figured out then, I should be able to take the segmentation output, which I\u2019m picturing as more of a black and white, and then get the size, count, and %area of everything white. You say you import them into CellProfiler as type Object, is that the convert image to objects module?</p>\n<p>Thanks for your help,</p>\n<p>Kristin</p>", "<p>Sorry to have missed this earlier! No, you can literally just set the input image type to Objects in NamesAndTypes and be off from there.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1609d058110d2b83ac73e199d0cdf5e7a8437e0.jpeg\" data-download-href=\"/uploads/short-url/tSeMDDk5GLyhRXBfIWA07jxY4OA.jpeg?dl=1\" title=\"image\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1609d058110d2b83ac73e199d0cdf5e7a8437e0_2_690x301.jpeg\" alt=\"image\" data-base62-sha1=\"tSeMDDk5GLyhRXBfIWA07jxY4OA\" width=\"690\" height=\"301\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1609d058110d2b83ac73e199d0cdf5e7a8437e0_2_690x301.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/d/1/d1609d058110d2b83ac73e199d0cdf5e7a8437e0_2_1035x451.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/d/1/d1609d058110d2b83ac73e199d0cdf5e7a8437e0.jpeg 2x\" data-dominant-color=\"DCDEE4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">1040\u00d7454 64.5 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>"], "28666": ["<p>Hello all,<br>\nI just installed Cellprofiler in order to automatically count yeasts (stained and unstained with methylene blue) on an haemocytometer.<br>\n<a class=\"attachment\" href=\"/uploads/short-url/oDMg4b3mh4r9HPh70uHByJWzWih.tiff\">yc2.tiff</a> (423.4 KB)</p>\n<p>I tried to adapt the script \u201cexample percentage positive\u201d but i do not discriminate the images with phase contrast, only by staining.<br>\nI am suspecting that the grid may be a problem too?</p>\n<p>Moreover, would intensity differences between alive and stained dead cells be enough to discriminate them?<br>\nThank you for your help!</p>\n<p>Have a good day</p>\n<p>ACM</p>", "<p>Hi. Did you ever get anywhere with this?</p>"], "77308": ["<p>Hello, I was trying to use Crop tool with \u201cEllipse\u201d cropping shape. However, after applying the crop, the whole image seems empty (black). In contrast, when I use the \u201cRectangle\u201d shape, everything works as expected, only the part outside the rectangle is cropped (black).  I append two screens shots.<br>\nThis is with Cellprofiler 4.2.1 on a Mac (Mohave)  - I don\u2019t know how it would look in 4.2.5, since I can\u2019t get that work on my Mac.</p>\n<p>Rectangle:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/7/c749dfd06b910a435aad351c8792f61e33cfe53a.jpeg\" data-download-href=\"/uploads/short-url/sqZi4BIzBP1MroA0mKIK4PCu1d0.jpeg?dl=1\" title=\"Screenshot 2023-02-15 at 15.53.48\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749dfd06b910a435aad351c8792f61e33cfe53a_2_634x500.jpeg\" alt=\"Screenshot 2023-02-15 at 15.53.48\" data-base62-sha1=\"sqZi4BIzBP1MroA0mKIK4PCu1d0\" width=\"634\" height=\"500\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749dfd06b910a435aad351c8792f61e33cfe53a_2_634x500.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749dfd06b910a435aad351c8792f61e33cfe53a_2_951x750.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/7/c749dfd06b910a435aad351c8792f61e33cfe53a_2_1268x1000.jpeg 2x\" data-dominant-color=\"818181\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-02-15 at 15.53.48</span><span class=\"informations\">1272\u00d71002 65.3 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Ellipse<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/b/4/b4cc9901b48cf8d3f3686337a10876e83e3034b0.jpeg\" data-download-href=\"/uploads/short-url/pNqib2cXMAz5dzFqfj8TGMx0f9S.jpeg?dl=1\" title=\"Screenshot 2023-02-15 at 17.29.43\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4cc9901b48cf8d3f3686337a10876e83e3034b0_2_690x357.jpeg\" alt=\"Screenshot 2023-02-15 at 17.29.43\" data-base62-sha1=\"pNqib2cXMAz5dzFqfj8TGMx0f9S\" width=\"690\" height=\"357\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4cc9901b48cf8d3f3686337a10876e83e3034b0_2_690x357.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4cc9901b48cf8d3f3686337a10876e83e3034b0_2_1035x535.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/b/4/b4cc9901b48cf8d3f3686337a10876e83e3034b0_2_1380x714.jpeg 2x\" data-dominant-color=\"AAAAAA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screenshot 2023-02-15 at 17.29.43</span><span class=\"informations\">1920\u00d7996 94 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Is this a bug, or what is wrong?<br>\nPS: in the Ellipse screen shot, in the left image the outside is all black, because I made the outside black in ImageJ. However the ellipse crop result was the same (all black) when using the original image (the one seen in the rectangle screen shot )</p>", "<p>Hi <a class=\"mention\" href=\"/u/thomasb\">@ThomasB</a></p>\n<p>Sorry to hear that you\u2019re have problems installing CP 425, can you describe better what\u2019s happening?</p>\n<p>Regarding the crop module, it looks like your crop module is configured to crop just the first 1000x1000 pixels of your original image (this region looks black).</p>\n<p>Can you please change the coordinates of the ellipse center to the center of your original image (x:2000 and y:2500, assuming your original image have 4000x5000 pixel) and then changing the Ellipse radius in X for 2000 and the Y for 2250 for example should resolve your problem.</p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/business4/uploads/imagej/original/3X/c/e/ce76a4a59b066efd56a88887744232dbc9457620.jpeg\" data-download-href=\"/uploads/short-url/tssy8RC0JSA7gbV3UGTlX3XOKyI.jpeg?dl=1\" title=\"Screen Shot 2023-02-15 at 12.28.56 PM\"><img src=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce76a4a59b066efd56a88887744232dbc9457620_2_690x284.jpeg\" alt=\"Screen Shot 2023-02-15 at 12.28.56 PM\" data-base62-sha1=\"tssy8RC0JSA7gbV3UGTlX3XOKyI\" width=\"690\" height=\"284\" srcset=\"https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce76a4a59b066efd56a88887744232dbc9457620_2_690x284.jpeg, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce76a4a59b066efd56a88887744232dbc9457620_2_1035x426.jpeg 1.5x, https://global.discourse-cdn.com/business4/uploads/imagej/optimized/3X/c/e/ce76a4a59b066efd56a88887744232dbc9457620_2_1380x568.jpeg 2x\" data-dominant-color=\"B1B1B1\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">Screen Shot 2023-02-15 at 12.28.56 PM</span><span class=\"informations\">1920\u00d7791 107 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg>\n</div></a></div></p>\n<p>Best,<br>\nMario</p>", "<p>Hi Mario,<br>\nthanks for the help. Indeed, a lapse on my part. The original images were 1300x1000 hence I supposedly set the ellipse more or less in the middle, forgetting that I tiled the images so they were bigger.</p>\n<p>As to CP 425 not working\u2026   Well, not much to describe. First time to start it I had to open it with the open command (because of the Apple security).  But then, just nothing happens, nothing starts up.</p>\n<p>From the console log, I just get a message that CP exited with abnormal code 1 :</p>\n<p>Feb 16 12:33:24 dbm079-101 diagnosticd[46814]: System mode client started - Console (47645) - mode: 0xb, filter: \u201c\u201d<br>\nFeb 16 12:33:30 dbm079-101 com.apple.xpc.launchd[1] (com.apple.quicklook[47646]): Endpoint has been activated through legacy launch(3) APIs. Please switch to XPC or bootstrap_check_in(): com.apple.quicklook<br>\nFeb 16 12:33:30 dbm079-101 com.apple.xpc.launchd[1] (com.apple.imfoundation.IMRemoteURLConnectionAgent): Unknown key for integer: _DirtyJetsamMemoryLimit<br>\nFeb 16 12:33:44 dbm079-101 com.apple.xpc.launchd[1] (org.cellprofiler.CellProfiler.39504[47650]): Service exited with abnormal code: 1<br>\nFeb 16 12:33:50 dbm079-101 com.apple.xpc.launchd[1] (com.apple.mdworker.shared.10000000-0000-0000-0000-000000000000[47644]): Service exited due to SIGKILL | sent by mds[45191]<br>\nFeb 16 12:34:03 dbm079-101 Console[47645]: BUG in libdispatch client: vnode, monitored resource vanished before the source cancel handler was invoked { 0x600001035680[source], ident: 15 / 0xf, handler: 0x7fff5a6b281b }<br>\nFeb 16 12:34:03 dbm079-101 com.apple.xpc.launchd[1] (com.apple.imfoundation.IMRemoteURLConnectionAgent): Unknown key for integer: _DirtyJetsamMemoryLimit</p>", "<p>Hi <a class=\"mention\" href=\"/u/thomasb\">@ThomasB</a></p>\n<p>For CP425, can you uninstall all CellProfiler versions from your computer and install again CP425?<br>\nAfter the installation process can you go to System Preferences \u2192 Security&amp;Privacy And allow CellProfiler?</p>\n<p>Hope this process solves the issue,</p>\n<p>Best,<br>\nMario</p>"]}